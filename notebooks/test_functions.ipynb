{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "init"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "from autosearch.functions.text_analysis import chunk_pdf\n",
    "\n",
    "from autosearch.database.paper_database import PaperDatabase\n",
    "from autosearch.analysis.document_analyzer import DocumentAnalyzer\n",
    "from autosearch.functions.create_teachable_groupchat import create_teachable_groupchat\n",
    "\n",
    "import autogen\n",
    "from typing import List, Dict, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure credentials from environment variables\n",
    "api_key = os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\")\n",
    "endpoint = os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project_dir = \"./project_test\"\n",
    "os.makedirs(Project_dir, exist_ok=True)\n",
    "paperdb_dir = f\"{Project_dir}/paperdb\"\n",
    "db_dir = f\"{Project_dir}/db\"  \n",
    "global initiate_db \n",
    "initiate_db = False\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-32k\"]#, \"gpt-4\", \"gpt-35-turbo-16k\", \"gpt-4-0613\", \"gpt-3.5-turbo\", \"gpt-35-turbo\", \"gpt-35-turbo-0613\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,  # config_list should be defined or imported\n",
    "    \"timeout\": 120,\n",
    "    # \"seed\": 42,\n",
    "}\n",
    "\n",
    "paper_db = PaperDatabase(paperdb_dir)\n",
    "\n",
    "# Initialize the DocumentAnalyzer\n",
    "analyzer = DocumentAnalyzer(api_key, endpoint, project_dir=Project_dir)\n",
    "\n",
    "\n",
    "project_config = {\n",
    "    \"paper_db\": paper_db,\n",
    "    \"doc_analyzer\": analyzer,\n",
    "    \"project_dir\": Project_dir,\n",
    "    \"db_dir\": db_dir,\n",
    "    \"config_list\": config_list,\n",
    "    \"initiate_db\": initiate_db\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_teachable_groupchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate_db\n",
    "prompt = \"\"\"\n",
    "For each memorization task, initiate your process with 'MEMORIZE_ARTICLE:'  \n",
    "Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. \n",
    "Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. \n",
    "Upon choosing to memorize, you MUST finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
    "\"\"\"\n",
    "instract_assistant, instract_user = create_teachable_groupchat(\"instract_assistant\", \"instract_user\", db_dir, config_list, verbosity=3)\n",
    "\n",
    "instract_user.initiate_chat(instract_assistant, silent=True, message=prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv retieval, Arxiv search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Human-Centred Learning Analytics and AI in Education: a Systematic Literature Review\"\n",
    "\n",
    "from autosearch.functions.academic_search import AcademicSearch\n",
    "\n",
    "\n",
    "# Usage example\n",
    "function = AcademicSearch(project_config=project_config)\n",
    "# Get the function details\n",
    "function_details = function.get_function_details()\n",
    "print(function_details)\n",
    "\n",
    "# Now you can use function_details with your agent\n",
    "# agent.equip_function(function_details)\n",
    "\n",
    "# Or use the function directly\n",
    "\n",
    "function.func(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"message\":[\"Large Language Models\", \"Assessing Language Models\", \"AI safety and reliability\"],\n",
    "    \"n_results\":3\n",
    "}\n",
    "\n",
    "from autosearch.functions.academic_retriever import AcademicRetriever , academic_retriever\n",
    "from typing import get_type_hints\n",
    "\n",
    "# Usage example\n",
    "academic_retriever = AcademicRetriever   (project_config=project_config)\n",
    "# Get the function details\n",
    "function_details = academic_retriever.get_function_details()\n",
    "print(function_details)\n",
    "\n",
    "# Now you can use function_details with your agent\n",
    "# agent.equip_function(function_details)\n",
    "\n",
    "# Or use the function directly\n",
    "\n",
    "academic_retriever.func(**args)\n",
    "# academic_retriever(project_config, message, n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.functions.get_pdfs import GetPDFs\n",
    "\n",
    "args = {\n",
    "\"urls\": [\"http://arxiv.org/pdf/2305.13267v1\", \"http://arxiv.org/pdf/2305.06530v1\"],\n",
    "\"reasons\": ['factual_check'] * 2\n",
    "}\n",
    "   \n",
    "# Usage example\n",
    "get_pdfs_function = GetPDFs(project_config=project_config)\n",
    "# Get the function details\n",
    "function_details = get_pdfs_function.get_function_details()\n",
    "print(function_details)\n",
    "\n",
    "# Now you can use function_details with your agent\n",
    "# agent.equip_function(function_details)\n",
    "\n",
    "# Or use the function directly\n",
    "result = get_pdfs_function.func(**args)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.functions.get_pdf import GetPDF, get_pdf\n",
    "from typing import get_type_hints\n",
    "args = {\n",
    "\"url\": \"https://arxiv.org/pdf/2110.13711\",\n",
    "\"reason\": \"factual_check\",\n",
    "\"part\": \"full\"\n",
    "}\n",
    "\n",
    "# Usage example\n",
    "get_pdfs_function = GetPDF(project_config=project_config)\n",
    "# Get the function details\n",
    "function_details = get_pdfs_function.get_function_details()\n",
    "print(function_details)\n",
    "\n",
    "# Now you can use function_details with your agent\n",
    "# agent.equip_function(function_details)\n",
    "\n",
    "# Or use the function directly\n",
    "# result = get_pdfs_function.func(**args)\n",
    "hits = get_type_hints(get_pdf, include_extras=True)\n",
    "hits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papers = [f for f in os.listdir(\"./papers\") if os.path.isfile(os.path.join(f\"./papers\", f))]\n",
    "metadata = {\n",
    "    'pdf_url':\"https://doi.org/10.1016/j.mser.2020.100595\",\n",
    "    'title':'Polymer Informatics: Current Status and Critical Next Steps',\n",
    "    'authors':'Lihua Chena,Ghanshyam Pilaniab,Rohit Batrac,Tran Doan Huana,Chiho Kima,Christopher Kuennetha,Rampi Ramprasad',\n",
    "    'published':'2020-03-01',\n",
    "    'updated':'2020-03-01'\n",
    "    }\n",
    "pdf_file = \"/home/alibina/repo/usecases/autosearch/notebooks/papers/1-s2.0-S0927796X2030053X-am.pdf\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    'paper_db': paper_db,\n",
    "    'doc_analyzer': analyzer,\n",
    "    'project_dir': Project_dir,\n",
    "    'db_dir': db_dir,\n",
    "    'config_list': config_list,\n",
    "    'initiate_db': initiate_db\n",
    "}\n",
    "\n",
    "\n",
    "chunk_pdf(pdf_file, metadata, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## url_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.functions.url_check import UrlCheck\n",
    "\n",
    "args = {\n",
    "    \"paper_url\": \"https://arxiv.org/pdf/2107.03012.pdf\",\n",
    "    \"paper_title\": \"From algebra to analysis: new proofs of theorems by Ritt and Seidenberg\"\n",
    "}\n",
    "\n",
    "# Usage example\n",
    "get_pdfs_function = UrlCheck(project_config=project_config)\n",
    "# Get the function details\n",
    "function_details = get_pdfs_function.get_function_details()\n",
    "print(function_details)\n",
    "\n",
    "# Now you can use function_details with your agent\n",
    "# agent.equip_function(function_details)\n",
    "\n",
    "# Or use the function directly\n",
    "url_check_res, message = get_pdfs_function.func(**args)\n",
    "print(url_check_res, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## factal check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.functions.factual_check import FactualCheck\n",
    "\n",
    "\n",
    "# Usage example\n",
    "get_pdfs_function = FactualCheck(project_config=project_config)\n",
    "# Get the function details\n",
    "function_details = get_pdfs_function.get_function_details()\n",
    "print(function_details)\n",
    "\n",
    "# Now you can use function_details with your agent\n",
    "# agent.equip_function(function_details)\n",
    "\n",
    "\n",
    "\n",
    "args = [\n",
    "    {\n",
    "        \"text\": \"The use of neural networks in quantum chemistry, particularly in predicting the properties of molecules and materials, has seen significant advancements. Machine learning models can now compute electronic properties [1][2] and potential energy surfaces [3] with an accuracy that contests traditional quantum chemical methods. Notably, the works of Smith et al. (2017) and Chmiela et al. (2017) have demonstrated how neural networks can predict molecular energies and forces, a process traditionally monopolized by density functional theory (DFT) but at a fraction of the computational cost.\",\n",
    "        \"paper_title\": \"ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost\",\n",
    "        \"paper_url\": \"https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a\",\n",
    "        \"reason\": \"To check the accuracy of the advancement statement regarding neural networks predicting properties in quantum chemistry.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"One major limitation is the quality and quantity of data required to effectively train neural networks. Accurate and diverse datasets of molecular structures and their corresponding properties are essential for developing reliable models, but such data can be scarce and expensive to produce due to the computational resources needed for high-level quantum mechanical calculations.\",\n",
    "        \"paper_title\": \"Machine learning of accurate energy-conserving molecular force fields\",\n",
    "        \"paper_url\": \"http://arxiv.org/pdf/2101.02930v1\",\n",
    "        \"reason\": \"To validate the challenges related to the quality and quantity of data in the context of neural networks within quantum chemistry.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The collaboration between these fields also birthed differentiable programming frameworks for quantum chemistry, like TorchANI and TensorMol [4]. These latter-day frameworks enable researchers to quickly prototype neural networks that learn quantum mechanical laws directly from data, providing a valuable tool to accelerate discovery. This integration has offered promising results in tasks like molecular dynamics simulations, which are key for understanding chemical reactions and material properties.\", \n",
    "        \"paper_title\": \"Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning\", \n",
    "        \"paper_url\": \"http://arxiv.org/pdf/1805.11924v3\", \n",
    "        \"reason\": \"To confirm the factual information about differentiable programming frameworks and their impact on research in quantum chemistry and neural networks.\"\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "for arg in args:    \n",
    "    print(get_pdfs_function.func(**arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
