{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/autoget/lib/python3.12/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from autosearch.functions.text_analysis import chunk_pdf\n",
    "\n",
    "from autosearch.database.paper_database import PaperDatabase\n",
    "from autosearch.analysis.document_analyzer import DocumentAnalyzer\n",
    "from autosearch.research_project import ResearchProject\n",
    "from autosearch.write_blog import WriteBlog\n",
    "\n",
    "import autogen\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure credentials from environment variables\n",
    "config={\n",
    "    'doc_api_key': os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\"),\n",
    "    'doc_endpoint': os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "}\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Exploring the Intricacies of Polymer Representation: Unraveling Complexity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging session ID: fe1dd17f-70f0-4e58-adeb-67694cb5b0d2\n",
      "Equipping function 'academic_retriever' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'academic_search' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'get_pdf' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'get_pdfs' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'factual_check' to agent 'content_strategist'\n",
      "Equipping function 'academic_retriever' to agent 'content_strategist'\n",
      "Equipping function 'academic_search' to agent 'content_strategist'\n",
      "Equipping function 'get_pdf' to agent 'content_strategist'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The return type of the function 'wrapper' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equipping function 'get_pdfs' to agent 'content_strategist'\n",
      "Equipping function 'write_section' to agent 'writing_coordinator'\n",
      "Equipping function 'factual_check' to agent 'content_review_specialist'\n",
      "Equipping function 'academic_retriever' to agent 'content_review_specialist'\n",
      "Equipping function 'academic_search' to agent 'content_review_specialist'\n",
      "Equipping function 'get_pdf' to agent 'content_review_specialist'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The return type of the function 'wrapper' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equipping function 'plot_figure' to agent 'visualization_specialist'\n",
      "Equipping function 'academic_search' to agent 'topic_expert'\n",
      "Equipping function 'academic_retriever' to agent 'topic_expert'\n",
      "Equipping function 'academic_search' to agent 'research_resource_expert'\n",
      "Equipping function 'academic_retriever' to agent 'research_resource_expert'\n",
      "Equipping function 'get_pdf' to agent 'research_resource_expert'\n",
      "Processing local PDFs...\n",
      "Error: The folder ./papers does not exist.\n"
     ]
    }
   ],
   "source": [
    "test_project = ResearchProject(\n",
    "    project_id = \"project_test\",\n",
    "    version= \"0.2\",\n",
    "    config=config,\n",
    "    config_file=\"OAI_CONFIG_LIST-sweden-505\",\n",
    "    initiate_db= False,\n",
    "    funcClsList = [\"FactualCheck\", \"GetPDF\", \"GetPDFs\", \"UrlCheck\", \"AcademicRetriever\", \"AcademicSearch\", \"WriteSection\", \"PlotFigure\"],\n",
    "    communiteList = [\"outline_agents\", \"write_section_agents\", \"instructor_agents\"],\n",
    "    local_papers_dir=\"./papers\",\n",
    "    models = [\"gpt-35-turbo\", \"gpt-35-turbo-16k\"]\n",
    ")\n",
    "project_config = test_project.ProjectConfig\n",
    "# print(project_config.logging_session_id)\n",
    "# test_project.run(\n",
    "#     title=title,\n",
    "#     target_audience=\"expert in experimental polymer science and machine learning experts\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files_in_directory(directory):\n",
    "    try:\n",
    "        # List all files in the given directory\n",
    "        files = os.listdir(directory)\n",
    "        # Filter out directories, only keep files\n",
    "        files = [f[:-4] for f in files if os.path.isfile(os.path.join(directory, f)) and f.endswith(\".txt\")]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.api.arxiv_api import ArxivAPI\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "def process_single_paper(arxiv_id: str, ProjectConfig, max_token_size: int, reference: bool = False) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a single arXiv paper and return its chunks.\n",
    "    \"\"\"\n",
    "    arxiv_api = ArxivAPI()\n",
    "    try:\n",
    "        # Fetch the paper metadata\n",
    "        paper = arxiv_api.get_paper_metadata(arxiv_id)\n",
    "        \n",
    "        # Get the chunks\n",
    "        chunks = ProjectConfig.doc_analyzer.pdf2md_chunk(paper, max_token_size, reference)\n",
    "        \n",
    "        paper_data = []\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk_data = {\n",
    "                'arxiv_id': arxiv_id,\n",
    "                'page_content': chunk.page_content,\n",
    "            }\n",
    "            \n",
    "            # Unfold the metadata\n",
    "            if hasattr(chunk, 'metadata'):\n",
    "                for key, value in chunk.metadata.items():\n",
    "                    # Convert complex types to JSON strings\n",
    "                    if isinstance(value, (list, dict, np.integer, np.floating, np.ndarray)):\n",
    "                        chunk_data[f'metadata_{key}'] = json.dumps(value, cls=NumpyEncoder)\n",
    "                    else:\n",
    "                        chunk_data[f'metadata_{key}'] = value\n",
    "            \n",
    "            # Add paper information\n",
    "            paper_dict = paper.to_dict()\n",
    "            for k, v in paper_dict.items():\n",
    "                if isinstance(v, (list, dict, np.integer, np.floating, np.ndarray)):\n",
    "                    chunk_data[k] = json.dumps(v, cls=NumpyEncoder)\n",
    "                else:\n",
    "                    chunk_data[k] = v\n",
    "            \n",
    "            paper_data.append(chunk_data)\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing paper {arxiv_id}: {str(e)}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 21 papers...\n",
      "Error processing paper 751: Invalid ArXiv identifier or URL: 751\n",
      "Processed paper: 751\n",
      "Loading analysis result from ./project_test/0.2/output/json/2205.13757v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2103.14174v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2311.15481v3.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2105.05278v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/0912.3344v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2209.13557v2.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2311.14744v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2102.08134v2.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2109.02794v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2209.01307v4.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2406.04727v2.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2205.08619v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2212.08945v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2011.00508v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2010.07683v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/1812.11212v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2209.14803v1.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2403.20021v2.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/2312.04013v3.pdf.json\n",
      "Loading analysis result from ./project_test/0.2/output/json/1002.2059v1.pdf.json\n",
      "Created 17 docs with a total of 7976 tokens. Largest doc has 1116 tokens.\n",
      "Processed paper: 2102.08134v2\n",
      "Created 15 docs with a total of 5355 tokens. Largest doc has 899 tokens.\n",
      "Processed paper: 2311.14744v1\n",
      "Created 49 docs with a total of 20026 tokens. Largest doc has 1118 tokens.\n",
      "Processed paper: 2011.00508v1\n",
      "Created 17 docs with a total of 8494 tokens. Largest doc has 1197 tokens.\n",
      "Processed paper: 2209.13557v2\n",
      "Created 16 docs with a total of 9363 tokens. Largest doc has 1199 tokens.\n",
      "Processed paper: 2010.07683v1\n",
      "Created 8 docs with a total of 3450 tokens. Largest doc has 1184 tokens.\n",
      "Processed paper: 1002.2059v1\n",
      "Created 9 docs with a total of 4261 tokens. Largest doc has 1198 tokens.\n",
      "Processed paper: 1812.11212v1\n",
      "Created 19 docs with a total of 9950 tokens. Largest doc has 1188 tokens.\n",
      "Processed paper: 2312.04013v3\n",
      "Created 18 docs with a total of 10106 tokens. Largest doc has 1185 tokens.\n",
      "Processed paper: 0912.3344v1\n",
      "Created 7 docs with a total of 4122 tokens. Largest doc has 1199 tokens.\n",
      "Processed paper: 2103.14174v1\n",
      "Created 19 docs with a total of 8222 tokens. Largest doc has 1121 tokens.\n",
      "Created 23 docs with a total of 10638 tokens. Largest doc has 1160 tokens.\n",
      "Processed paper: 2205.13757v1\n",
      "Processed paper: 2205.08619v1\n",
      "Created 19 docs with a total of 9886 tokens. Largest doc has 1195 tokens.\n",
      "Processed paper: 2209.14803v1\n",
      "Created 27 docs with a total of 13612 tokens. Largest doc has 1200 tokens.\n",
      "Processed paper: 2406.04727v2\n",
      "Created 22 docs with a total of 11668 tokens. Largest doc has 1193 tokens.\n",
      "Processed paper: 2105.05278v1\n",
      "Created 11 docs with a total of 7998 tokens. Largest doc has 1191 tokens.\n",
      "Processed paper: 2403.20021v2\n",
      "Created 34 docs with a total of 23516 tokens. Largest doc has 1196 tokens.\n",
      "Processed paper: 2311.15481v3\n",
      "Created 18 docs with a total of 13880 tokens. Largest doc has 1197 tokens.\n",
      "Processed paper: 2109.02794v1\n",
      "Created 30 docs with a total of 15560 tokens. Largest doc has 1198 tokens.\n",
      "Processed paper: 2209.01307v4\n",
      "Created 66 docs with a total of 55774 tokens. Largest doc has 1197 tokens.\n",
      "Processed paper: 2212.08945v1\n",
      "Data saved to ./graphrag/input/arxiv_papers_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "def process_arxiv_papers(arxiv_ids: List[str], ProjectConfig, output_dir: str = \".\", max_workers: int = 5, max_token_size: int = 1200, reference: bool = False):\n",
    "    \"\"\"\n",
    "    Process a list of arXiv papers in parallel, extract chunks, and combine them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    arxiv_ids (List[str]): List of arXiv IDs to process.\n",
    "    output_dir (str): Directory to save the CSV file. Defaults to current directory.\n",
    "    max_workers (int): Maximum number of worker threads. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Combined DataFrame of all chunks from all papers.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_arxiv_id = {executor.submit(process_single_paper, arxiv_id, ProjectConfig, max_token_size, reference): arxiv_id for arxiv_id in arxiv_ids}\n",
    "        for future in as_completed(future_to_arxiv_id):\n",
    "            arxiv_id = future_to_arxiv_id[future]\n",
    "            try:\n",
    "                paper_data = future.result()\n",
    "                all_data.extend(paper_data)\n",
    "                print(f\"Processed paper: {arxiv_id}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"Paper {arxiv_id} generated an exception: {exc}\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_chunks = pd.DataFrame(all_data)\n",
    "    # drop column \"id\"\n",
    "    df_chunks = df_chunks.drop(columns=[\"id\"])  \n",
    "      \n",
    "    # Save to CSV\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_filename = os.path.join(output_dir, f\"arxiv_papers_chunks.csv\")\n",
    "    df_chunks.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "  \n",
    "    return df_chunks, csv_filename\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "paper_dir = \"/home/azureuser/autogen_uscases/autosearch/notebooks/graphrag/input-txt\"\n",
    "# arxiv_ids = list_files_in_directory(paper_dir)\n",
    "arxiv_ids = ['2209.14803v1', '2103.14174v1', '2105.05278v1', '2403.20021v2', '1002.2059v1', '2205.08619v1', '2311.14744v1', '2109.02794v1', '2102.08134v2', '2209.13557v2', '2209.01307v4', '2205.13757v1', '0912.3344v1', '2311.15481v3', '751', '2212.08945v1', '2011.00508v1', '2312.04013v3', '2406.04727v2', '2010.07683v1', '1812.11212v1']\n",
    "\n",
    "print(f\"Processing {len(arxiv_ids)} papers...\")\n",
    "output_directory = \"./graphrag/input\"\n",
    "max_workers = 20\n",
    "\n",
    "# Run the process\n",
    "df_chunks, csv_filename = process_arxiv_papers(arxiv_ids, project_config, output_directory, max_workers, max_token_size=1200, reference=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
