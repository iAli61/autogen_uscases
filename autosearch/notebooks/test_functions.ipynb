{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "init"
    ]
   },
   "outputs": [],
   "source": [
    "from autosearch.functions.get_pdfs import get_pdfs, initialize_get_pdfs\n",
    "from autosearch.functions.get_pdf import get_pdf, initialize_get_pdf\n",
    "from autosearch.functions.text_analysis import chunk_pdf\n",
    "from autosearch.functions.url_check import url_check\n",
    "\n",
    "from autosearch.database.paper_database import PaperDatabase\n",
    "from autosearch.analysis.document_analyzer import DocumentAnalyzer\n",
    "from autosearch.functions.create_teachable_groupchat import create_teachable_groupchat\n",
    "\n",
    "import autogen\n",
    "from typing import List, Dict, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure credentials from environment variables\n",
    "api_key = os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\")\n",
    "endpoint = os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'gpt-4', 'api_key': '38ae4759658a4466b454666531283601', 'api_type': 'azure', 'base_url': 'https://aoai-gpt4-505.openai.azure.com/', 'api_version': '2023-08-01-preview'}, {'model': 'gpt-4-32k', 'api_key': '38ae4759658a4466b454666531283601', 'base_url': 'https://aoai-gpt4-505.openai.azure.com/', 'api_type': 'azure', 'api_version': '2023-08-01-preview'}]\n"
     ]
    }
   ],
   "source": [
    "Project_dir = \"./project_test\"\n",
    "os.makedirs(Project_dir, exist_ok=True)\n",
    "paperdb_dir = f\"{Project_dir}/paperdb\"\n",
    "db_dir = f\"{Project_dir}/db\"  \n",
    "global initiate_db \n",
    "initiate_db = False\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-32k\"]#, \"gpt-4\", \"gpt-35-turbo-16k\", \"gpt-4-0613\", \"gpt-3.5-turbo\", \"gpt-35-turbo\", \"gpt-35-turbo-0613\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "print(config_list)\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,  # config_list should be defined or imported\n",
    "    \"timeout\": 120,\n",
    "    # \"seed\": 42,\n",
    "}\n",
    "\n",
    "paper_db = PaperDatabase(paperdb_dir)\n",
    "\n",
    "# Initialize the DocumentAnalyzer\n",
    "analyzer = DocumentAnalyzer(api_key, endpoint, project_dir=Project_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_teachable_groupchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate_db\n",
    "prompt = \"\"\"\n",
    "For each memorization task, initiate your process with 'MEMORIZE_ARTICLE:'  \n",
    "Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. \n",
    "Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. \n",
    "Upon choosing to memorize, you MUST finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\n",
    "\"\"\"\n",
    "instract_assistant, instract_user = create_teachable_groupchat(\"instract_assistant\", \"instract_user\", db_dir, config_list, verbosity=3)\n",
    "\n",
    "instract_user.initiate_chat(instract_assistant, silent=True, message=prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_55019/2254256996.py\", line 16, in <module>\n",
      "    get_pdfs(**args)\n",
      "  File \"/home/alibina/repo/usecases/autosearch/src/autosearch/functions/get_pdfs.py\", line 49, in get_pdfs\n",
      "ValueError: Global configuration not initialized. Call initialize_global_config first.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# # Initialize the global configuration\n",
    "initialize_get_pdfs(\n",
    "    paper_db=paper_db,\n",
    "    doc_analyzer=analyzer,\n",
    "    project_dir=Project_dir,\n",
    "    db_dir=db_dir,\n",
    "    config_list=config_list,\n",
    "    initiate_db=initiate_db\n",
    ")\n",
    "\n",
    "args = {\n",
    "\"urls\": [\"http://arxiv.org/pdf/2305.13267v1\", \"http://arxiv.org/pdf/2305.06530v1\"],\n",
    "\"reasons\": ['factual_check'] * 2\n",
    "}\n",
    "   \n",
    "get_pdfs(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 48 docs with a total of 13306 tokens. Largest doc has 2014 tokens.\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 23\n",
      "Insert of existing embedding ID: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 24\n",
      "Insert of existing embedding ID: 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 27\n",
      "Insert of existing embedding ID: 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 28\n",
      "Insert of existing embedding ID: 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "Error: 'NoneType' object is not subscriptable\n",
      "\u001b[31mtext: ## 3.4.3 CIFAR-10\n",
      "\n",
      "\n",
      "\n",
      "CIFAR-10 (Krizhevsky, 2009) is an image dataset consisting of 60000 images of size 32x32. We use this dataset primarily for our ablations (Section 4). Due to the relatively small number of examples compared to ImageNet, models reach convergence after 100k steps.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 33\n",
      "Insert of existing embedding ID: 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 33\n",
      "Insert of existing embedding ID: 33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 39\n",
      "Insert of existing embedding ID: 39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Abstract\\n\\n\\n\\nTransformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long se- quences, which allows them to produce long coherent outputs: entire paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessi- bility. We postulate that having an explicit hierarchical architecture is the key to Trans- formers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierar- chical. We use the best performing upsam- pling and downsampling layers to create Hour- glass - a hierarchical Transformer language model. Hourglass improves upon the Trans- former baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In partic- ular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 gen- eration task and improves language modeling efficiency on the widely studied enwik8 bench- mark.\\n\\nclose-by in the generated sequence, like in summa- rization, where the summary may need to refer to information scattered across the context, or in large- scale image generation, where pixels belonging to the same object may be far apart in the generation order. Transformers excel at such tasks thanks to self-attention, and they are used with longer and longer contexts.\\n\\n6@1\\n\\n1.18\\n\\nTransformer-XL Hourglass\\n\\n1.16\\n\\n8@\\n\\n2@1 4@4 2@1\\n\\n1.14\\n\\nBit per character on enwik8 valid set\\n\\narXiv:2110.13711v2 [cs.LG] 16 Apr 2022\\n\\n## 1 Introduction\\n\\n\\n\\nTransformer models (Vaswani et al., 2017) are ca- pable of solving many sequence modeling tasks, including classical NLP tasks (Devlin et al., 2019), summarization (Zhang et al., 2020), language mod- eling (Radford et al., 2019; Brown et al., 2020), code generation (Chen et al., 2021), or even mu- sic generation (Huang et al., 2018; Dhariwal et al., 2020) and image generation (Parmar et al., 2018; Chen et al., 2020; Ramesh et al., 2021). One com- pelling feature of Transformers is their ability to handle long contexts given as part of the input. This is particularly visible in tasks where the out- put depends on parts of the context that may not be\\n\\n*Equal contribution. Order determined by coin toss.\\n\\n2@1 8@4 2@1\\n\\n10@1\\n\\n1.12 - 2@14@32@1\\n\\n12@1\\n\\n3@18@4 3@1\\n\\n14@1\\n\\n2@18@3 2@1\\n\\n1.10\\n\\n4@18@44@1\\n\\n2@1 16@3 2@1\\n\\n1.08\\n\\n4@1 3@3 4@6 3@3 4@1\\n\\n5@1 8@2 5@1\\n\\n1.0 1.2 1.4 1.6 1.8\\n\\n2.0\\n\\n2.2 2.4\\n\\nSeconds per one training step\\n\\nFigure 1: Bits-per-character vs. training cost for base- line (orange) and hierarchical Transformers (green). We observe significant perplexity improvements on en- wik8 over the vanilla Transformer-XL baseline, see text for details.\\n\\nThe ability of Transformers to handle long con- texts comes at a price: each self-attention layer, at least in its original form, has complexity quadratic in the length of the context. When a stack of n Transformer layers is used, both memory and time complexity is equal to O(L2n) where L is a se- quence length and n number of decoder blocks. Due to this limitation, vanilla transformers are in- feasible to train on tasks with very long input se- quences, for instance, on high-resolution images. This issue has been studied extensively, and a num- ber of techniques were introduced that modify at- tention mechanism without changing overall trans- former architecture (Child et al., 2019; Roy et al., 2020; Ren et al., 2021). These sparse attention mechanisms reduce the complexity of self-attention\\n\\nbut still force the model to operate on the sequence of the same length as the input.\\n\\nFor generative Transformer models, operating at the original scale of the input sequence is necessary, at least in the early and final layers, as the input must be processed at first and generated at the end (Section 4.3). But forcing the models to operate at this granularity throughout the layer stack has both fundamental and practical shortcomings:\\n\\n· Fundamentally, we aim for the models to cre- ate high-level representations of words, enti- ties, or even whole events - which occur at a very different granularity than single letters that the model receives on input.\\n\\n· On the practical side, even layers with linear complexity can be slow and memory-intensive when processing very long sequences.\\n\\nTo alleviate these issues, we propose to change the Transformer architecture to first shorten the in- ternal sequence of activations when going deeper in the layer stack and then expand it back before generation. We merge tokens into groups using a shortening operation (Section 2.1) and so reduce the overall sequence length, and then up-sample them again combining with the sequence from ear- lier layers (Section 2.3), The first part is analogous to the Funnel-Transformer architecture (Dai et al., 2020), and the whole architecture takes inspiration from U-Nets (Ronneberger et al., 2015). In contrast to both these architectures, the model we present is autoregressive, which is harder to ensure in hierar- chical models than in vanilla Transformers.\\n\\nThe resulting model - which we call Hourglass - is an autoregressive Transformer language model that operates on shortened sequences. It yields significant performance improvements for different attention types (Fig. 6,7). We tested Hourglass with Transformer-XL (Dai et al., 2019) and Reformer (Kitaev et al., 2020) blocks on enwik8 dataset. In both cases, it is not only better in terms of perplex- ity, but it is faster and uses less memory during training. We also propose a regularization tech- nique for hierarchical Transformers called shorten factor dropout which improves perplexity upon baselines trained with fixed shorten factor (see Sec- tion 4.1). Finally, Hourglass achieves the new state- of-the-art among Transformer models for image generation of ImageNet32 (see Tab. 3).\\n\\n## 2 Model\\n\\n\\n\\nStandard self-attention mechanism uses full token- level sequence representations. In the Hourglass, we bring efficiency to the model by utilizing short- ening, which allows us to use the Transformer lay- ers on inputs with significantly smaller lengths. A high-level overview of our proposed model archi- tecture is shown in figures 2 and 3.\\n\\nAttention type in the vanilla layers and shortened layers is a configurable parameter. By default we use relative attention defined in Transformer-XL (Dai et al., 2019). Any attention module can be used - we show significant efficiency gains when applying Hourglass also for LSH (Kitaev et al., 2020) attention (see Section 3.2 and Fig. 7).\\n\\n## 2.1 Methods of shortening the input sequence\\n\\n\\n\\nShortening can be defined as any function S that accepts a tensor x of shape (l, d) and returns a ten- sor x\\' of shape (+, d), where k is a hyperparameter called shorten factor.\\n\\nA simple shortening method is 1D average pool- ing with stride k and pool size k, applied along the sequence dimension l. Another way of shortening is what we will further call linear pooling (l and d denote sequence length and dmodel):\\n\\n## Algorithm 2 LinearPooling\\n\\n\\n\\nx\\' ~ Reshape(x, (Z, k . d) x\\' ~ Linear Projection(x\\')\\n\\nShortening can be also performed by attention, as was introduced in (Dai et al., 2020): x\\' = S(x)+ Attention(Q = S(x), K = V = x) where S is shortening function, originally S = AvgPool. Directly after this attention operation, a position- wise feed-forward with a residual is performed, so that these two layers form a Transformer block (Vaswani et al., 2017). In this work we also try S = Linear Pool and find it more effective on image tasks (see Tab. 8).\\n\\n## 2.2 Shortening and autoregressive property\\n\\n\\n\\nInformation leaks Shortening interferes with the standard causal masking used in Transformer decoders. Namely, in any shortened representation by a factor of k each shortened token contributes to predicting up to the next k tokens in the finest scale, that is if e is the shortened sequence and x is the sequence on the finest scale, e0 is not only used\\n\\nInput tokens\\n\\nOutput tokens\\n\\nPre Vanilla Layers\\n\\nShortened1 Layers\\n\\nPost Vanilla Layers\\n\\nShortened1 Layers\\n\\nShortened2 Layers\\n\\n:unselected: 000 :unselected: :unselected:\\n\\nShortening, sf = k1\\n\\nsf = k2\\n\\nUpsampling, sf = k1\\n\\n:unselected: OOO :unselected:\\n\\n:unselected:\\n\\nsf=k2\\n\\n000\\n\\nL/k1k2 tokens\\n\\nL/k1 tokens\\n\\nL tokens\\n\\nL/k1 tokens\\n\\nL tokens\\n\\nFigure 2: Hourglass - a high-level architecture overview. The arrows denote residual connections.\\n\\nto generate x0; in fact, the same embedding is used to generate tokens xo, ... , Ck-1.\\n\\nTherefore, we need to guarantee that e0 and any other ei cannot access information about tokens they will implicitly predict. To ensure that, we apply another shift right by k - 1 tokens, directly before any shortening by a factor of k (Fig. 4). The shift is the smallest that does not cause an informa- tion leak (see Fig. 5 for an example of a shifting that leads to a leak). We included a more detailed analysis of this fact in the Appendix (Section A.2).\\n\\nReduced expressivity Let us consider an Hour- glass model with shortening by a factor of k and no transformer blocks operating on the finest scale (that is, a model without vanilla layers).\\n\\nIn this situation\\n\\nP(x) = IT0 P(xileo, ... , e| + ]) =\\n\\nI2 P(x; 20, ... , [ 4 ].k-1)\\n\\nbecause for predicting xi we combine the pro- cessing done on shortened representations e with token-independent operations. This means token Xi is generated independently from the tokens X 4 .k. ... , Xi-1. This situation is detrimental to the model\\'s capabilities, though including at least one vanilla layer solves this issue. In the Appendix we provide a detailed example illustrating this prob- lem (Section A.1).\\n\\n## 2.3 Upsampling methods\\n\\n\\n\\nUpsampling is a crucial part of the Hourglass ar- chitecture since we need to convert shortened rep- resentations back to the full token-level sequence in order to perform language modeling.\\n\\nA method proposed in (Dai et al., 2020) is re- peating each shortened vector shorten factor times. This method is computationally efficient, but it does not distinguish tokens with respect to position inside the group.\\n\\nAnother method is linear upsampling which works analogously to linear pooling - it projects vectors of shape ({, d) to ({, k . d) and then re- shapes to l vectors, each of dimension d. This method is fast and allows to project shortened em- beddings differently for each position in the group. This happens because the (k · d) × d projection ma- trix can be thought of as k separate d × d matrices, one per each position.\\n\\nWe also investigated a method which we further call attention upsampling. It is similar to atten- tion pooling (Dai et al., 2020) and to the aggre- gation layer from (Subramanian et al., 2020). It works as follows: x = U(x, x\\') + Attention(Q = U(x, x\\'), K = V = x\\') where x are embeddings from just before the shortening, x\\' are final short- ened embeddings and U is an arbitrary upsampling function. After the attention operation there is also a residual with a feed-forward layer.\\n\\nLinear upsampling learns a fixed pattern that is the same for each shortened token. Attention upsampling has the advantage of being content- based - each token can extract relevant infor- mation from the shortened embeddings. We set U(x, x\\') = x + LinearUpsampling(x\\') which allows to explicitly inject group-level information into the attention queries. We experimentally show that variants of attention upsampling lead to the best results for our model across different datasets (see Tab. 7).\\n\\n## Algorithm 1 HourglassLM\\n\\n\\n\\n## procedure HOURGLASS(x, [k, ... s_factors])\\n\\n\\n\\nx + PreVanillaLayers(x) x\\' < Shortening(ShiftRight(x, k-1), k)\\n\\nif EMPTY(s_factors) then x\\' < ShortenedLayers(x\\') else\\n\\nx\\' + HOURGLASS(x\\', s_factors) end if\\n\\na+ x + Upsampling(x, x\\', k) x + PostVanillaLayers(x) return x\\n\\nFigure 3: The architecture starts with pre vanilla layers - a stack of Transformer blocks operating on the full token-level sequence. After them we insert shortening layer where k is the shorten factor parameter (Fig. 4). The sequence is shifted right before shortening to pre- vent information leak (Fig. 5). Then we recursively insert another Hourglass block operating on k times smaller scale. On the final level of shortening, we ap- ply shortened layers - Transformer blocks operating on the smallest scale. Upsampling layer brings the result- ing activations x\\' back to the original resolution. After upsampling and residual, the activations are processed by token-level post vanilla layers.\\n\\n## 3 Experiments\\n\\n\\n\\nIn this section, we present experimental results of Hourglass. We start with a quick analysis of time and memory complexity of the approach (Section 3.1). Then we investigate the efficiency gains of applying Hourglass to Transformers with different attention types (Section 3.2). Finally, we use Hour- glass with relative attention parametrization from Transformer-XL (Dai et al., 2019), evaluate it on three language modeling tasks, and compare the results with other models. (Sections 3.3, 3.4)\\n\\nTo show cross-domain generalization of our method, we train our model on one dataset related to Natural Language Processing and two from the Computer Vision field.\\n\\nTo ensure consistency in presenting config- urations of our model, we introduce a nota- tion describing hierarchy of our architecture: (N1@f1, ... , Nk@fk) where each entry (Nj@fj) means Nj layers shortened by factor fj.\\n\\nOur model implementation is open source.1\\n\\n1github. com/google/trax/blob/master/trax/models/research/hourglass.py\\n\\n:unselected:\\n\\n:unselected: :unselected:\\n\\n:unselected:\\n\\n:unselected: 000\\n\\nInitial ShiftRight(1)\\n\\n:selected: :unselected: :unselected:\\n\\nShiftRight(sf-1)\\n\\n:selected: :selected: :selected:\\n\\n000\\n\\nShortening\\n\\n000\\n\\nFigure 4: An overview of our shortening approach. Different colors denote token positions. Initially, we shift right by one, which is a standard step in TransformerLM. Then, just before performing short- ening, we additionally shift the tokens right by shorten factor - 1 to preserve the autoregressive prop- erty of the model.\\n\\nShiftRight(sf-2)\\n\\n:selected: :selected:\\n\\n:unselected:\\n\\n000\\n\\nShortening\\n\\n:selected:\\n\\nUpsampling\\n\\n000\\n\\nFigure 5: An example of information leak. If the shift right factor is too small, after upsampling the knowl- edge from the next tokens leaks to previous ones vio- lating autoregressiveness and making decoding impos- sible.\\n\\n## 3.1 Computational cost analysis\\n\\n\\n\\nIn vanilla Transformers, the number of parameters can indicate the computation required to train the model. This is not true for Hourglass - for instance, it can have 128 layers operating on a sequence shortened by 32 and still fit into the memory of a single GPU. A weak correlation between true Hourglass\\' computational cost and its number of parameters can be observed in Table 1.\\n\\nHourglass achieves the biggest speedup with the standard O((2) attention. In that case, a single shortening by a shorten factor k reduces the complexity to (12) so by a factor of k2. For more recent linear-time attention mechanisms (Katharopoulos et al., 2020; Choromanski et al., 2021) the reduction would be smaller - but still by a factor of k. Feed-forward layers also have linear complexity so shortening reduces it by a factor of k.\\n\\nIn Table 1 we show an empirical efficiency com- parison between Hourglass and Transformer-XL.\\n\\nHierarchy\\n\\nBPC\\n\\nGB\\n\\nSpeed\\n\\n#Param\\n\\n6@1 (Baseline)\\n\\n1.182\\n\\n4.53\\n\\n0.95\\n\\n21M\\n\\n2@11@32@1\\n\\n1.163\\n\\n4.41\\n\\n1.11\\n\\n24M\\n\\n2@14@42@1\\n\\n1.143\\n\\n4.41\\n\\n1.10\\n\\n34M\\n\\n8@1 (Baseline)\\n\\n1.151\\n\\n5.75\\n\\n0.73\\n\\n28M\\n\\n2@14@32@1\\n\\n1.128\\n\\n4.88\\n\\n1.00\\n\\n34M\\n\\n2@18@42@1\\n\\n1.128\\n\\n4.98\\n\\n0.99\\n\\n48M\\n\\n2@11@24@41@22@1\\n\\n1.115\\n\\n4.69\\n\\n0.86\\n\\n48M\\n\\n2@18@32@1\\n\\n1.111\\n\\n5.50\\n\\n0.88\\n\\n48M\\n\\n10@1 (Baseline)\\n\\n1.128\\n\\n6.99\\n\\n0.56\\n\\n34M\\n\\n3@18@43@1\\n\\n1.109\\n\\n6.14\\n\\n0.76\\n\\n55M\\n\\n12@1 (Baseline)\\n\\n1.115\\n\\n8.12\\n\\n0.47\\n\\n41M\\n\\n4@18@44@1\\n\\n1.098\\n\\n7.20\\n\\n0.62\\n\\n62M\\n\\n2@116@32@1\\n\\n1.096\\n\\n5.89\\n\\n0.71\\n\\n75M\\n\\n14@1 (Baseline)\\n\\n1.102\\n\\n9.35\\n\\n0.40\\n\\n48M\\n\\n5@18@25@1\\n\\n1.079\\n\\n9.57\\n\\n0.45\\n\\n69M\\n\\nBit per character on enwik8 valid set\\n\\nTable 1: Efficiency comparison between Hourglass variants and Transformer-XL baseline on enwik8 - we report validation set perplexity (BPC), running memory (GB) and number of training steps per second (Speed). We observe significant perplexity gains over the base- line for a matching computation cost. It is also vis- ible that for Hourglass the number of model parame- ters (#Param) correlates poorly with true computational cost.\\n\\n## 3.2 Impact of Hourglass\\n\\n\\n\\nTo demonstrate the efficiency of Hourglass, we measured how computational cost decreases and perplexity improves, purely adding the technique to Transformer-XL (Dai et al., 2019) and Re- former (Kitaev et al., 2020) backbones (results de- picted in Figures 6 and 7, respectively).\\n\\n6\\n\\nTransformer-XL Hourglass\\n\\n1.18\\n\\n1.16\\n\\n1.14\\n\\n2@18@4 2@1\\n\\n10\\n\\n3@1 8@4 3@1\\n\\n1.12\\n\\n12\\n\\n1.10\\n\\n2@1 16@3 2@1\\n\\n4@1 3@3 4@6 3@3 4@1\\n\\n14\\n\\n5@18@2 5@1\\n\\n1.08 5 6\\n\\n8 9 Maximum observed memory during training [GB] 7\\n\\nFigure 6: Comparison between Transformer-XL base- line and Hourglass on Enwik8 valid set w.r.t. maximum memory used during training. All models are trained for 200k steps with the same hyperparameters.\\n\\n1.275\\n\\n6\\n\\nBaseline (LSH) Hourglass (LSH)\\n\\n1.250\\n\\n--\\n\\n1.225\\n\\n2@16@32@1 9\\n\\nBit per character on enwik8 valid set\\n\\nIn both cases, models are implemented under the same codebase and the only difference between Hourglass and its corresponding baseline is the us- age of shortening and upsampling layers. We show that by incorporating a single shortening of the in- put, we can train larger models with the same mem- ory requirements and training speed and achieve better perplexity than baselines.\\n\\n## 3.3 Enwik8\\n\\n\\n\\nEnwik8 (Mahoney, 2011) is a byte-level language modeling benchmark containing the first 100M bytes of unprocessed English Wikipedia text, split into 90M train, 5M valid, and 5M test sets.\\n\\nSimilarly to (Dai et al., 2019) and (Beltagy et al., 2020), we evaluate our model on the test set, splitting it into overlapping sequences of size l = 4096 with a step size of 128 and calcu- late the test loss only over the last 128 tokens. With a (4@1, 8@3, 4@1) hierarchy, dmodel = 768, dff = 3072 and 8 heads, we reach 0.98 test bits- per-character.\\n\\n1.200\\n\\n12\\n\\n1.175\\n\\n3@19@33@1\\n\\n16\\n\\n1.150\\n\\n4@112@34@1\\n\\n20\\n\\n1.125\\n\\n6@115@36@1\\n\\n0.6 0.8\\n\\n1.0 1.2\\n\\nSeconds per one training step\\n\\n1.4 1.6\\n\\nFigure 7: Comparison between Reformer baseline and Hourglass, both with LSH attention, on Enwik8 valid set w.r.t. cost of one training step in seconds.\\n\\n## 3.4 Image Generation\\n\\n\\n\\nWe use datasets introduced in (van den Oord et al., 2016a) which are downsampled versions of the pop- ular ImageNet. In the autoregressive image genera- tion setup, they consist of respectively 32 × 32 × 3 and 64 × 64 × 3 tokens, corresponding to RGB channels, per image. As the only preprocessing step we flatten the images.\\n\\n## 3.4.1 ImageNet32\\n\\n\\n\\nFor our main result the following hierarchy is used: (3@1,24@3, 3@1). We use dmodel = 512, dff = 2048, 8 attention heads and 0.01 dropout rate. With this configuration we achieve 3.741 bits/dim, yielding the new state-of-the-art among autoregres- sive (Transformer-based) models on this dataset, compared to the previous state-of-the-art of 3.758 bpd by (Ho et al., 2019).\\n\\nEnwik8\\n\\n#Param\\n\\nBPC\\n\\nTransformer-XL (2019) 24L\\n\\n277M\\n\\n0.99\\n\\nHourglass\\n\\n146M\\n\\n0.98\\n\\nAdaptive-Span (2019) 24L\\n\\n209M\\n\\n0.98\\n\\nTransformer-LS (2021)\\n\\n110M\\n\\n0.97\\n\\nFeedback Transformer (2021)\\n\\n77M\\n\\n0.96\\n\\nExpire-Span (2021) 24L\\n\\n277M\\n\\n0.95\\n\\nTable 2: Enwik8 Results. We report bits-per-character (BPC) on the test set and number of model parameters. Hourglass applied to Transformer-XL significantly out- performs its baseline. Our technique could be also used with other more performant attention methods which we leave for future work.\\n\\nInpu\\n\\nCompletions\\n\\nInput\\n\\nCompletions\\n\\nFigure 8: Examples of our model completions, where bottom half of each image was generated by our model, prompted by the upper half.\\n\\n## 3.4.2 ImageNet64\\n\\n\\n\\nThe sequence length that our model can handle is limited mainly by the computational complexity of used attention module. We replace relative atten- tion in vanilla layers by LSH attention (Kitaev et al., 2020), which allows us to handle 12288-long se- quences. To achieve relative attention parametriza- tion, the LSH attention is combined with rotary positional embeddings (Su et al., 2021). In short- ened layers, standard relative attention is used. For LSH attention, we set chunk length to 128 and use 2 hashes, which results in small memory complex- ity in our full-size layers. In this setup, we reach a score of 3.443 bpd with a (3@1, 12@3, 3@1) ar- chitecture. All attention layers had dmodel = 768, dff = 3072 and 8 heads. No dropout was used.\\n\\n## 3.4.3 CIFAR-10\\n\\n\\n\\nCIFAR-10 (Krizhevsky, 2009) is an image dataset consisting of 60000 images of size 32x32. We use this dataset primarily for our ablations (Section 4). Due to the relatively small number of examples compared to ImageNet, models reach convergence after 100k steps.\\n\\n## 4 Ablations\\n\\n\\n\\nIn this section, we start by introducing a training technique called shorten factor dropout (Section 4.1), and then analyze Hourglass\\'s components de-\\n\\n\\n\\n|ImageNet32|BPD|\\n|---|---|\\n|PixelCNN (van den Oord et al., 2016b)|3.83|\\n|Image Transformer (Parmar et al., 2018)|3.77|\\n|Axial Transformer (Ho et al., 2019)|3.76|\\n|Hourglass|3.74|\\n|VDM (Kingma et al., 2021)|3.72|\\n|DenseFlow (Grcić et al., 2021)|3.63|\\n|ImageNet64|BPD|\\n|Reformer (Kitaev et al., 2020)|3.65|\\n|Performer (Choromanski et al., 2021)|3.64|\\n|Hourglass|3.44|\\n|Sparse Transformer (Child et al., 2019)|3.44|\\n|Routing Transformer (Roy et al., 2020)|3.43|\\n|Combiner (Ren et al., 2021)|3.42|\\n|VDM (2021)|3.40|\\n|DenseFlow (2021)|3.35|\\n\\n\\nTable 3: Bits per Dimension (BPD) on downsampled imagenet. Autoregressive models are separated by a horizontal line from non-autoregressive ones. On Ima- geNet32, our model yields new state-of-the-art for au- toregressive models.\\n\\nscribed above. We show that shortened layers be- have similarly to full token-level layers in terms of scalability (Section 4.2). Then we study the ef- fect of different distributions of (pre, post) vanilla layers on Hourglass\\' accuracy (Section 4.3). We further analyze the performance of various upsam- pling and downsampling methods (Sections 4.4 and 4.5). Finally, we discuss different shorten factors and multi-stage shortening in Section 4.6.\\n\\nWe conduct the ablations on both text and image generation to show applicability across different domains. We report bits per character (BPC) on the enwik8 validation (dev) set evaluated without context (sequence length 2048) and bits per dim (BPD) on the CIFAR-10 test set. For the exact hyperparameter setup refer to the Appendix.\\n\\n## 4.1 Shorten factor dropout\\n\\n\\n\\nDifferent shorten factors can be used for the same model when using parameterless pooling methods. We propose a training procedure where the shorten factor is randomly sampled with uniform distribu- tion from a predefined set in each step. We observe that such a training regime improves validation loss compared to a baseline trained with a single, fixed shorten factor. For example, a model trained with shorten factor randomly sampled from {2, 3} performs better when evaluated with any of these shorten factors, compared to models trained with a corresponding fixed shorten factor (Tab. 4).\\n\\nWe hypothesise that such a technique promotes a more uniform distribution of information over the sequence of tokens. It may be essential for fixed-size pooling techniques as they do not ac-\\n\\ncount for variable length constituents like words. By spreading information uniformly, we prevent a situation where we lose content by shortening three information-dense tokens or lose available capacity by merging three low information ones.\\n\\nShorten factor dropout is not limited to our ar- chitecture and can be applied to any model that utilizes shortening, particularly (Dai et al., 2020).\\n\\n\\n\\n|Hierarchy|Train k|Val k = 2|Val k = 3|\\n|---|---|---|---|\\n|2@18@k2@1|{2,3}|1.104|1.116|\\n||2|1.116||\\n||3||1.124|\\n|4@112@k4@1|{2,3}|1.086|1.094|\\n||2|1.098||\\n||3||1.101|\\n|5@110@k5@1|{2,3}|1.082|1.087|\\n||2|1.096||\\n||3||1.095|\\n\\n\\nTable 4: Comparison between models trained with shorten factor dropout (Train k = {2,3}, Section 4.1) and fixed shorten factor baselines on enwik8.\\n\\n## 4.2 Scaling shortened layers\\n\\n\\n\\nIn this study, we show that layers operating on the shortened sequence contribute significantly to Hourglass\\'s accuracy. In Table 5 we measure the impact of scaling the depth of the shortened part of the model with a fixed number of vanilla layers.\\n\\nWe also check if scaling laws of Transformers, described in (Kaplan et al., 2020), hold by com- paring a regression line fitted to various Hourglass configurations and one fitted to Transformer-XL baseline. We observe in Figure 1 that the slopes are very similar, which indicates that the laws hold.\\n\\n\\n\\n|Number of shortened layers|enwik8|CIFAR-10|\\n|---|---|---|\\n|Baseline (n = 1)|1.164|3.28|\\n|n = 4|1.134|3.16|\\n|n = 8|1.111|3.07|\\n|n = 16|1.096|3.03|\\n\\n\\nTable 5: Impact of increasing the number of shortened layers on perplexity. Vanilla layers: (1, 1) for CIFAR- 10 and (2, 2) for enwik8, shorten factor 3 used in both.\\n\\n## 4.3 Impact of vanilla layers\\n\\n\\n\\nWe observe a significant contribution to Hourglass\\' performance with increasing the number of vanilla layers. One reason is that we perform more compu- tations as in vanilla layers we process the sequence in token-level - no shortening is applied. We also see that the distribution of vanilla layers before shortening and after shortening does impact the training (see Tab. 6), and equal distribution leads to the best perplexity.\\n\\n\\n\\n|Vanilla layers|enwik8|CIFAR-10|\\n|---|---|---|\\n|(0, 0)|1.460|3.429|\\n|(0, 2)|1.176|3.108|\\n|(2, 0)|1.189|3.035|\\n|(1, 1)|1.171|3.012|\\n|(2, 2)|1.128|2.966|\\n\\n\\nTable 6: Impact of the distribution of vanilla layers on enwik8 (BPC) and CIFAR-10 score (BPD). We see that equal distribution of layers before and after shortening leads to better results on both datasets.\\n\\n## 4.4 Upsampling method\\n\\n\\n\\nIn Table 7 we investigate different possibilities of choosing the upsampling method. For attention- free methods, linear upsampling performs better on images, while repeat upsampling works well for text. Attention upsampling works well regardless of the function U and has the lowest perplexity.\\n\\n\\n\\n|Upsampling method|enwik8|CIFAR-10|\\n|---|---|---|\\n|Repeat|1.148|3.062|\\n|Linear|1.163|3.020|\\n|U(x, x\\') = x|1.145|2.967|\\n|U(x, x\\') = x + Linear(x\\')|1.132|3.012|\\n\\n\\nTable 7: Upsampling method ablation - baseline config- urations are (2@1, 24@4, 2@1) and (1@1,8@3,1@1) for enwik8 and CIFAR-10, respectively.\\n\\n## 4.5 Pooling method\\n\\n\\n\\nTable 8 presents impact of pooling method on both enwik8 (BPC) and CIFAR-10 (BPD). Atten- tion pooling reaches the lowest perplexity for both datasets. Average pooling performs well on text among attention-free methods, while linear pool- ing works better for images. Both of these methods perform significantly worse for the other modality. Attention pooling demonstrates small differences with respect to chosen shortening function S (Sec- tion 2.1), still preserving the preference towards linear pooling on images and average pooling on text.\\n\\n\\n\\n|Pooling method|enwik8|CIFAR-10|\\n|---|---|---|\\n|AvgPool|1.129|3.116|\\n|Attention, S = AvgPool|1.124|3.012|\\n|Attention, S = Linear Pool|1.142|2.998|\\n|LinearPool|1.159|2.998|\\n\\n\\nTable 8: Ablation of pooling methods. Attention pool- ing achieves the best perplexity on both datasets.\\n\\n## 4.6 Shortening strategies\\n\\n\\n\\nWhile the analysis above gives a clear indication of what methods to choose for shortening and upsam- pling, we are still left with the question of which shorten factors to use and whether to do single- stage or multi-stage shortening.\\n\\nConsistently, it is beneficial to do at least one shortening and by a factor of at least 3, while keep- ing 2-3 vanilla layers. Beyond that, a number of different configurations can yield similar results. In Table 1 we present the different hierarchical con- figurations that we tested on enwik8 and plotted in Figure 1. It can be seen that configurations with similar computation costs perform similarly. The sequence length used in these experiments is 2048 - we hypothesise that more hierarchy may be bene- ficial with even longer sequences.\\n\\n## 5 Related Work\\n\\n\\n\\nShortening in Transformers Shortening in our work is inspired by Funnel-Transformer (Dai et al., 2020). The key difference is that they train an en- coder model for text classification, where our work is entirely focused on language modeling, which provides additional challenges we had to solve re- garding shortening in the autoregressive setup (Sec- tion 2.2). Another difference is that they use repeat upsampling method while we use attention. There are also a few works related to character-level mod- eling which use shortening, namely (Clark et al., 2021) and (Tay et al., 2021). However, the authors of these works focused mainly on shortening se- quence in encoder part of the transformer, whereas we focused on applying shortening in decoder.\\n\\nThe idea of shortening is also discussed in (Sub- ramanian et al., 2020). However, proposed architec- tures either focus on downsampling or upsampling, while Hourglass is a U-Net-like architecture and is symmetric in these terms. Their models use transformer layers on the finest scales when post- processing final representations. We do these also, in the beginning, to preprocess tokens on the finest scale, and we have found it essential to the score (Section 4.3). Our attention upsampling method is similar to their aggregation layer in the bottom-up model, however we use one upsampling for each scale change while they combine different scales to create one global upsampling.\\n\\nRelative positional encoding Our work is pri- marily built on the backbone of Transformer-XL (Dai et al., 2019) - we use the same relative at- tention parametrization. Instead of the segment- level recurrence mechanism, we use shortening to make our model more efficient and feasible to train on longer sequences. Another relative atten- tion parametrization is RoFormer (Su et al., 2021) where rotary positional embeddings are introduced.\\n\\nWe find this work particularly relevant because the method is compatible with any attention type, in- cluding efficient attention, and can be combined with our model (Section 3.4.2).\\n\\nSparse Attention A well-known approach ad- dressing the memory bottleneck is utilizing sparsity patterns in the attention matrix - Routing (Roy et al., 2020) and Sparse Transformer (Child et al., 2019) are examples of such methods. Our solution is dif- ferent in the sense that it uses full attention - just with shortened sequence length. Combiner (Ren et al., 2021) makes a step further and provides full attention capabilities with similar computational complexity to Routing and Sparse transformers by leveraging structured factorization. This work, sim- ilarly to papers mentioned above on efficient trans- formers, concentrates on speeding up the attention component, while the most important feature of the Hourglass architecture is that it can use any attention module as a drop-in.\\n\\nImage generation on downsampled ImageNet VDM (Kingma et al., 2021) and DenseFlow (Grcić et al., 2021) are recently proposed state-of-the-art methods for density estimation on this dataset. The difference between these methods and Transformer- based methods (Parmar et al., 2018; Ho et al., 2019) including this work is that the former, unlike Trans- formers, are non-autoregressive.\\n\\n## 6 Conclusion\\n\\n\\n\\nIn this paper, we show how hierarchy can improve the efficiency of Transformers in a language mod- eling setup. Our proposed architecture, Hourglass, significantly outperforms the baseline both in terms of perplexity reached at a given computation cost (Figure 1) and empirical metrics like running mem- ory (Figure 6). Hourglass achieves state-of-the-art results among autoregressive models on the Ima- geNet32 generation task and competitive results on other image generation and language modeling tasks.\\n\\nHourglass can be used with any attention type, which opens many directions for future research re- lated to Transformers capable of processing longer sequences. Another line of future work might be related to advances in the shortening mechanism itself, for example, involving a dynamic pooling operation that could explicitly handle the problem of fixed-size groups in multi-stage shortening. We also leave open the problem of choosing the best hi-\\n\\nerarchy for a task. We conjecture that experiments with much longer contexts will provide better guid- ance for this choice and will benefit even more from the Hourglass architecture.\\n\\n## 7 Acknowledgments\\n\\n\\n\\nSome experiments were performed using the Entropy cluster funded by NVIDIA, Intel, the Polish National Science Center grant UMO- 2017/26/E/ST6/00622, and ERC Starting Grant TOTAL. The work of Henryk Michalewski was sup- ported by the Polish National Science Center grant UMO-2018/29/B/ST6/02959. The authors would like to thank Marek Cygan and Kamil Wilczek for their help with cluster setup, and Grzegorz Grudz- iński, Dawid Jamka and Sebastian Jaszczur for helpful discussions. This article describes a Team Programming Project completed at the University of Warsaw in the academic year 20/21. We are grateful to Janusz Jabłonowski, the head of Team Programming Projects, for his support and open- mindedness.\\n\\n## References\\n\\n\\n\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc- Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learn- ers.\\n\\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. 2020. Generative pretraining from pixels. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1691-1703. PMLR.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka- plan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas- try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cum- mings, Matthias Plappert, Fotios Chantzis, Eliza-\\n\\nbeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welin- der, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Eval- uating large language models trained on code.\\n\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers.\\n\\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sar- los, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2021. Rethinking attention with per- formers.\\n\\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2021. Canine: Pre-training an efficient tokenization-free encoder for language representa- tion.\\n\\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc V. Le. 2020. Funnel-transformer: Filtering out sequen- tial redundancy for efficient language processing.\\n\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V. Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing.\\n\\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. 2020. Jukebox: A generative model for music.\\n\\nAngela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. 2021. Address- ing some limitations of transformers with feedback memory.\\n\\nMatej Grcić, Ivan Grubišić, and Siniša Šegvić. 2021. Densely connected normalizing flows.\\n\\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. 2019. Axial attention in multi- dimensional transformers.\\n\\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. 2018. Music transformer.\\n\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario\\n\\nAmodei. 2020. Scaling laws for neural language models.\\n\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap- pas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear at- tention.\\n\\nDiederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. 2021. Variational diffusion models.\\n\\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer.\\n\\nAlex Krizhevsky. 2009. Learning multiple layers of features from tiny images.\\n\\nMatt Mahoney. 2011. Large text compression bench- mark.\\n\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image transformer.\\n\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image gener- ation.\\n\\nHongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. 2021. Combiner: Full attention transformer with sparse computation cost.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedi- cal image segmentation.\\n\\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient content-based sparse attention with routing transformers.\\n\\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yun- feng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.\\n\\nSandeep Subramanian, Ronan Collobert, Marc\\' Aurelio Ranzato, and Y-Lan Boureau. 2020. Multi-scale transformer language models.\\n\\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo- janowski, and Armand Joulin. 2019. Adaptive at- tention span in transformers.\\n\\nSainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. 2021. Not all memories are created equal: Learning to forget by expiring.\\n\\nYi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Si- mon Baumgartner, Cong Yu, and Donald Metzler. 2021. Charformer: Fast character transformers via gradient-based subword tokenization.\\n\\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. 2016a. Pixel recurrent neural net- works. CoRR, abs/1601.06759.\\n\\nAaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. 2016b. Conditional image generation with pixelcnn decoders.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\\n\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe- ter J. Liu. 2020. Pegasus: Pre-training with ex- tracted gap-sentences for abstractive summarization.\\n\\nChen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. 2021. Long-short transformer: Ef- ficient transformers for language and vision.\\n\\n## A Autoregressive shortening\\n\\n\\n\\nIn Section 2.2 we address two problems of short- ening in an autoregressive setup: information leaks and reduced expressivity. Here we study these is- sues in more detail.\\n\\n## A.1 Motivation behind using vanilla layers\\n\\n\\n\\nAt first sight, it may be tempting to create hier- archical models that directly shorten the input to maximize the efficiency gains. In this section, we explain why vanilla layers are crucial for modeling at least some sequences, especially due to autore- gressivity.\\n\\nConsider a sequence modeling task where the input is a random sequence with repeats, such as A#AC#CD#DB#B. The sequence consists of chunks L#L where L is a random uniform letter and # is a special symbol. A vanilla Transformer language model can achieve 66% sequence accu- racy on this task - it cannot predict the token at the beginning of the chunk, but it can predict the last token of the chunk by simply copying the token at 2 positions earlier, which is possible using a vanilla self-attention layer.\\n\\nIt is however not easy to learn this task in a short- ening setup when there are no vanilla layers operat- ing on the finest scale - this is the situation defined in Reduced expressivity subsection of Section 2.2. Assume shorten factor is k = 3 and the input is A#AB#BC#C. To avoid information leak, we shift the input sequence right by 1, and then by k - 1 = 2 directly before shortening. Then the sequence is 000A#AB#B. Our shortened embeddings are as follows: e0 = S(emb0, emb0, emb0), e1 = S(embA, emb#, embA) where emb is input em- bedding matrix and S is a shortening function.\\n\\n\\n\\n|Shortened embeddings Shifted input embeddings|[ 000] [A#A]\\n0A# AB#||[B#B]\\nBC#|\\n|---|---|---|---|\\n|Target sequence|A#A|B#B|C#C|\\n|Positions|123|456|789|\\n\\n\\nTable 9: Example input sequence which is difficult to model without vanilla layers. The model can use only input embeddings shifted by one from the residual and shortened embeddings (shorten factor is 3) to predict the target sequence. Note that it is impossible to pre- dict tokens at positions divisible by 3 using only that information.\\n\\nBecause no vanilla layers are used, for predic- tion we can use only shortened embeddings and input token embeddings shifted right by 1 from\\n\\nthe residual connection. Note that to predict the A token at position 3 we can use only embedding of emb# and e0 - both of these contain no informa- tion so we are unable to predict this token better than randomly (see Table 9). An analogous situa- tion occurs for prediction of any tokens at positions divisible by 3, which makes the model unable to achieve more than 50% accuracy when the task has vocabulary size of at least 2.\\n\\nThis issue can be solved by adding at least one vanilla layer to the model, so that it can attend within the neighborhood of k previous tokens. For this particular problem, it is sufficient to use local attention with context size k in vanilla layers which is significantly more efficient than full attention.\\n\\n## A.2 Information leaks - analysis\\n\\n\\n\\n## A.2.1 Definition of autoregressive model\\n\\n\\n\\nFormally, given a target sequence, x = x1, ... , In, an autoregressive model (e.g. transformer decoder) models the sequence as P(x) = Ii=1 P(xi|x1, ... , xi-1) and\\n\\nViP(xi|x1, ... , In) = P(xi|x1, ... , xi-1)\\n\\nnamely x¡ token depends only on previous tokens, never on itself nor next ones.\\n\\n## A.2.2 Definition of information leak\\n\\n\\n\\nWe say that a leak was caused by function Fn: An > An transforming sequence of input tokens (x1, x2, ... , In) into another sequence (u1, ... , Un) = F((x1, ... , In)) when BicjenP(x2 x1, ... , Xi-1, 2j) = P(x2|x1, ... , Xi-1), namely there exists j > i that token xi depends on xj which violates the autoregressive property.\\n\\n## A.2.3 Model representation\\n\\n\\n\\nLet Rk : An -> An be a shift right function which reindexes tokens by shifting each of them right by k positions:\\n\\nRk((x1, x2, ... In)) = (0, ... , 0, x1, ... , In-k) k\\n\\nSk : A* > A* shortening function with factor k which takes on input x1, ... , In sequence and returns $1, ... , Sm where m = \", Uk upsampling function which works in similar way but upsamples Uk((u1, ... , Um)) = u1, ... , Un.\\n\\nBetween them there is also applied D decoder function, D = D1 . ... . DI, where each Di is a\\n\\nfunction representing decoder block. Due to causal attention masking in the decoder block, there is no risk of information leak caused by function D.\\n\\n## A.2.4 Leak description\\n\\n\\n\\nBecause of mentioned attention mask, we will omit the flow of information between tokens caused by the influence of attention mechanism because this mask keeps the autoregressive property.\\n\\nNow, let (x1, ... , In) be an input sequence and (u1, ... , Un) = U(D(Sk(Ts((x1, ... , In))))) = F. In order to preserve autoregressive property, it is obligatory that no leak occurs.\\n\\nWe will show that shift by any value 0 < s < k - 1 where k is the shorten factor will cause a leak.\\n\\nTo start with, consider input sequence (x1, ... , In) and perform operation F. Rs((x1, ... , In)) = (0, ... , 0, x1, ... , In-s) = r. Assuming that n is s divisible by s, we have Sk(r) = (v1, ... , Un ) = v where each vi consists of information obtained in (™(i-1)-k+1, ... , Tik). Now let see that operation D preserves autoregressive property, let d = D(t). Now, U(d) = (u1, ... , Un) and each uj depends on d k Now consider s ≤ k - 2 and let (u1, ... , un) = F((x1, ... , In)) will be a result of our Transformer part. Let take u1 which depends on d1 and d1 depends on (r1, ... , Tk) = (0, ... , 0, x1, ... , Ik-s). For that reason di depends on x1, x2, ... , Dk-s, SO we have\\n\\nP(x1|@k-s) ¥P(x1)\\n\\nwhich violates the autoregressive property.\\n\\n## B Experimental setup\\n\\n\\n\\n## B.1 Common parameters\\n\\n\\n\\nHere we list common hyperparameters used for all experiments mentioned in the paper. We use Adam optimizer with 61 = 0.9, 62 = 0.98 and € = le-9. Weight decay and gradient clipping is not used.\\n\\nIn terms of model details, we decided to use a Pre-Norm architecture and FastGelu activation in feed-forward layers.\\n\\n## B.2 Enwik8\\n\\n\\n\\nWe use dmodel = 512, dff = 2048 and 8 attention heads. Models in ablation study are trained for 200k steps with cosine learning rate schedule, set- ting cycle length for 200k steps and linear warmup of 4000 steps.\\n\\nFor the main result achieving 0.98 bpc with 4@1,8@3,4@1 hierarchy, we set dmodel = 768, dff = 3072 and nheads = 8 which results in 146M parameters. It is trained for a total number of 350k steps with one cycle of cosine schedule. Linear warmup of 20k steps is used.\\n\\nAt the beginning of our work on this paper, we have performed grid search over following hyper- parameters for enwik8:\\n\\n· batch size: {8, 16, 32}, finally chosen 8\\n\\n· dropout: {0.05, 0.1, 0.15, 0.20}, finally cho- sen 0.15\\n\\n· learning rate: {1e-4, 2e-4, 3e-4, 4e-4, 5e-4}, finally chosen 4e-4\\n\\nAll next experiments were conducted using these parameters without additional searching.\\n\\n## B.3 Downsampled ImageNet - common parameters\\n\\n\\n\\nFor ImageNet32 and ImageNet64 experiments we use inverse square root learning rate decay from (Vaswani et al., 2017), setting warmup steps to 8000 in both experiments. Total batch size is 64.\\n\\n## B.4 ImageNet32\\n\\n\\n\\nIn this dataset, we operate on input sequence length of 3072. We use dmodel = 512, df = 2048, 8 attention heads and 0.01 dropout rate. We perform 400k training steps with linear warmup and inverse square root decay and then we train for additional 70k steps with cosine learning rate decay, starting from the learning rate from the previous schedule at 400k and decreasing it to 0 at 470k steps.\\n\\n## B.5 ImageNet64\\n\\n\\n\\nAs an input we receive a sequence of 12288 tokens representing 64 x 64 × 3 images. We set dmodel = 768, df = 3072, 8 attention heads and dropout equal to 0. We perform 300k steps with linear warmup and inverse square root decay.\\n\\n## B.6 CIFAR-10\\n\\n\\n\\nAll the ablation studies are run for 100k training steps, unless otherwise specified. Input sequence has length 3072 and model parameters are as fol- lows: dmodel = 512, df = 2048, 8 attention heads and dropout equal to 0. Total batch size is 8. Co- sine learning rate decay with linear warmup of 5000 steps and 100k cycle length is used.\\n\\n## C Environment setup\\n\\n\\n\\n## C.1 Hardware\\n\\n\\n\\nExperiments are conducted on several setups.\\n\\n· Ablation Study and short training sessions were computed on nodes consisting of 4x Ti- tan V with 12GB memory each, 64GB RAM, Intel Xeon E5-2660 v4 CPU\\n\\n· longer trainings were completed on 8x RTX 2080 Ti with 11GB memory each, 128GB RAM and Intel Xeon E5-2660 v4 CPU.\\n\\n· Few longest trainings were conducted on 8 × 8 TPUv3 units, each with 16GB memory.\\n\\n## C.2 Software\\n\\n\\n\\nAll experiments were performed on Linux operat- ing system using Trax library version 1.3.9 along with all its dependencies from this particular re- lease date. Additionally, to run shorten factor dropout experiments we modified the Transformer- XL codebase in PyTorch.\\n\\n## D Reproducibility\\n\\n\\n\\nTo ensure the reproducibility of this work and to support open science principles, we made our code publicly available at github.com/google/trax. In this reposi- tory, we also provide Google Colab notebooks where the evaluation of our main Enwik8 and ImageNet32/64 results can be reproduced.23\\n\\n## D.1 Randomness\\n\\n\\n\\nSeeds in all experiments were chosen randomly, however each experiment contains history which allows retrieving all randomly set parameters for reproductions.\\n\\nFor each ablation described in the ablation study section, we rerun the baseline 3 times to calculate standard deviation. All other experiments are run only once due to costs and since the variance we noticed was minimal.\\n\\n## D.2 Experiment representation\\n\\n\\n\\nEach experiment is represented by a configuration file that unambiguously determines the whole setup - all hyperparameters and training details like spe- cific optimizers, data preprocessing functions, or batch size per device.\\n\\n2https://github. com/google/trax/blob/master/trax/models/research/examples/hourglass_enwik8. ipynb\\n\\n3https: //github. com/google/trax/blob/master/trax/models/research/examples/hourglass_downsampled_imagenet. ipynb\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_get_pdf(\n",
    "    paper_db=paper_db,\n",
    "    doc_analyzer=analyzer,\n",
    "    project_dir=Project_dir,\n",
    "    db_dir=db_dir,\n",
    "    config_list=config_list,\n",
    "    initiate_db=initiate_db\n",
    ")\n",
    "\n",
    "get_pdf(url=\"https://arxiv.org/pdf/2110.13711\", reason=\"factual_check\", part=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunk_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 54 docs with a total of 31570 tokens. Largest doc has 2947 tokens.\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 40\n",
      "Insert of existing embedding ID: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 40\n",
      "Insert of existing embedding ID: 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 41\n",
      "Insert of existing embedding ID: 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 41\n",
      "Insert of existing embedding ID: 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 42\n",
      "Insert of existing embedding ID: 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 42\n",
      "Insert of existing embedding ID: 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 43\n",
      "Insert of existing embedding ID: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 46\n",
      "Insert of existing embedding ID: 46\n",
      "Add of existing embedding ID: 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 46\n",
      "Insert of existing embedding ID: 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 48\n",
      "Insert of existing embedding ID: 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 49\n",
      "Insert of existing embedding ID: 49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 54\n",
      "Insert of existing embedding ID: 54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 57\n",
      "Insert of existing embedding ID: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[92m    Location = ./project_test/db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 60\n",
      "Insert of existing embedding ID: 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 62\n",
      "Insert of existing embedding ID: 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# papers = [f for f in os.listdir(\"./papers\") if os.path.isfile(os.path.join(f\"./papers\", f))]\n",
    "metadata = {\n",
    "    'pdf_url':\"https://doi.org/10.1016/j.mser.2020.100595\",\n",
    "    'title':'Polymer Informatics: Current Status and Critical Next Steps',\n",
    "    'authors':'Lihua Chena,Ghanshyam Pilaniab,Rohit Batrac,Tran Doan Huana,Chiho Kima,Christopher Kuennetha,Rampi Ramprasad',\n",
    "    'published':'2020-03-01',\n",
    "    'updated':'2020-03-01'\n",
    "    }\n",
    "pdf_file = \"/home/alibina/repo/usecases/autosearch/notebooks/papers/1-s2.0-S0927796X2030053X-am.pdf\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    'paper_db': paper_db,\n",
    "    'doc_analyzer': analyzer,\n",
    "    'project_dir': Project_dir,\n",
    "    'db_dir': db_dir,\n",
    "    'config_list': config_list,\n",
    "    'initiate_db': initiate_db\n",
    "}\n",
    "\n",
    "\n",
    "chunk_pdf(pdf_file, metadata, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## url_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True The provided paper URL is from arxiv.org and is for the paper titled 'From algebra to analysis: new proofs of theorems by Ritt and Seidenberg'.\n"
     ]
    }
   ],
   "source": [
    "url_check_res, message = url_check(\"https://arxiv.org/pdf/2107.03012.pdf\", \"From algebra to analysis: new proofs of theorems by Ritt and Seidenberg\")\n",
    "print(url_check_res, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## factal check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'gpt-4',\n",
       "  'api_key': '38ae4759658a4466b454666531283601',\n",
       "  'api_type': 'azure',\n",
       "  'base_url': 'https://aoai-gpt4-505.openai.azure.com/',\n",
       "  'api_version': '2023-08-01-preview'},\n",
       " {'model': 'gpt-4-32k',\n",
       "  'api_key': '38ae4759658a4466b454666531283601',\n",
       "  'base_url': 'https://aoai-gpt4-505.openai.azure.com/',\n",
       "  'api_type': 'azure',\n",
       "  'api_version': '2023-08-01-preview'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided paper URL, https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a, is not from arxiv.org. Please provide a valid arxiv URL.\n",
      "The provided paper URL, http://arxiv.org/pdf/2101.02930v1, is not for the paper titled 'Machine learning of accurate energy-conserving molecular force fields'. Please provide a valid arxiv URL for the paper.\n",
      "The article, 'Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning [http://arxiv.org/pdf/1805.11924v3] updated on 2018-11-15T05:19:07+00:00', has already been read and shared with you in your memory.\n",
      "max token limit: 31744\n",
      "factual_check model: gpt-4-32k\n",
      "\u001b[33mfactual_checker_user\u001b[0m (to factual_checker):\n",
      "\n",
      "\n",
      "Below, you will find a passage labeled \"TEXT\" that references a specific paper: 'Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning [http://arxiv.org/pdf/1805.11924v3]' alongside its corresponding \"PAPER_CONTENT.\" Your task is to read the \"PAPER_CONTENT\" and verify the factual accuracy of the \"TEXT\" as it pertains to the paper.\n",
      "\n",
      "Once you have assessed the factual accuracy, you MUST provide feedback, begining with 'FEEDBACK:'. Following your assessment, please write a summary of the paper. Begin this summary with 'Summary of Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning [http://arxiv.org/pdf/1805.11924v3]: '\n",
      "\n",
      "TEXT:\n",
      "The collaboration between these fields also birthed differentiable programming frameworks for quantum chemistry, like TorchANI and TensorMol [4]. These latter-day frameworks enable researchers to quickly prototype neural networks that learn quantum mechanical laws directly from data, providing a valuable tool to accelerate discovery. This integration has offered promising results in tasks like molecular dynamics simulations, which are key for understanding chemical reactions and material properties.\n",
      "\n",
      "PAPER_CONTENT:\n",
      "## I. S. Novikov ª, Y. V. Suleimanovb,c* and A. V. Shapeeva*\n",
      "\n",
      "\n",
      "\n",
      "We propose a methodology for fully automated calculation of thermal rate coefficients of gas phase chemical reactions, which is based on combining the ring polymer molecular dynamics (RPMD) with the machine-learning interatomic potentials actively learning on-the-fly. Based on the original computational procedure implemented in the RPMDrate code, our methodology grad- ually and automatically constructs the potential energy surfaces (PESs) from scratch with the data set points being selected and accumulated during the RPMDrate simulation. Such an approach ensures that our final machine-learning model provides reli- able description of the PES which avoids artifacts during exploration of the phase space by RPMD trajectories. We tested our methodology on two representative thermally activated chemical reactions studied recently by RPMDrate at temperatures within the interval of 300-1000 K. The corresponding PESs were generated by fitting to only a few thousands automatically generated structures (less than 5000) while the RPMD rate coefficients retained the deviation from the reference values within the typical convergence error of RPMDrate. In future, we plan to apply our methodology to chemical reactions which proceed via complex- formation thus providing a completely general tool for calculating RPMD thermal rate coefficients for any polyatomic gas phase chemical reaction.\n",
      "\n",
      "[physics.chem-ph] 15 Nov 2018\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "\n",
      "\n",
      "Accurate and efficient computation of thermal chemical reaction rate coefficients represents one of the most challenging problems for modern theoretical physical chemistry. Rigorous quantum dynamics calculations scale exponentially with the dimensionality of the system and are therefore limited to chemical reactions involving just a few atoms. 1 Classical description of chemical reactivity allows practical simulations of polyatomic systems, but the problem is complicated at low temperatures, at which quantum-mechanical effects of nuclear motions such as zero-point energy, tunneling, and resonance effects become critically important (though the contribution of the later effect to thermal rate coefficients is less studied2). Recently, it has been\n",
      "\n",
      "demonstrated that the ring polymer molecular dynamics (RPMD) 3,4 provides systematically accurate approach for calculating thermal rate coefficients in multifarious scenarios.5 This semiclassical method scales \"classically\" with the number of atoms and is based on the isomorphism between the quantum statistical mechanics of the physical system and the classical statistical mechanics of a fictitious ring polymer consisting of nbeads copies (beads) of the original system connected by harmonic springs. 6\n",
      "\n",
      "RPMD is exact in the high-temperature limit as it converges to classical molecular dynamics. It has been also shown that RPMD\n",
      "\n",
      "rate-theory gives a lower bound to RPMD transition state theory, which describes an instantaneous quantum flux from reactants to products7 and describes fluctuations around the instanton geometry (in the deep-tunnelling regime)8, thus explaining why RPMD provides reliable estimates of the quantum rate coefficient at low temperatures. General computational procedure for calculating RPMD rate coefficients for polyatomic chemical reactions has been developed9 and implemented in the RPMDrate code. 10 Its application to various gas phase chemical reactions has proven that the method is very accurate for estimating thermal rate coefficients even in the most challenging benchmark cases. 9,11-15\n",
      "\n",
      "arXiv: 1805.11924v3\n",
      "\n",
      "Despite the instantaneous success of RPMDrate code, 16 the current version is restricted to a limited number of chemical reactions for which the underlying potential energy surfaces (PESs) are available in an analytical form. For the code to become a generally useful tool, efficient ways to couple RPMD with electronic structure evaluations are required. In principle, PES can be calculated on-the-fly but even with the most advanced supercomputers it is extremely CPU-intensive and is generally limited to fairly short propagation times. This challenge has been partially solved by approximating a limited number of quantum- mechanical calculations (typically tens of thousands), constructing a PES using the permutation invariant polynomial-neural network (PIP-NN) method. 17-19 However, during preliminary RPMDrate simulations for several polyatomic systems, conver- gence issues have been detected due to artifacts in the PIP-NN PESs resulted from a lack of points in data sets in certain areas\n",
      "\n",
      "a Skolkovo Institute of Science and Technology, Skolkovo Innovation Center, Nobel St. 3, Moscow 143026, Russia\n",
      "\n",
      "b Computation-based Science and Technology Research Center, Cyprus Institute, 20 Kavafi Street, Nicosia 2121, Cyprus\n",
      "\n",
      "c Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139, United States\n",
      "\n",
      "* Email:ysuleyma@mit.edu, a.shapeev@skoltech.ru\n",
      "\n",
      "(see, e.g., the Supporting Information file of Ref.20). As compared to classical trajectories which are normally used for verifi- cation of the PESs, RPMD trajectories provide more enhanced sampling of the phase space by ring polymer beads which could enter the potential artifact zones.\n",
      "\n",
      "During the last years, the application of machine learning to constructing PESs has gained a lot of attention. 21-48 The methods are based on neural networks, 21-33,45,46,48 Gaussian processes, 34-38 and other methods. 39,40 Also closely related are energy-free (i.e., non-conservative) machine-learning force fields. 41-43 Among those is the Moment Tensor Potential (MTP). 39,49 We use MTP as the interatomic interaction model in this work.\n",
      "\n",
      "The goal of this work is to propose an algorithm of automatically constructing an approximation to the reference PES for any given molecular system for subsequent calculation of RPMD thermal rate coefficients. The main challenge in automatically constructing such an approximation is to automatically assemble the training set that can be used to fit a good potential. A natural idea would be to use RPMDrate itself to sample the needed configurations for training, but the original version of RPMDrate requires a fitted potential to run. This seems to be a vicious circle: we need a training set in order to fit a potential, while we need a potential in order to sample a relevant training set. We resolve this challenge by applying the active learning (AL) approach, proposed in Ref.50 for linearly parametrized potentials and extended to nonlinearly parametrized models in Refs. 49,51 The idea of the approach is to let RPMDrate sample the needed configurations, and for each configuration decide on-the-fly whether a potential can yield reliable energies and forces or it needs to be trained on this configuration. The underlying algorithm for choosing configurations for training is based on a D-optimality criterion for selecting the configurations in the training set (after computing its energy and forces using an ab initio potential). The core of this criterion is the so-called maxvol algorithm, proposed in Ref.52. We refer to the combined approach as AL-MTP (active-learning moment tensor potential).\n",
      "\n",
      "In this paper we propose and test a combination of AL-MTP and RPMDrate for predicting chemical reaction thermal rate coefficients. For the present study, we have selected two exemplifying systems, namely, OH + H2 -> H + H2O and CH4 + CN -> CH3 + HCN, recently studied using RPMDrate 53,54. The RPMD rate coefficients and the corresponding analytical PESs 54,55 for these chemical reactions were readily available to us at the time we started this project. As our main purpose is to demonstrate the feasibility of our new approach, we consider these PESs as ab initio models and compare the rate coefficients predicted by these models to the ones calculated using the MTPs. We emphasize that although the practical purpose would be to fit machine- learning PESs to accurate quantum-mechanical models and hence calculate accurate reaction rates, the purpose of this work is to test the accuracy of our approach and hence we fit our PESs to the existing accurate and efficient PESs for which can compute the reaction rates used as a reference for our models.\n",
      "\n",
      "## 2 Methodology\n",
      "\n",
      "\n",
      "\n",
      "## 2.1 Machine-Learning Interatomic Potential\n",
      "\n",
      "\n",
      "\n",
      "## 2.1.1 Moment Tensor Potentials\n",
      "\n",
      "\n",
      "\n",
      "We assume that the energy of a configuration is partitioned into a sum of contributions of each of the n atoms E = 27 1 Vi. Each contribution is further expanded as a linear combination of basis functions BQ,\n",
      "\n",
      "Vi = >&Ba,\n",
      "\n",
      "(1)\n",
      "\n",
      "α\n",
      "\n",
      "where Sa are the parameters of the potential that are found (regressed) from the data. The basis functions BQ depend on the atomic environment of the i-th atom consisting of all j-th atoms that are within the distance of Rcut from the i-th atom. The environment is expressed by the interatomic vectors rij and the types of atoms z; and zj. In order to account for all the physical symmetries, we introduce the moment tensor descriptors 39\n",
      "\n",
      "Mu,v (ri) = >fu(raj), zi, kj) rij\n",
      "\n",
      ":selected: v times\n",
      "\n",
      "(2)\n",
      "\n",
      ":selected: ... & rij .\n",
      "\n",
      "j\n",
      "\n",
      "Here the symbol \"\" denotes the outer product (so that rij & rij is a matrix, rij & rij & rij is a three-dimensional tensor, etc.). The first part, fu(rij), zi, 2j), can be thought of as the radial part of the descriptors, while rij ... Orij is the angular part. The radial part is further expanded as :selected: :selected: :selected:\n",
      "\n",
      "fu (rij), zi, zj) = .(8) CH,Zi,2; PB (rij),\n",
      "\n",
      "(3)\n",
      "\n",
      "β\n",
      "\n",
      "2\n",
      "\n",
      "where CH, zi, kj cup2; z; is another set of parameters to be fitted and Op are the radial basic functions (expressed through the Chebyshev polynomials and ensuring a smooth cut-off to 0 for r > Rcut). One can think of the functions fu as the ones that define the shells of neighboring atoms, while the coefficients CH, zi, kj .(B) express the relative weights of atomic species zj in the u-th shell of the i-th atom.\n",
      "\n",
      "We then construct our basis functions BQ as different contractions of the moment tensor descriptors (2) to a scalar, such as\n",
      "\n",
      "Bo(Ti) = Mo,o(Ti),\n",
      "\n",
      "B1(ri) = M0,0(ri)M1,0(ri),\n",
      "\n",
      "B2(Ti) = Mo,2(Ti) : M1,2(Ti), ...\n",
      "\n",
      "We denote the parameters of MTP to be fitted by 0 := (Sa, CH,zi, 2) and hence we denote the MTP energy of a configuration x by E = E(0; x).\n",
      "\n",
      "## 2.1.2 Fitting\n",
      "\n",
      "\n",
      "\n",
      "Let {x(k)} be a training set with K configurations. Each configuration is supplied with an ab initio energy EAI(x(k)) and forces fAI(x(k)) on each of the atoms. The fitting consists of finding the parameters 0 that minimize the following loss function\n",
      "\n",
      "n fiAT (2(k) - fi(0; xe (k)|2| -> min,\n",
      "\n",
      "L(0) = k=1\n",
      "\n",
      "K (EAI(@(k)) - E(0; x(k)) ) 2 + WE\n",
      "\n",
      "(4)\n",
      "\n",
      "i=1\n",
      "\n",
      "where wf is a non-negative weight expressing the importance of forces relative to energy in the fitting.\n",
      "\n",
      "## 2.1.3 Active Learning\n",
      "\n",
      "\n",
      "\n",
      "Within the active learning concept, we construct the training set adaptively. To achieve that, we need an algorithm that will decide whether to include a given configuration x* that is generated by the RPMDrate code. To that end, we need a new concept-active set. Suppose that the number of parameters 0 is m. The active set is then a subset of size m of the training set (for convenience denoted by x(1), ... , x(m)) that maximizes the determinant\n",
      "\n",
      "DE (0; xe (1) . 00m DE . .\n",
      "\n",
      "(0;x(1))\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ". (0;x(m))\n",
      "\n",
      "JE\n",
      "\n",
      "(0;x(m)) .\n",
      "\n",
      "DE 00m\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "In order to find the active set we use the so-called maxvol algorithm proposed in Ref.52. For a configuration x*, we then define its extrapolation grade y(x*) as the maximum, by the absolute value, factor by which the above determinant can increase if we try to replace each x(i) by x *. We emphasize that y(x*) does not depend on the ab initio data, it depends only on the geometric information of the configuration x *. Thus, it is not necessary to carry out ab initio calculations to calculate the extrapolation grade.\n",
      "\n",
      "In order to formulate our active learning algorithm, we introduce two thresholds: 7th and Ith, 1 < 7th < Ith. These thresholds define the bounds of permissible extrapolation. Thus our AL algorithm can be systemized as follows:\n",
      "\n",
      "· For each configuration x* occurring in the RPMDrate simulation, we calculate y( ** ). If y( ** ) < 7th then x* will not be added to the training set. Otherwise, there are two possibilities:\n",
      "\n",
      "a. 7th Sy( ** ) < Ith. In this case, we think of y(x*) as sufficiently high for x* to be added to the training set, but not too high to terminate the RPMDrate simulation. Hence, in this case, we mark (save to a file) the configuration x* and proceed with the RPMDrate simulation.\n",
      "\n",
      "b. y(x*) > Ith. In this case, the extrapolation grade is too high, therefore we add x* to the training set and terminate the RPMDrate simulation. We then update the active set with the marked configurations, calculate their ab initio energies and forces, add them to the training set, refit the potential, and repeat the entire RPMDrate simulation from the beginning.\n",
      "\n",
      "As a result, our algorithm will restart RPMDrate several times until the training set covers well the needed region in the phase space. We emphasize that the potential is fixed at each RPMDrate run, thus ensuring that the code samples a proper canonical ensemble at each run.\n",
      "\n",
      "Through the algorithm described above, our potential is trained in a fully automatic manner, lifting the need in tedious manual analysis of the quality of the PES being constructed. The scheme of our AL-MTP algorithm is shown in Fig. 1.\n",
      "\n",
      "3\n",
      "\n",
      "## RPMDrate\n",
      "\n",
      "\n",
      "\n",
      "Conf.\n",
      "\n",
      "Active learning\n",
      "\n",
      "E, f\n",
      "\n",
      "Yes\n",
      "\n",
      "Extrapolation?\n",
      "\n",
      "No\n",
      "\n",
      "Calculate E, f\n",
      "\n",
      "Store conf.\n",
      "\n",
      "Too high\n",
      "\n",
      "Extrapolation?\n",
      "\n",
      "No\n",
      "\n",
      "Yes\n",
      "\n",
      "Terminate RPMD, update training set, re-train MTP, restart RPMDrate\n",
      "\n",
      "Figure 1 Active learning scheme. The RPMDrate code generates a configuration for which we calculate an extrapolation grade. If the grade is low, we calculate the energy and forces for this configuration and continue the RPMDrate simulation. Otherwise, if the grade is high, but not too high to terminate the RPMDrate run, we mark (save to a file) the configuration and proceed with the RPMDrate run. Finally, if the extrapolation grade is too high, we terminate RPMDrate, update the training set, re-train the potential and restart the entire RPMDrate calculation.\n",
      "\n",
      "4\n",
      "\n",
      "## 2.2 Application to OH + H2 -> H + H2O and CH4 + CN -> CH3 + HCN\n",
      "\n",
      "\n",
      "\n",
      "We apply our AL algorithm in combination with MTPs to the calculation of RPMD rate coefficients for the following two representative chemical reactions: OH + H2 -> H + H2O and CH4 + CN -> CH3 + HCN. Below we show that the AL-MTP algorithm is capable of accurate prediction of chemical reaction rate coefficients for various temperatures and different number of ring polymer beads for both systems.\n",
      "\n",
      "## 2.2.1 RPMDrate computational details\n",
      "\n",
      "\n",
      "\n",
      "We carry out the RPMD computations using the RPMDrate code which is well-documented in Ref. 10. Below we only briefly describe the key steps of the RPMDrate computational procedure. The rate coefficient is calculated using the Bennett-Chandler factorization 56,57 as a product of a static (centroid density quantum transition state theory (QTST) rate coefficient, kQTST ) and a dynamic (ring polymer transmission coefficient, k) factors. The first step is the construction of potential of mean force (PMF) W (§) along the dimensionless reaction coordinate § defined in terms of two dividing surfaces given by Eqs.(4-10) in Ref. 10. The profile connects the reactant (§ = 0) and transition state (§ = 1) regions. We generate this profile using the umbrella integration technique 58,59 and use to calculate kQTST . The second step is the calculation of k using a combination of constrained (parent) and unconstrained (child) trajectories. We perform steps consequently in order to detect the maximum value of W (§#) during the first step and to start the calculation of k from the coordinate &# (for parent trajectory) during the second step. The final rate coefficient is given by the product of two factors, kRPMD = kQTST X K.\n",
      "\n",
      "We study the first reaction, OH + H2 -> H + H2O, at T = 300 K and T = 1000 K with nbeads = 1 at both temperatures, nbeads = 128 at the low temperature and nbeads = 16 at the high temperature. We run the second reaction, CH4 + CN -> CH3 + HCN, at T = 300 K and T = 600 K with the same number of ring polymer beads at the low and the high temperatures as for the first reaction.\n",
      "\n",
      "The remaining input parameters for the RPMDrate simulations are similar to those used in numerous studies of thermally activated chemical reactions.5 In order to obtain the PMF profiles for both chemical reactions, we divide the interval -0.05 ≤ § ≤ 1.05 into 111 windows of width 0.01. Umbrella force constant was set to ki = 2.72((T/K) eV) for each window centered at Și, i = 1, ... , 111. In every window, we run 80 constrained RPMD trajectories with the sampling period of 50 ps and the equilibration period of 15 ps. Finally, the propagation time step was set equal to 0.0001 ps.\n",
      "\n",
      "For the calculation of K, we choose slightly different parameters depending on the chemical reaction. For the OH + H2 system, all the calculations (except the computation with nbeads = 128) are carried out at 20000 unconstrained child trajectories (Ntotalchild) with the equilibration time of 10 ps (tequilibration) and 100 child trajectories per one initially constrained configura- tion (Nchild). All the unconstrained child trajectories run for tchild = 0.05 ps with the time step dt = 0.00005 ps. For the case of nbeads = 128, we increase the number of the unconstrained child trajectories up to 25000 and the time step is set to 0.0001 ps. For CH4 + CN system, we take the following parameters: Ntotalchild = 50000, tequilibration = 5 ps, Nchild = 100, tchild = 0.06 ps, and dt = 0.0001 ps.\n",
      "\n",
      "As it was mentioned above, we consider the potentials described in Refs. 54,55 as ab initio models for the present exemplifying study. The potential for the OH + H2 -> H + H2O reaction has been developed using the Neural Networks (NN) fitting55 and is denoted as NN1 PES. Another potential, applied for the CH4 + CN system, is a combination of various semi-empirical potentials 54 including 34 parameters that were obtained after fitting this potential on the dataset, describing the stationary points, the reaction path and the reaction swath. For simplicity, we shall call this potential CH4+CN PES though we note that its original abbreviation is different (PES2017). We also note that the previous RPMD studies using these PESs demonstrated very good agreement with the experimental measurements of rate coefficients 53,54 .\n",
      "\n",
      "We fit MTP with 92 basis functions BQ, 4 radial functions fu and 12 radial basis functions 48. This results into approximately 300 and 500 MTP parameters for OH + H2 and CH4 + CN, respectively. We choose Rcut = 4 and 6 A, respectively, for these systems. The active learning was performed with Yth = 2 and Ith = 10, thus the interval of high but permissible grades is [2, 10).\n",
      "\n",
      "As described above, we need to compute kQTST (the first step of RPMDrate) and k (the second step of RPMDrate). In order to obtain kQTST, we focus only on the region connecting the reactants with the transition state (i.e., § € (-0.05, 1.05)). During the second step - computation of k - the RPMD trajectories visit the products region, i.e., § > 1.05. The geometries of the configurations in the reactants and products regions are different and, thus, we use a slightly different MTPs for the calculation of kQTST and k trained on two datsets. More precisely, during the first RPMDrate step we form the reactants set (kQTST set) that consists of configurations selected from the reactants region and learn on-the-fly the first MTP. During the second RPMDrate step, we start from the MTP and the training set derived after the first step, update the training set with the additional configurations (the products set, or, k set) and learn on-the-fly the second MTP. Thus, the two MTPs differ by their training sets-the first training set is a subset of the second one. Having computed kQTST and k, respectively, by these two MTPs, we obtain the final RPMD rate coefficient.\n",
      "\n",
      "5\n",
      "\n",
      "## 3 Results and discussion\n",
      "\n",
      "\n",
      "\n",
      "The PMF profiles W (§) for the OH + H2 and CH4 + CN reactions are plotted in Figs. 2 and 3, respectively. For both reactions and two representative temperatures, the results obtained using MTPs are close to the ab initio profiles, the difference is less than 0.3 kcal/mol. Time-dependent k's obtained by the MTP and ab initio models for the OH + H2 and CH4 + CN reactions are shown in Figs. 4 and 5, respectively. Similarly to the PMF profiles, the results obtained using MTP are in a very good agreement with the ab initio counterparts. The values of the centroid density TST rate coefficient kQTST, the ring polymer recrossing factor k and the RPMD rate coefficient kRPMD are also summarized for the OH + H2 and CH4 + CN reactions in Tables 1 and 2, respectively. The agreement with the previous RPMD rate coefficients is very good, the relative root-mean-square deviation between MTP and the reference rate coefficients is about 20% or less and is comparable to typical convergence error of the RPMDrate computational procedure 5,10\n",
      "\n",
      "The number of configurations selected in the reactants region (kQTST set size), the products region (k set size) and the total training set sizes (kRPMD set size) are reported in Table 3. As it could be seen, we select many more configurations from the reactants region than those we add from the products region (see Fig. 6). The reason of it is as follows. During the first RPMDrate step, we need to approximate the PMF difference between the reactants and the transition state as accurate as possible due to its exponential contribution to kQTST. Thus, we need to predict the PMF profile across each umbrella window, especially near the transition state. This fact is illustrated in Fig. 7. Indeed, most of configurations for both systems were selected for § € (0.95, 1.05), i.e., near the transition state. Then, it happens that for the purposes of calculating RPMD rate coefficients a potential that is well-trained in the reactants region needs much less data to be fitted in the products region (only a few bonds significantly differ, while most of the bonds in the molecular systems are the same in both regions).\n",
      "\n",
      "The size of a total training set significantly depends on the number of different atomic types in the molecule. Note that the number of parameters Cu,21, 2, grows as the square of number of atomic types (as they depend on pairs of types of interacting atoms, (Zi, Zj)). Thus, there are 2.25 times more coefficients in the potential for the CH4 + CN system than for the OH + H2 one. As it can be seen from Table 3, we need approximately 2.25 times more configurations in the training sets for the CH4 + CN system than for the OH + H2 one. This confirms that the number of coefficients grows quadratically with the number of atomic types and thus the proposed algorithms should be applicable for large molecular systems, direct description of which is problematic due to high dimensionality.\n",
      "\n",
      "We additionally test how the accuracy improves when the number of MTP parameters increases. The test is done for the CH4 + CN system, T = 300 K, nbeads = 1. On Fig. 8 the PMF profile and the transmission coefficient are plotted for three potentials, with 150, 250, and 500 parameters, respectively. As it could be seen, the training set size increases with the number of parameters, and so is the accuracy.\n",
      "\n",
      "The remaining two factors that affect the size of our training set are the number of ring polymer beads and the temperature. Increasing the number of ring polymer beads leads to more enhanced phase space exploration thus more configurations are necessary in the training set. The same is valid for the temperature factor: as the temperature increases, the energy dispersion increases and therefore we need more configurations in the dataset in order to describe all possible energy levels. We attribute both correlations to the fact that higher temperature and higher number of beads imply that we need to sample a larger region in the phase space and therefore collect more configurations for training. In any case, the maximal training set size is less than 5000, thus, we needed to carry out less than 5000 ab initio calculations in order to obtain an MTP for both exemplifying chemical systems considered in the present study.\n",
      "\n",
      "## 4 Conclusions\n",
      "\n",
      "\n",
      "\n",
      "In summary, we propose a fully automated procedure for calculating ring polymer molecular dynamics (RPMD) rate coeffi- cients using the potential energy surface (PES) generated on-the-fly by the moment tensor potentials (MTP) with active learning (AL). The procedure follows the original Bennett-Chandler factorization implemented in the RPMDrate code which splits the calculation in two steps-a static (centroid density quantum transition state theory (QTST) rate coefficient) and a dynamic (ring polymer transmission coefficient) factors. During each step, the active-learning algorithm accumulates automatically the dataset sample, ensuring that the fit of the PES is appropriate for calculating the RPMD rate coefficient for a given temperature and number of ring polymer beads. In order to determine whether the current point should be added to the training or not, set we calculate the energy gradient with respect to the parameters of the potential and the so-called extrapolation grade. If the ex- trapolation grade is greater than the lower bound of permissible extrapolation, we mark the current point (save to a file). If the extrapolation grade is greater than the upper bound of permissible extrapolation, we terminate RPMDrate, update the training set using maxvol algorithm and refit the potential. Such an approach ensures that the final machine-learning PES model avoids artifacts during exploration of the phase space by RPMD trajectories which have been observed for several PESs fitted by neural\n",
      "\n",
      "6\n",
      "\n",
      "networks20. The methodology is tested on two representative thermally activated chemical reactions, namely, OH + H2 and CH4 + CN which were previously studied by RPMD. 53,54 The deviation of the present RPMD rate coefficients obtained using the AL-MTP approach from the reference values is within the convergence error of the RPMDrate computational procedure.\n",
      "\n",
      "In future, we plan to extend our methodology to chemical reactions which proceed via complex formation in order to propose a completely general tool for calculating RPMD rate coefficients for any polyatomic chemical reactions. In principle, the Bennett- Chandler factorization can be also implemented in this case5 though the contribution from the real-time propagation of the dynamic factor significantly increases leading to possible alterations to the AL-MTP algorithm. This work is currently on-going. Finally, we would like to note that our AL-MTP approach could be used in calculations of other dynamical properties (such as RPMD diffusion coefficients), applicability of the algorithm does not depend on a physical quantity predicted.\n",
      "\n",
      "## 5 Acknowledgements\n",
      "\n",
      "\n",
      "\n",
      "The work of I.S.N. and A. V.S. was supported by the Russian Science Foundation (grant number 18-13-00479). Y.V.S. thanks the European Regional Development Fund and the Republic of Cyprus for support through the Research Promotion Foundation (Project Cy-Tera ΝΕΑ ΥΠΟΔΟΜΗ/ ΣΤΡΑΤΗ/0308/31). We thank Jesus Castillo for providing analytical gradients for the NN1 PES. We also thank our colleagues, Konstantin Gubaev and Evgeny Podryabinkin, for giving advance access to the code implementing AL-MTP. This work was performed, in part, by A.V.S. at the Center for Integrated Nanotechnologies, an Office of Science User Facility operated for the U.S. Department of Energy (DOE) Office of Science by Los Alamos National Laboratory (Contract DE-AC52-06NA25396) and Sandia National Laboratories (Contract DE-NA-0003525).\n",
      "\n",
      "## References\n",
      "\n",
      "\n",
      "\n",
      "1 B. Fu, X. Shan, D. H. Zhang, and D. C. Clary, Chem. Soc. Rev., 2017, 46, 7625-7649.\n",
      "\n",
      "2 H. Guo, International Reviews in Physical Chemistry, 2012, 31(1), 1-68.\n",
      "\n",
      "3 I. R. Craig and D. E. Manolopoulos, J. Chem. Phys., 2004, 121(8), 3368-3373.\n",
      "\n",
      "4 S. Habershon, D. E. Manolopoulos, T. E. Markland, and T. F. M. III, Annu. Rev. Phys. Chem., 2013, 64(1), 387-413.\n",
      "\n",
      "5 Y. V. Suleimanov, F. J. Aoiz, and H. Guo, J. Phys. Chem. A, 2016, 120(43), 8488-8502.\n",
      "\n",
      "6 D. Chandler and P. G. Wolynes, J. Chem. Phys., 1981, 74(7), 4078-4095.\n",
      "\n",
      "7 T. J. Hele and S. C. Althorpe, The Journal of chemical physics, 2013, 139(8), 084115.\n",
      "\n",
      "8 J. O. Richardson and S. C. Althorpe, The Journal of chemical physics, 2009, 131(21), 214106.\n",
      "\n",
      "9 Y. V. Suleimanov, R. Collepardo-Guevara, and D. E. Manolopoulos, J. Chem. Phys., 2011, 134, 044131.\n",
      "\n",
      "10 Y. Suleimanov, J. Allen, and W. Green, Comp. Phys. Comm., 2013, 184(3), 833-840.\n",
      "\n",
      "11 R. Pérez de Tudela, F. J. Aoiz, Y. V. Suleimanov, and D. E. Manolopoulos, J. Phys. Chem. Let., 2012, 3(4), 493-497.\n",
      "\n",
      "12 Y. Li, Y. V. Suleimanov, M. Yang, W. H. Green, and H. Guo, J. Phys. Chem. Lett., 2012, 4(1), 48-52.\n",
      "\n",
      "13 R. Pérez de Tudela, Y. V. Suleimanov, J. O. Richardson, V. Sáez Rábanos, W. H. Green, and F. J. Aoiz, J. Phys. Chem. Lett., 2014, 5(23), 4219-4224.\n",
      "\n",
      "14 K. M. Hickson, J .- C. Loison, H. Guo, and Y. V. Suleimanov, J. Phys. Chem. Lett., 2015, 6(21), 4194-4199.\n",
      "\n",
      "15 Y. V. Suleimanov, A. Aguado, S. Gmez-Carrasco, and O. Roncero, J. Phys. Chem. Lett., 2018, 9(9), 2133-2137.\n",
      "\n",
      "16 http://rpmdrate.cyi.ac.cy/publications/index.html. 17 J. Zuo, Y. Li, H. Guo, and D. Xie, J. Phys. Chem. A, 2016, 120(20), 3433-3440. 18 M. Bai, D. Lu, Y. Li, and J. Li, Phys. Chem. Chem. Phys., 2016, 18, 32031-32041. 19 J. Zuo, C. Xie, H. Guo, and D. Xie, J. Phys. Chem. Lett., 2017, 8(14), 3392-3397.\n",
      "\n",
      "20 S. S. Kumar, F. Grussie, Y. V. Suleimanov, H. Guo, and H. Kreckel, Science Advances, 2018, 4(6). 21 J. Behler and M. Parrinello, Phys. Rev. Lett., 2007, 98(14), 146401. 22 N. Artrith and A. M. Kolpak, Comput. Mater. Sci., 2015, 110, 20-28. 23 J. Behler, Phys. Chem. Chem. Phys., 2011, 13(40), 17930-17955. 24 J. Behler, J. Phys. Condens. Matter., 2014, 26(18), 183001.\n",
      "\n",
      "25 J. R. Boes, M. C. Groenenboom, J. A. Keith, and J. R. Kitchin, Int. J. Quantum Chem., 2016, 116(13), 979-987.\n",
      "\n",
      "26 P. E. Dolgirev, I. A. Kruglov, and A. R. Oganov, AIP Advances, 2016, 6(8), 085318.\n",
      "\n",
      "27 M. Gastegger and P. Marquetand, J. Chem. Theory Comput., 2015, 11(5), 2187-2198.\n",
      "\n",
      "28 S. Manzhos, R. Dawes, and T. Carrington, Int. J. Quantum Chem., 2015, 115(16), 1012-1020.\n",
      "\n",
      "7\n",
      "\n",
      "29 S. K. Natarajan, T. Morawietz, and J. Behler, Phys. Chem. Chem. Phys., 2015, 17(13), 8356-8371.\n",
      "\n",
      "30 N. Lubbers, J. S. Smith, and K. Barros, jun , 2018, 148(24), 241715.\n",
      "\n",
      "31 J. S. Smith, O. Isayev, and A. E. Roitberg, Chem. Sci., 2017, 8(4), 3192-3203.\n",
      "\n",
      "32 B. Kolb, L. C. Lentz, and A. M. Kolpak, apr , 2017, 7(1).\n",
      "\n",
      "33 K. Schütt, P .- J. Kindermans, H. E. S. Felix, S. Chmiela, A. Tkatchenko, and K .- R. Müller In Advances in Neural Information Processing Systems, pp. 992-1002, 2017. 34 A. P. Bartók, M. C. Payne, R. Kondor, and G. Csányi, Phys. Rev. Lett., 2010, 104(13), 136403. 35 W. J. Szlachta, A. P. Bartók, and G. Csányi, Phys. Rev. B, 2014, 90(10), 104108.\n",
      "\n",
      "36 V. L. Deringer and G. Csányi, Mar , 2017, 95, 094203. 37 V. L. Deringer, C. J. Pickard, and G. Csányi, Phys. Rev. Lett., 2018, 120(15), 156001.\n",
      "\n",
      "38 A. Grisafi, D. M. Wilkins, G. Csányi, and M. Ceriotti, Phys. Rev. Lett., 2018, 120(3), 036002.\n",
      "\n",
      "39 A. Shapeev, Multiscale Model. Simul., 2016, 14(3), 1153-1173.\n",
      "\n",
      "40 A. Thompson, L. Swiler, C. Trott, S. Foiles, and G. Tucker, J. Comput. Phys., 2015, 285, 316 - 330.\n",
      "\n",
      "41 V. Botu and R. Ramprasad, Phys. Rev. B, 2015, 92(9), 094306. 42 Z. Li, J. R. Kermode, and A. De Vita, Mar , 2015, 114, 096405.\n",
      "\n",
      "43 I. Kruglov, O. Sergeev, A. Yanilkin, and A. R. Oganov, Sci. Rep., 2017, 7(1), 8512.\n",
      "\n",
      "44 S. Chmiela, H. E. Sauceda, K .- R. Müller, and A. Tkatchenko, arXiv preprint arXiv: 1802.09238, 2018.\n",
      "\n",
      "45 L. Zhang, J. Han, H. Wang, R. Car, and E. Weinan, Physical review letters, 2018, 120(14), 143001.\n",
      "\n",
      "46 K. Ryczko, K. Mills, I. Luchak, C. Homenick, and I. Tamblyn, Computational Materials Science, 2018, 149, 134-142.\n",
      "\n",
      "47 K. Kanamori, K. Toyoura, J. Honda, K. Hattori, A. Seko, M. Karasuyama, K. Shitara, M. Shiga, A. Kuwabara, and I. Takeuchi, Physical Review B, 2018, 97(12), 125124.\n",
      "\n",
      "48 K. Yao, J. E. Herr, D. W. Toth, R. Mckintyre, and J. Parkhill, Chemical science, 2018, 9(8), 2261-2269.\n",
      "\n",
      "49 K. Gubaev, E. Podryabinkin, G. L. Hart, and A. Shapeev, Work in progress.\n",
      "\n",
      "50 E. V. Podryabinkin and A. V. Shapeev, Comput. Mater. Sci., 2017, 140, 171-180.\n",
      "\n",
      "51 K. Gubaev, E. V. Podryabinkin, and A. V. Shapeev, J. Chem. Phys., 2018, 148(24), 241727.\n",
      "\n",
      "52 S. Goreinov, I. Oseledets, D. Savostyanov, E. Tyrtyshnikov, and N. Zamarashkin in Matrix Methods: Theory, Algorithms And Applications: Dedicated to the Memory of Gene Golub; World Scientific, 2010; pp. 247-256.\n",
      "\n",
      "53 J. Castillo and Y. Suleimanov, Phys. Chem. Chem. Phys., 2017, 19(43), 29170-29176.\n",
      "\n",
      "54 J. Espinosa-Garcia, C. Rangel, and Y. V. Suleimanov, Phys. Chem. Chem. Phys., 2017, 19(29), 19341-19351. 55 J. Chen, X. Xu, X. Xu, and D. H. Zhang, J. Chem. Phys., 2013, 138(15), 154301. 56 D. Chandler, J. Chem. Phys., 1978, 68(6), 2959-2970.\n",
      "\n",
      "57 C. H. Bennett, in Algorithms for Chemical Computations; chapter 4, pp. 63-97. 58 J. Kästner and W. Thiel, J. Chem. Phys., 2005, 123(14), 144104. 59 J. Kästner and W. Thiel, J. Chem. Phys., 2006, 124(23), 234106.\n",
      "\n",
      "8\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "W(§) / eV\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0.7 MTP PES, T = 300 K, nbeads = 1 NN1 PES, T = 300 K, nbeads = 1 MTP PES, T = 1000 K, nbeads NN1 PES, T = 1000 K, nbeads = 1 0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "W(§) / eV\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "MTP PES, T = 1000 K, nbeads = 16 NN1 PES, T = 1000 K, nbeads = 16 MTP PES, T = 300 K, nbeads = 128 NN1 PES, T = 300 K, nbeads = 128\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "0\n",
      "\n",
      "-0.2 0 0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "ξ\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "1.2\n",
      "\n",
      "-0.2 0\n",
      "\n",
      "0.2\n",
      "\n",
      "0.4 0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "ξ\n",
      "\n",
      "1 1.2\n",
      "\n",
      "Figure 2 Comparison of potential of mean force profiles for the OH + H2 -> H + H2O reaction calculated by the Moment Tensor Potential (MTP) PES and NN1 (reference PES) under various temperatures and number of beads.\n",
      "\n",
      "0.4\n",
      "\n",
      "MTP PES, T = 300 K, nbeads CH4+CN PES, T = 300 K, nbeads MTP PES, T = 600 K, nbeads\n",
      "\n",
      "0.35\n",
      "\n",
      "1\n",
      "\n",
      "CH4+CN PES, T = 600 K, nbeads\n",
      "\n",
      "0.3\n",
      "\n",
      "0.25\n",
      "\n",
      "0.3 MTP PES, T = 600 K, nbeads = 16 CH4+CN PES, T = 600 K, nbeads = 16 MTP PES, T = 300 K, nbeads = 128 CH4+CN PES, T = 300 K, nbeads = 128\n",
      "\n",
      "-\n",
      "\n",
      "0.25\n",
      "\n",
      "0.2\n",
      "\n",
      "W(č) / eV\n",
      "\n",
      "0.2W(§) / eV\n",
      "\n",
      "0.15\n",
      "\n",
      "0.15\n",
      "\n",
      "0.1\n",
      "\n",
      "0.1\n",
      "\n",
      "--\n",
      "\n",
      "0.05\n",
      "\n",
      "0.05\n",
      "\n",
      "0\n",
      "\n",
      "-0.2 0 0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "ξ\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "1.2\n",
      "\n",
      "-0.2\n",
      "\n",
      "0 0.2\n",
      "\n",
      "0.4\n",
      "\n",
      "0.6\n",
      "\n",
      "0.8\n",
      "\n",
      "ξ\n",
      "\n",
      "1 1.2\n",
      "\n",
      "Figure 3 Comparison of potential of mean force profiles for the CH4 + CN -> CH3 + HCN reaction calculated by the Moment Tensor Potential (MTP) PES and CH4 + CN (reference PES) under various temperatures and number of beads.\n",
      "\n",
      "9\n",
      "\n",
      "1 MTP PES, T = 300 K, nbeads = 1 NN1 PES, T = 300 K, nbeads = 1 MTP PES, T = 1000 K, nbeads = 1 NN1 PES, T = 1000 K, nbeads =\n",
      "\n",
      "0.9\n",
      "\n",
      "1\n",
      "\n",
      "MTP PES, T = 1000 K, nbeads = 16 NN1 PES, T = 1000 K, nbeads = 16 MTP PES, T = 300 K, nbeads = 128 NN1 PES, T = 300 K, nbeads = 128\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.8\n",
      "\n",
      "k(t)\n",
      "\n",
      "0.7\n",
      "\n",
      "k(t)\n",
      "\n",
      "0.7\n",
      "\n",
      "0.6\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0\n",
      "\n",
      "10 20\n",
      "\n",
      "30\n",
      "\n",
      "t / fs\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "0.4 0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "t / fs\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "Figure 4 Comparison of the time-dependent ring polymer transmission coefficients for the OH + H2 -> H + H2O reaction calculated by the Moment Tensor Potential (MTP) PES and NN1 (reference PES) under various temperatures and number of beads.\n",
      "\n",
      "1 MTP PES, T = 300 K, nbeads = 1 CH4+CN PES, T = 300 K, nbeads MTP PES, T = 600 K, nbeads = 1 CH4+CN PES, T = 600 K, nbeads =\n",
      "\n",
      "0.9\n",
      "\n",
      "= 1\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "MTP PES, T = 600 K, nbeads = 16 CH4+CN PES, T = 600 K, nbeads = 16 MTP PES, T = 300 K, nbeads = 128 CH4+CN PES, T = 300 K, nbeads = 128\n",
      "\n",
      "0.9\n",
      "\n",
      "0.8\n",
      "\n",
      "0.7\n",
      "\n",
      "0.7\n",
      "\n",
      "k(t)\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "t / fs\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "k(t)\n",
      "\n",
      "60\n",
      "\n",
      "0.6\n",
      "\n",
      "0.5\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "t / fs\n",
      "\n",
      "Figure 5 Comparison of the time-dependent transmission coefficients for the CH4 + CN -> CH3 + HCN reaction obtained by Moment Tensor Potential (MTP) and CH4 + CN (reference PES) under various temperatures and number of beads.\n",
      "\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "||T = 300 K nbeads = 1|T= 300 K nbeads = 128|T= 1000 K nbeads = 1|T= 1000 K nbeads = 16|\n",
      "|---|---|---|---|---|\n",
      "|KAI (cm3 s-1) RQTST (cm3 s-1) error (%)|5.74 × 10-16 5.37 × 10-16 6.5 %|2.37 × 10-14 1.84 × 10-14 22.3 %|2.78 × 10-12 2.91 × 10-12 4.7 %|3.72 × 10-12 3.97 × 10-12 6.7 %|\n",
      "|KAI|0.613|0.528|0.666|0.599|\n",
      "|KMTP|0.626|0.527|0.649|0.589|\n",
      "|error (%)|2.1 %|0.2 %|2.6 %|1.7 %|\n",
      "|KRPMD (cm3 s-1) KRPMD (cm3 s-1) LMTP error (%)|3.52 × 10-16 3.36 × 10-16 4.5 %|1.25 × 10-14 9.70 × 10-15 22.4 %|1.85 × 10-12 1.89 × 10-12|2.23 × 10-12 2.34 × 10-12|\n",
      "||||2.2 %|4.9 %|\n",
      "\n",
      "\n",
      "Table 1 Comparison of the quantum transition state theory (QTST) rate coefficient kQTST, ring polymer transmission coefficient k, and final rate coefficient kRPMD calculated by the NN1 and MTP PESs for the OH + H2 system under various conditions.\n",
      "\n",
      "\n",
      "\n",
      "||T= 300 K nbeads = 1|T= 300 K nbeads = 128|T= 600 K nbeads = 1|T= 600 K nbeads = 16|\n",
      "|---|---|---|---|---|\n",
      "|LAT TST (cm3 s-1) KQTST LMTP (cm3 s-1) error (%)|1.69 × 10-13 1.61 × 10-13 4.7 %|1.13 × 10-11 1.35 × 10-11 19.5 %|6.10 × 10-12 6.17 × 10-12 1.1 %|3.63 × 10-11 3.48 × 10-11 4.1 %|\n",
      "|AI κ|0.267|0.184|0.304|0.250|\n",
      "|&MTP|0.256|0.185|0.317|0.251|\n",
      "|error (%)|4.1 %|0.5 %|4.3 %|0.4 %|\n",
      "|LAI KRPMD (cm3 s-1) KMTP FRPMD (cm3 s-1) error (%)|4.51 × 10-14 4.12 × 10-14 8.6 %|2.08 × 10-12 2.50 × 10-12 20.2 %|1.85 × 10-12 1.95 × 10-12 5.4 %|9.07 x 10-12 8.73×10-12|\n",
      "|||||3.7 %|\n",
      "\n",
      "\n",
      "Table 2 Comparison of the quantum transition state theory (QTST) rate coefficient kQTST, ring polymer transmission coefficient k, and final rate coefficient kRPMD calculated by the CH4 + CN and MTP PESs for the CH4 + CN system under various conditions.\n",
      "\n",
      "\n",
      "\n",
      "|OH + H2 -> H + H2O||||CH4 + CN -> CH3 + HCN||||\n",
      "|---|---|---|---|---|---|---|---|\n",
      "|T, nbeads|kQTST set size|κ set size|KRPMD set size|T, nbeads|kQTST set size|κ set size|KRPMD set size|\n",
      "|300 K, 1|1401|96|1497|300 K, 1|3348|581|3929|\n",
      "|300 K, 128|1816|44|1860|300 K, 128|4138|380|4518|\n",
      "|1000 K, 1|1784|123|1907|600 K, 1|3904|544|4448|\n",
      "|1000 K, 16|2014|83|2097|600 K, 16|4572|320|4892|\n",
      "\n",
      "\n",
      "Table 3 Number of configurations selected in the reactants region (kQTST set size), in the products region (k set size), and the total training set size (kRPMD set size) for the OH + H2 and CH4 + CN systems.\n",
      "\n",
      "11\n",
      "\n",
      "OH+H2, reactants set OH+H2, products set\n",
      "\n",
      "2000\n",
      "\n",
      "5000\n",
      "\n",
      "CH4+CN, reactants set CH4+CN, products set\n",
      "\n",
      "4000\n",
      "\n",
      "1500\n",
      "\n",
      "3000\n",
      "\n",
      "N\n",
      "\n",
      "1000\n",
      "\n",
      "N\n",
      "\n",
      "2000\n",
      "\n",
      "500\n",
      "\n",
      "1000\n",
      "\n",
      "0\n",
      "\n",
      "300,1\n",
      "\n",
      "300,128\n",
      "\n",
      "1000,1\n",
      "\n",
      "T/K, nbeads\n",
      "\n",
      "1000,16\n",
      "\n",
      "0\n",
      "\n",
      "300,1\n",
      "\n",
      "300,128\n",
      "\n",
      "600,1\n",
      "\n",
      "T/K, nbeads\n",
      "\n",
      "600,16\n",
      "\n",
      "Figure 6 The reactants and products set sizes for the OH+H2 and CH4+CN systems. For both reactions under various conditions the largest number of configurations, N, was selected in the reactants region.\n",
      "\n",
      "12\n",
      "\n",
      "450\n",
      "\n",
      "OH+H2, T = 300 K, nbeads = 1\n",
      "\n",
      "400\n",
      "\n",
      "350\n",
      "\n",
      "300\n",
      "\n",
      "250\n",
      "\n",
      "N\n",
      "\n",
      "200\n",
      "\n",
      "150\n",
      "\n",
      "100\n",
      "\n",
      "50\n",
      "\n",
      "0\n",
      "\n",
      "-0.05 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1\n",
      "\n",
      "ξ\n",
      "\n",
      "1400\n",
      "\n",
      "CH4+CN, T = 300 K, nbeads = 128\n",
      "\n",
      "1200\n",
      "\n",
      "1000\n",
      "\n",
      "800\n",
      "\n",
      "N\n",
      "\n",
      "600\n",
      "\n",
      "400\n",
      "\n",
      "200\n",
      "\n",
      "0\n",
      "\n",
      "-0.05 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1\n",
      "\n",
      "us\n",
      "\n",
      "Figure 7 Dependence of the number of configurations on the reaction coordinates for the OH+H2 and CH4+CN systems. The numbers are given for the intervals (-0.05, 0), (0, 0.05), ... , (1, 1.05). The transition state is located near the point § = 1, the largest number of configurations N was selected around this point.\n",
      "\n",
      "13\n",
      "\n",
      "0.3 MTP-150 PES, #datapoints: 1634 MTP-250 PES, #datapoints: 2177 MTP-500 PES, #datapoints: 3929 CH4+CN PES\n",
      "\n",
      "0.25\n",
      "\n",
      "1\n",
      "\n",
      "MTP-150 PES, #datapoints: 1634 MTP-250 PES, #datapoints: 2177 MTP-500 PES, #datapoints: 3929\n",
      "\n",
      "0.9\n",
      "\n",
      "CH4+CN PES\n",
      "\n",
      "0.8\n",
      "\n",
      "0.2\n",
      "\n",
      "0.7\n",
      "\n",
      "W(§) / eV\n",
      "\n",
      "0.15\n",
      "\n",
      "1\n",
      "\n",
      "0.6\n",
      "\n",
      "k(t)\n",
      "\n",
      "0.5\n",
      "\n",
      "0.1\n",
      "\n",
      "0.4\n",
      "\n",
      "0.3\n",
      "\n",
      "0.05\n",
      "\n",
      "0.2\n",
      "\n",
      "0\n",
      "\n",
      "-0.2\n",
      "\n",
      "0 0.2\n",
      "\n",
      "0.4 0.6\n",
      "\n",
      "us\n",
      "\n",
      "0.8\n",
      "\n",
      "1\n",
      "\n",
      "0.1\n",
      "\n",
      "1.2\n",
      "\n",
      "0\n",
      "\n",
      "10\n",
      "\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "t / fs\n",
      "\n",
      "40\n",
      "\n",
      "50\n",
      "\n",
      "60\n",
      "\n",
      "Figure 8 The dependence of the accuracy of the potential of mean force and transmission coefficient on the number of parameters in Moment Tensor Potentials (150, 250, and 500) and on the dataset size. The potentials are labeled MTP-150, MTP-250, and MTP-500, respectively. The data is for the CH4 + CN system, T = 300 K, and nbeads = 1. The number of datapoints improves the accuracy of the calculated coefficients.\n",
      "\n",
      "14\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mfactual_checker\u001b[0m (to factual_checker_user):\n",
      "\n",
      "FEEDBACK:\n",
      "The text accurately reflects the content and conclusions of the paper \"Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning.\" The text correctly summarizes the interdisciplinary approach, combining ring polymer molecular dynamics with machine-learning interatomic potentials actively learning on-the-fly (referred to as AL-MTP within the paper) for calculating thermal rate coefficients for gas-phase chemical reactions. Furthermore, the paper content validates the mentioned utility of machine learning methods such as moment tensor potentials (MTP) in exploring potential energy surfaces (PESs) and predicting reaction rates efficiently. The claim that these advancements have yielded promising results in molecular dynamics simulations is corroborated by the paper’s results, demonstrating that the methodology can construct PESs gradually and reliably from a limited set of automatically generated structures, thereby retaining accurate RPMD rate coefficients. These techniques indeed offer tools that can speed up the understanding and discovery process in chemical reactions and material property examination, as mentioned in the text.\n",
      "\n",
      "Summary of Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning [http://arxiv.org/pdf/1805.11924v3]:\n",
      "The paper presents a novel methodology for calculating thermal rate coefficients of gas-phase chemical reactions by combining ring polymer molecular dynamics (RPMD) with machine-learning interatomic potentials using active learning (AL-MTP). The method constructs potential energy surfaces (PESs) from scratch, automatically selecting dataset points during the RPMD simulations, ultimately leading to a reliable description of the PESs. The study tested the methodology on two thermally activated chemical reactions at temperatures between 300-1000 K, finding that the machine-learned PES required significantly fewer structures than traditional methods while retaining accuracy within the typical convergence error of RPMDrate.\n",
      "\n",
      "The authors point to the scalability and accuracy of ring polymer molecular dynamics, highlighting it as a method that roughly scales classically with system size while capturing quantum mechanical effects such as zero-point energy and tunneling, which are critical at low temperatures. They also introduce Moment Tensor Potentials (MTP) and an active learning approach that selects configurations during RPMD simulations to optimize the training set, minimize computational efforts, and improve the PES.\n",
      "\n",
      "The effectiveness of the methodology was demonstrated with two reactions (OH + H2 -> H + H2O and CH4 + CN -> CH3 + HCN), with a comparison against reference rates from previously studied PES. It was shown that the machine-learning-based method could provide accurate predictions for reaction rates with substantially fewer training configurations.\n",
      "\n",
      "The work concludes with a view toward extending the methodology to more complex reaction systems and expresses the potential of the AL-MTP approach to be used in computing other dynamical properties beyond just reaction rates. The authors also acknowledge support and collaboration from partner institutions and colleagues, reiterating that the work is situated at the cutting edge of computational chemistry and machine learning integration.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FEEDBACK:\n",
      "The text accurately reflects the content and conclusions of the paper \"Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning.\" The text correctly summarizes the interdisciplinary approach, combining ring polymer molecular dynamics with machine-learning interatomic potentials actively learning on-the-fly (referred to as AL-MTP within the paper) for calculating thermal rate coefficients for gas-phase chemical reactions. Furthermore, the paper content validates the mentioned utility of machine learning methods such as moment tensor potentials (MTP) in exploring potential energy surfaces (PESs) and predicting reaction rates efficiently. The claim that these advancements have yielded promising results in molecular dynamics simulations is corroborated by the paper’s results, demonstrating that the methodology can construct PESs gradually and reliably from a limited set of automatically generated structures, thereby retaining accurate RPMD rate coefficients. These techniques indeed offer tools that can speed up the understanding and discovery process in chemical reactions and material property examination, as mentioned in the text.\n",
      "\n",
      "Summary of Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning [http://arxiv.org/pdf/1805.11924v3]:\n",
      "The paper presents a novel methodology for calculating thermal rate coefficients of gas-phase chemical reactions by combining ring polymer molecular dynamics (RPMD) with machine-learning interatomic potentials using active learning (AL-MTP). The method constructs potential energy surfaces (PESs) from scratch, automatically selecting dataset points during the RPMD simulations, ultimately leading to a reliable description of the PESs. The study tested the methodology on two thermally activated chemical reactions at temperatures between 300-1000 K, finding that the machine-learned PES required significantly fewer structures than traditional methods while retaining accuracy within the typical convergence error of RPMDrate.\n",
      "\n",
      "The authors point to the scalability and accuracy of ring polymer molecular dynamics, highlighting it as a method that roughly scales classically with system size while capturing quantum mechanical effects such as zero-point energy and tunneling, which are critical at low temperatures. They also introduce Moment Tensor Potentials (MTP) and an active learning approach that selects configurations during RPMD simulations to optimize the training set, minimize computational efforts, and improve the PES.\n",
      "\n",
      "The effectiveness of the methodology was demonstrated with two reactions (OH + H2 -> H + H2O and CH4 + CN -> CH3 + HCN), with a comparison against reference rates from previously studied PES. It was shown that the machine-learning-based method could provide accurate predictions for reaction rates with substantially fewer training configurations.\n",
      "\n",
      "The work concludes with a view toward extending the methodology to more complex reaction systems and expresses the potential of the AL-MTP approach to be used in computing other dynamical properties beyond just reaction rates. The authors also acknowledge support and collaboration from partner institutions and colleagues, reiterating that the work is situated at the cutting edge of computational chemistry and machine learning integration.\n"
     ]
    }
   ],
   "source": [
    "from autosearch.functions.factual_check import factual_check, initialize_factual_check\n",
    "\n",
    "initialize_factual_check(\n",
    "    paper_db=paper_db,\n",
    "    doc_analyzer=analyzer,\n",
    "    project_dir=Project_dir,\n",
    "    db_dir=db_dir,\n",
    "    config_list=config_list,\n",
    "    initiate_db=initiate_db\n",
    ")\n",
    "\n",
    "args = [\n",
    "    {\n",
    "        \"text\": \"The use of neural networks in quantum chemistry, particularly in predicting the properties of molecules and materials, has seen significant advancements. Machine learning models can now compute electronic properties [1][2] and potential energy surfaces [3] with an accuracy that contests traditional quantum chemical methods. Notably, the works of Smith et al. (2017) and Chmiela et al. (2017) have demonstrated how neural networks can predict molecular energies and forces, a process traditionally monopolized by density functional theory (DFT) but at a fraction of the computational cost.\",\n",
    "        \"paper_title\": \"ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost\",\n",
    "        \"paper_url\": \"https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a\",\n",
    "        \"reason\": \"To check the accuracy of the advancement statement regarding neural networks predicting properties in quantum chemistry.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"One major limitation is the quality and quantity of data required to effectively train neural networks. Accurate and diverse datasets of molecular structures and their corresponding properties are essential for developing reliable models, but such data can be scarce and expensive to produce due to the computational resources needed for high-level quantum mechanical calculations.\",\n",
    "        \"paper_title\": \"Machine learning of accurate energy-conserving molecular force fields\",\n",
    "        \"paper_url\": \"http://arxiv.org/pdf/2101.02930v1\",\n",
    "        \"reason\": \"To validate the challenges related to the quality and quantity of data in the context of neural networks within quantum chemistry.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The collaboration between these fields also birthed differentiable programming frameworks for quantum chemistry, like TorchANI and TensorMol [4]. These latter-day frameworks enable researchers to quickly prototype neural networks that learn quantum mechanical laws directly from data, providing a valuable tool to accelerate discovery. This integration has offered promising results in tasks like molecular dynamics simulations, which are key for understanding chemical reactions and material properties.\", \n",
    "        \"paper_title\": \"Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning\", \n",
    "        \"paper_url\": \"http://arxiv.org/pdf/1805.11924v3\", \n",
    "        \"reason\": \"To confirm the factual information about differentiable programming frameworks and their impact on research in quantum chemistry and neural networks.\"\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "for arg in args:    \n",
    "    print(factual_check(**arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
