{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.functions.text_analysis import chunk_pdf\n",
    "\n",
    "from autosearch.database.paper_database import PaperDatabase\n",
    "from autosearch.analysis.document_analyzer import DocumentAnalyzer\n",
    "from autosearch.research_project import ResearchProject\n",
    "from autosearch.write_blog import WriteBlog\n",
    "\n",
    "import autogen\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure credentials from environment variables\n",
    "config={\n",
    "    'doc_api_key': os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\"),\n",
    "    'doc_endpoint': os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "}\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Exploring the Intricacies of Polymer Representation: Unraveling Complexity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging session ID: 98477259-bc8d-4df9-ac50-6e2149524f68\n",
      "Equipping function 'academic_retriever' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'academic_search' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'get_pdf' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'get_pdfs' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'factual_check' to agent 'content_strategist'\n",
      "Equipping function 'academic_retriever' to agent 'content_strategist'\n",
      "Equipping function 'academic_search' to agent 'content_strategist'\n",
      "Equipping function 'get_pdf' to agent 'content_strategist'\n",
      "Equipping function 'get_pdfs' to agent 'content_strategist'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The return type of the function 'wrapper' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equipping function 'write_section' to agent 'writing_coordinator'\n",
      "Equipping function 'factual_check' to agent 'content_review_specialist'\n",
      "Equipping function 'academic_retriever' to agent 'content_review_specialist'\n",
      "Equipping function 'academic_search' to agent 'content_review_specialist'\n",
      "Equipping function 'get_pdf' to agent 'content_review_specialist'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The return type of the function 'wrapper' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equipping function 'plot_figure' to agent 'visualization_specialist'\n",
      "Equipping function 'academic_search' to agent 'topic_expert'\n",
      "Equipping function 'academic_retriever' to agent 'topic_expert'\n",
      "Equipping function 'academic_search' to agent 'research_resource_expert'\n",
      "Equipping function 'academic_retriever' to agent 'research_resource_expert'\n",
      "Equipping function 'get_pdf' to agent 'research_resource_expert'\n",
      "Processing local PDFs...\n",
      "Error: The folder ./papers does not exist.\n"
     ]
    }
   ],
   "source": [
    "test_project = ResearchProject(\n",
    "    project_id = \"project_test\",\n",
    "    version= \"0.2\",\n",
    "    config=config,\n",
    "    config_file=\"OAI_CONFIG_LIST-sweden-505\",\n",
    "    initiate_db= False,\n",
    "    funcClsList = [\"FactualCheck\", \"GetPDF\", \"GetPDFs\", \"UrlCheck\", \"AcademicRetriever\", \"AcademicSearch\", \"WriteSection\", \"PlotFigure\"],\n",
    "    communiteList = [\"outline_agents\", \"write_section_agents\", \"instructor_agents\"],\n",
    "    local_papers_dir=\"./papers\",\n",
    "    models = [\"gpt-35-turbo\", \"gpt-35-turbo-16k\"]\n",
    ")\n",
    "project_config = test_project.ProjectConfig\n",
    "# print(project_config.logging_session_id)\n",
    "# test_project.run(\n",
    "#     title=title,\n",
    "#     target_audience=\"expert in experimental polymer science and machine learning experts\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    ('write_section',\n",
    "     {\n",
    "         \"title\": \"Modern Approaches in Polymer Representation\",\n",
    "         \"brief\": \"Emphasize accessible content on the impact of machine learning on polymer representation with detailed insights from primary sources or direct implementation case studies.\",\n",
    "         \"mind_map\": \"digraph G {\\n   node [shape=box, style=\\\"rounded,filled\\\", color=lightblue2];\\n   // Define nodes\\n   Article [label=\\\"Exploring the Intricacies of Polymer Representation\\\"];\\n   Introduction [label=\\\"Introduction to Polymer Complexity\\\"];\\n   HistoricalOverview [label=\\\"Historical Overview of Polymer Representation Techniques\\\"];\\n   TheoreticalFoundations [label=\\\"Theoretical Foundations of Polymer Modeling\\\"];\\n   ModernApproaches [label=\\\"Modern Approaches in Polymer Representation\\\"];\\n   CaseStudies [label=\\\"Case Studies: Machine Learning in Polymer Representation\\\"];\\n   ChallengesLimitations [label=\\\"Challenges and Limitations in Current Methodologies\\\"];\\n   FutureDirections [label=\\\"Future Directions and Theoretical Implications\\\"];\\n\\n   // Connect nodes\\n   Article -> Introduction;\\n   Article -> HistoricalOverview;\\n   Article -> TheoreticalFoundations;\\n   Article -> ModernApproaches;\\n   Article -> CaseStudies;\\n   Article -> ChallengesLimitations;\\n   Article -> FutureDirections;\\n\\n   // Legends and References\\n   { rank=same; ModernApproaches -> Ref1 [label=\\\"Ref: Representing Polymers as Periodic Graphs [1]\\\"] }\\n   { rank=same; ModernApproaches -> Ref2 [label=\\\"Ref: Potentials and challenges of polymer informatics [2]\\\"] }\\n   { rank=same; CaseStudies -> Ref3 [label=\\\"Ref: Rethinking Interphase Representations [3]\\\"] }\\n}\\n\"\n",
    "     }\n",
    "     ),\n",
    "    # ('factual_check',\n",
    "    #  {\n",
    "    #      \"text\": \"Polymers are substantial cornerstones in the fabric of modern materials science, ubiquitous in applications ranging from packaging and textiles to high-performance engineering plastics and biocompatible materials. The versatility of these macromolecules lies in the infinitude of their structures and compositions, which command their physical and chemical properties. To understand, predict, and manipulate these properties, scientists must adeptly represent polymers at both molecular and macroscopic levels. This section delves into the fundamental concepts of polymer representation, exploring the nuances of chemical structure, topology, and configuration crucial for building a comprehensive understanding of polymer science.\",\n",
    "    #      \"paper_title\": \"Polymer Structure, Properties, and Applications\",\n",
    "    #      \"paper_url\": \"https://example.com/polymer_structure_properties_applications\",\n",
    "    #      \"reason\": \"To verify the general statements about polymer applications, their versatility, and the importance of representation at molecular and macroscopic levels.\"\n",
    "    #  }\n",
    "    #     #  {\n",
    "    #  #     \"text\": \"The collaboration between these fields also birthed differentiable programming frameworks for quantum chemistry, like TorchANI and TensorMol [4]. These latter-day frameworks enable researchers to quickly prototype neural networks that learn quantum mechanical laws directly from data, providing a valuable tool to accelerate discovery. This integration has offered promising results in tasks like molecular dynamics simulations, which are key for understanding chemical reactions and material properties.\",\n",
    "    #  #     \"paper_title\": \"Automated Calculation of Thermal Rate Coefficients using Ring Polymer Molecular Dynamics and Machine-Learning Interatomic Potentials with Active Learning\",\n",
    "    #  #     \"paper_url\": \"http://arxiv.org/pdf/1805.11924v3\",\n",
    "    #     #     \"reason\": \"To confirm the factual information about differentiable programming frameworks and their impact on research in quantum chemistry and neural networks.\"\n",
    "    #     # }\n",
    "    #  ),\n",
    "    # ('get_pdfs', {\n",
    "    #     \"urls\": [\"http://arxiv.org/pdf/2305.13267v1\", \"http://arxiv.org/pdf/2305.06530v1\"],\n",
    "    #     \"reasons\": ['factual_check'] * 2\n",
    "    # }),\n",
    "    # ('get_pdf', {\n",
    "    #     \"url\": \"https://arxiv.org/pdf/2110.13711\",\n",
    "    #     \"reason\": \"factual_check\",\n",
    "    #     \"part\": \"full\"\n",
    "    # }),\n",
    "    # ('url_check',{\n",
    "    #         \"paper_url\": \"https://arxiv.org/pdf/2107.03012.pdf\",\n",
    "    #         \"paper_title\": \"From algebra to analysis: new proofs of theorems by Ritt and Seidenberg\"\n",
    "    # }),\n",
    "    # ('academic_retriever',{\n",
    "    #     \"queries\":[\"Large Language Models\", \"Assessing Language Models\", \"AI safety and reliability\"],\n",
    "    #     \"n_results\":3\n",
    "    # }),\n",
    "    # ('academic_search',{\"query\":\"Human-Centred Learning Analytics and AI in Education: a Systematic Literature Review\"}),\n",
    "]\n",
    "\n",
    "for fucn_name, args in tests:\n",
    "    results = None\n",
    "    for func in test_project.functions:\n",
    "        if func.name == fucn_name:\n",
    "            print(f\"Running {fucn_name} with args: {args}\")\n",
    "            results = func.func(**args)\n",
    "            print(results)\n",
    "\n",
    "    if results is None:\n",
    "        print(f\"Function {fucn_name} not found\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.data.paper import Paper\n",
    "\n",
    "pdf_path = \"/home/alibina/repo/usecases/autosearch_projects/polymer_representation/papers/schmid-2022-understanding-and-modeling-polymers-the-challenge-of-multiple-scales.pdf\"\n",
    "\n",
    "pdf_file = os.path.basename(pdf_path)\n",
    "paper = Paper(\n",
    "            title=os.path.splitext(pdf_file)[0],\n",
    "            authors=[],\n",
    "            url=pdf_path,\n",
    "            source='local'\n",
    "        )\n",
    "\n",
    "\n",
    "processed_paper = test_project.ProjectConfig.doc_analyzer.process_local_pdf(paper, project_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[func.name for func in test_project.functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.functions.plot_figure import plot_figure\n",
    "\n",
    "\n",
    "plot_result = plot_figure(\n",
    "    project_config=project_config,\n",
    "    plot_type=\"line\",\n",
    "    data_description=\"Monthly sales data for the year 2023 for the following data Jan: 1000, Feb: 1200, Mar: 1300, Apr: 1100, May: 1400, Jun: 1500, Jul: 1600, Aug: 1700, Sep: 1800, Oct: 1900, Nov: 2000, Dec: 2100\",\n",
    "    x_label=\"Month\",\n",
    "    y_label=\"Sales ($)\",\n",
    "    title=\"Monthly Sales Trend - 2023\",\n",
    "    additional_instructions=\"Use a blue line with circular markers. Add a legend if possible.\"\n",
    ")\n",
    "\n",
    "print(plot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files_in_directory(directory):\n",
    "    try:\n",
    "        # List all files in the given directory\n",
    "        files = os.listdir(directory)\n",
    "        # Filter out directories, only keep files\n",
    "        files = [f[:-4] for f in files if os.path.isfile(os.path.join(directory, f)) and f.endswith(\".txt\")]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from graphrag.index import run_pipeline, run_pipeline_with_config\n",
    "from graphrag.index.config import PipelineCSVInputConfig, PipelineWorkflowReference\n",
    "from graphrag.index.input import load_input\n",
    "\n",
    "async def index_chunks_with_graphrag(chucks_csv: str, base_dir: str):\n",
    "    \"\"\"\n",
    "    Index chunks using graphrag package.\n",
    "    \n",
    "    Args:\n",
    "    chucks_csv (str): csv file containing chunks to be indexed.\n",
    "    base_dir (str): base directory to save output files.\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    # Configure input\n",
    "    input_config = PipelineCSVInputConfig(\n",
    "        file_pattern=chucks_csv,\n",
    "        base_dir=base_dir,\n",
    "        source_column=\"url\",  # Using 'url' as the source column\n",
    "        text_column=\"page_content\",\n",
    "        timestamp_column=\"last_updated_date\",  # Using 'last_updated_date' as the timestamp column\n",
    "        timestamp_format=\"%Y-%m-%dT%H:%M:%S%z\",  # Format for ISO 8601 timestamps\n",
    "        title_column=\"title\",\n",
    "    )\n",
    "\n",
    "    # Load input\n",
    "    dataset = await load_input(input_config)\n",
    "\n",
    "    # Define workflows\n",
    "    workflows = [\n",
    "        PipelineWorkflowReference(\n",
    "            name=\"entity_extraction\",\n",
    "            config={\n",
    "                \"entity_extract\": {\n",
    "                    \"strategy\": {\n",
    "                        \"type\": \"nltk\",\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        ),\n",
    "        PipelineWorkflowReference(\n",
    "            name=\"entity_graph\",\n",
    "            config={\n",
    "                \"cluster_graph\": {\"strategy\": {\"type\": \"leiden\"}},\n",
    "                \"embed_graph\": {\n",
    "                    \"strategy\": {\n",
    "                        \"type\": \"node2vec\",\n",
    "                        \"num_walks\": 10,\n",
    "                        \"walk_length\": 40,\n",
    "                        \"window_size\": 2,\n",
    "                        \"iterations\": 3,\n",
    "                        \"random_seed\": 597832,\n",
    "                    }\n",
    "                },\n",
    "                \"layout_graph\": {\n",
    "                    \"strategy\": {\n",
    "                        \"type\": \"umap\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Run pipeline\n",
    "    tables = []\n",
    "    async for table in run_pipeline(dataset=dataset, workflows=workflows):\n",
    "        tables.append(table)\n",
    "    \n",
    "    pipeline_result = tables[-1]\n",
    "\n",
    "    # Process and save results\n",
    "    if pipeline_result.result is not None:\n",
    "        result_df = pipeline_result.result\n",
    "        result_df.to_csv(os.path.join(output_dir, \"indexed_chunks.csv\"), index=False)\n",
    "        print(f\"Indexed chunks saved to {os.path.join(output_dir, 'indexed_chunks.csv')}\")\n",
    "        \n",
    "        # Print some sample results\n",
    "        first_result = result_df.head(1)\n",
    "        print(f\"level: {first_result['level'].iloc[0]}\")\n",
    "        print(f\"embeddings: {first_result['embeddings'].iloc[0]}\")\n",
    "        print(f\"entity_graph_positions: {first_result['node_positions'].iloc[0]}\")\n",
    "    else:\n",
    "        print(\"No results from indexing!\")\n",
    "\n",
    "    # Clean up temporary file\n",
    "    os.remove(temp_csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosearch.api.arxiv_api import ArxivAPI\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "def process_single_paper(arxiv_id: str, ProjectConfig, max_token_size: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a single arXiv paper and return its chunks.\n",
    "    \"\"\"\n",
    "    arxiv_api = ArxivAPI()\n",
    "    try:\n",
    "        # Fetch the paper metadata\n",
    "        paper = arxiv_api.get_paper_metadata(arxiv_id)\n",
    "        \n",
    "        # Get the chunks\n",
    "        chunks = ProjectConfig.doc_analyzer.pdf2md_chunck(paper, max_token_size)\n",
    "        \n",
    "        paper_data = []\n",
    "        \n",
    "        # Process each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk_data = {\n",
    "                'arxiv_id': arxiv_id,\n",
    "                'page_content': chunk.page_content,\n",
    "            }\n",
    "            \n",
    "            # Unfold the metadata\n",
    "            if hasattr(chunk, 'metadata'):\n",
    "                for key, value in chunk.metadata.items():\n",
    "                    # Convert complex types to JSON strings\n",
    "                    if isinstance(value, (list, dict, np.integer, np.floating, np.ndarray)):\n",
    "                        chunk_data[f'metadata_{key}'] = json.dumps(value, cls=NumpyEncoder)\n",
    "                    else:\n",
    "                        chunk_data[f'metadata_{key}'] = value\n",
    "            \n",
    "            # Add paper information\n",
    "            paper_dict = paper.to_dict()\n",
    "            for k, v in paper_dict.items():\n",
    "                if isinstance(v, (list, dict, np.integer, np.floating, np.ndarray)):\n",
    "                    chunk_data[k] = json.dumps(v, cls=NumpyEncoder)\n",
    "                else:\n",
    "                    chunk_data[k] = v\n",
    "            \n",
    "            paper_data.append(chunk_data)\n",
    "        \n",
    "        return paper_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing paper {arxiv_id}: {str(e)}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 21 papers...\n",
      "PDF file already exists: ./project_test/0.2/output/2105.05278v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2102.08134v2.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2403.20021v2.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2209.14803v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2109.02794v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2311.14744v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/1002.2059v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2205.08619v1.pdf\n",
      "Created 17 docs with a total of 6477 tokens. Largest doc has 954 tokens.\n",
      "Processed paper: 2311.14744v1\n",
      "Created 8 docs with a total of 3450 tokens. Largest doc has 1184 tokens.\n",
      "Processed paper: 1002.2059v1\n",
      "Created 20 docs with a total of 11033 tokens. Largest doc has 1185 tokens.\n",
      "Processed paper: 2102.08134v2\n",
      "PDF file already exists: ./project_test/0.2/output/2209.13557v2.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2103.14174v1.pdf\n",
      "Created 23 docs with a total of 13794 tokens. Largest doc has 1195 tokens.\n",
      "Processed paper: 2209.14803v1\n",
      "Created 11 docs with a total of 7998 tokens. Largest doc has 1191 tokens.\n",
      "Error processing paper 751: Invalid ArXiv identifier or URL: 751\n",
      "Processed paper: 2403.20021v2\n",
      "Processed paper: 751\n",
      "PDF file already exists: ./project_test/0.2/output/0912.3344v1.pdf\n",
      "Created 34 docs with a total of 17380 tokens. Largest doc has 1197 tokens.\n",
      "Created 9 docs with a total of 5511 tokens. Largest doc has 1199 tokens.\n",
      "Processed paper: 2209.13557v2\n",
      "Processed paper: 2103.14174v1\n",
      "PDF file already exists: ./project_test/0.2/output/2205.13757v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2209.01307v4.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2311.15481v3.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2212.08945v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2011.00508v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2312.04013v3.pdf\n",
      "Created 44 docs with a total of 26020 tokens. Largest doc has 1199 tokens.\n",
      "Processed paper: 2205.08619v1\n",
      "Created 18 docs with a total of 13880 tokens. Largest doc has 1197 tokens.\n",
      "Processed paper: 2109.02794v1\n",
      "Created 20 docs with a total of 11570 tokens. Largest doc has 1185 tokens.\n",
      "Processed paper: 0912.3344v1\n",
      "Created 26 docs with a total of 12238 tokens. Largest doc has 1188 tokens.\n",
      "Processed paper: 2205.13757v1\n",
      "Created 60 docs with a total of 31569 tokens. Largest doc has 1196 tokens.\n",
      "Processed paper: 2011.00508v1\n",
      "Created 32 docs with a total of 22804 tokens. Largest doc has 1197 tokens.\n",
      "Processed paper: 2312.04013v3\n",
      "Created 34 docs with a total of 23516 tokens. Largest doc has 1196 tokens.\n",
      "Processed paper: 2311.15481v3\n",
      "PDF file already exists: ./project_test/0.2/output/2406.04727v2.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/2010.07683v1.pdf\n",
      "PDF file already exists: ./project_test/0.2/output/1812.11212v1.pdf\n",
      "Created 53 docs with a total of 27457 tokens. Largest doc has 1199 tokens.\n",
      "Processed paper: 2105.05278v1\n",
      "Created 31 docs with a total of 17403 tokens. Largest doc has 1200 tokens.\n",
      "Processed paper: 2406.04727v2\n",
      "Created 21 docs with a total of 14190 tokens. Largest doc has 1199 tokens.\n",
      "Processed paper: 2010.07683v1\n",
      "Created 66 docs with a total of 55774 tokens. Largest doc has 1197 tokens.\n",
      "Processed paper: 2212.08945v1\n",
      "Created 47 docs with a total of 25353 tokens. Largest doc has 1199 tokens.\n",
      "Processed paper: 2209.01307v4\n",
      "Created 28 docs with a total of 18117 tokens. Largest doc has 1199 tokens.\n",
      "Processed paper: 1812.11212v1\n",
      "Data saved to ./graphrag/input/arxiv_papers_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "def process_arxiv_papers(arxiv_ids: List[str], ProjectConfig, output_dir: str = \".\", max_workers: int = 5, max_token_size: int = 1200):\n",
    "    \"\"\"\n",
    "    Process a list of arXiv papers in parallel, extract chunks, and combine them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    arxiv_ids (List[str]): List of arXiv IDs to process.\n",
    "    output_dir (str): Directory to save the CSV file. Defaults to current directory.\n",
    "    max_workers (int): Maximum number of worker threads. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Combined DataFrame of all chunks from all papers.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_arxiv_id = {executor.submit(process_single_paper, arxiv_id, ProjectConfig, max_token_size): arxiv_id for arxiv_id in arxiv_ids}\n",
    "        for future in as_completed(future_to_arxiv_id):\n",
    "            arxiv_id = future_to_arxiv_id[future]\n",
    "            try:\n",
    "                paper_data = future.result()\n",
    "                all_data.extend(paper_data)\n",
    "                print(f\"Processed paper: {arxiv_id}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"Paper {arxiv_id} generated an exception: {exc}\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_chunks = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_filename = os.path.join(output_dir, f\"arxiv_papers_chunks.csv\")\n",
    "    df_chunks.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "  \n",
    "    \n",
    "    return df_chunks, csv_filename\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "paper_dir = \"/home/azureuser/autogen_uscases/autosearch/notebooks/graphrag/input-txt\"\n",
    "# arxiv_ids = list_files_in_directory(paper_dir)\n",
    "arxiv_ids = ['2209.14803v1', '2103.14174v1', '2105.05278v1', '2403.20021v2', '1002.2059v1', '2205.08619v1', '2311.14744v1', '2109.02794v1', '2102.08134v2', '2209.13557v2', '2209.01307v4', '2205.13757v1', '0912.3344v1', '2311.15481v3', '751', '2212.08945v1', '2011.00508v1', '2312.04013v3', '2406.04727v2', '2010.07683v1', '1812.11212v1']\n",
    "\n",
    "print(f\"Processing {len(arxiv_ids)} papers...\")\n",
    "output_directory = \"./graphrag/input\"\n",
    "max_workers = 10\n",
    "\n",
    "# Run the process\n",
    "df_chunks, csv_filename = process_arxiv_papers(arxiv_ids, project_config, output_directory, max_workers, max_token_size=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await index_chunks_with_graphrag(os.path.abspath(csv_filename), base_dir='/home/azureuser/autogen_uscases/autosearch/notebooks/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2209.14803v1', '2103.14174v1', '2105.05278v1', '2403.20021v2', '1002.2059v1', '2205.08619v1', '2311.14744v1', '2109.02794v1', '2102.08134v2', '2209.13557v2', '2209.01307v4', '2205.13757v1', '0912.3344v1', '2311.15481v3', '751', '2212.08945v1', '2011.00508v1', '2312.04013v3', '2406.04727v2', '2010.07683v1', '1812.11212v1']\n"
     ]
    }
   ],
   "source": [
    "print(arxiv_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['arxiv_id', 'page_content', 'metadata_source', 'metadata_pages',\n",
       "       'metadata_tokens', 'title', 'authors', 'url', 'pdf_url', 'local_path',\n",
       "       'abstract', 'published_date', 'last_updated_date', 'source', 'summary',\n",
       "       'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
