## 1 Introduction

arXiv:2209.01307v4 [cs.LG] 26 Apr 2023

TransPolymer: a Transformer-based language model for polymer property predictions

Changwen Xu, ++ Yuyang Wang, ,I and Amir Barati Farimani*, 1$, 1,ยง + Department of Materials Science and Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA $Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA ยง Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA

E-mail: barati@cmu.edu

Abstract

Accurate and efficient prediction of polymer properties is of great significance in polymer design. Conventionally, expensive and time-consuming experiments or sim- ulations are required to evaluate polymer functions. Recently, Transformer models, equipped with self-attention mechanisms, have exhibited superior performance in nat- ural language processing. However, such methods have not been investigated in poly- mer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer property prediction. Our proposed polymer tokenizer with chemical aware- ness enables learning representations from polymer sequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on large unlabeled dataset via Masked Language Modeling. Experimental results further

1

manifest the important role of self-attention in modeling polymer sequences. We high- light this model as a promising computational tool for promoting rational polymer design and understanding structure-property relationships from a data science view.

The accurate and efficient property prediction is essential to the design of polymers in various applications, including polymer electrolytes, 1,2 organic optoelectronics, 3,4 energy storage, 5,6 and many others.7,8 Rational representations which map polymers to continuous vector space are crucial to applying machine learning tools in polymer property prediction. Fingerprints (FPs), which have been proven to be effective in molecular machine learning models, are introduced for polymer-related tasks.9 Recently, deep neural networks (DNNs) have revo- lutionized polymer property prediction by directly learning expressive representations from data to generate deep fingerprints, instead of relying on manually engineered descriptors. 10 Rahman et al. used convolutional neural networks (CNNs) for the prediction of mechani- cal properties of polymer-carbon nanotube surfaces, 11 whereas CNNs suffered from failure to consider molecular structure and interactions between atoms. Graph neural networks (GNNs), 12 which have outperformed many other models on several molecules and polymer benchmarks, 13-17 are capable of learning representations from graphs and finding optimal fingerprints based on downstream tasks. 10 For example, Park et al. 18 trained graph convo- lutional neural networks (GCNN) for predictions of thermal and mechanical properties of polymers and discovered that the GCNN representations for polymers resulted in compara- ble model performance to the popular extended-connectivity circular fingerprint (ECFP) 19,20 representation. Recently, Aldeghi et al. adapted a graph representation of molecular en- sembles along with a GNN architecture to capture pivotal features and accomplish accu- rate predictions of electron affinity and ionization potential of conjugated polymers. 21 How- ever, GNN-based models require explicitly known structural and conformational information, which would be computationally or experimentally expensive to obtain. Plus, the degree of

2

polymerization varies for each polymer, which makes it even harder to accurately represent polymers as graphs. Using the repeating unit only as graph is likely to result in missing structural information. Therefore, the optimal method of graph representation for polymers is still obscure.

Meanwhile, language models, like recurrent neural networks (RNNs) based models, 22-25 treat polymers as character sequences for featurization. Chemistry sequences have the same structure as a natural language like English, as suggested by Cadeddu et al., in terms of the distribution of text fragments and molecular fragments.26 This elucidates the development of sequence models similar to those in computational linguistics for extracting information from chemical sequences and realizing the intuition of understanding chemical texts just like understanding natural languages. Multiple works have investigated the development of deep language models for polymer science. Simine et al. managed to predict spectra of conjugated polymers by long short-term memory (LSTM) from coarse-grained represen- tations of polymers.27 Webb et al. propose coarse-grained polymer genomes as sequences and apply LSTM to predict the properties of different polymer classes. 28 Patel et al. fur- ther extend the coarse-grained string featurization to copolymer systems and develop GNN, CNN, as well as LSTM to model encoded copolymer sequences. 29 Bhattacharya et al. lever- age RNNs with sequence embedding to predict aggregate morphology of macromolecules. 30 Plus, sequence models could represent molecules and polymers with Simplified Molecular-

Input Line-Entry system (SMILES) 31 and convert the strings to embeddings for vectoriza- tion. Some works, like BigSMILES, 32 have also investigated the string-based encoding of macromolecules. Goswami et al. created encodings from polymer SMILES as input for the LSTM model for polymer glass transition temperature prediction. 33 However, RNN-based models are generally not competitive enough to encode chemical knowledge from polymer sequences because they rely on previous hidden states for dependencies between words and tend to lose information when they reach deeper steps. In recent years, the exceptionally su- perior performance demonstrated by Transformer34 on numerous natural language processing

3

(NLP) tasks has shed light on studying chemistry and materials science by language models. Since proposed, Transformer and its variants have soon brought about significant changes in NLP tasks over the past few years. Transformer is featured with using attention mecha- nism only so that it can capture relationships between tokens in a sentence without relying on past hidden states. Many Transformer-based models like BERT, 35 RoBERTa,36 GPT,37 ELMo, 38 and XLM39 have emerged as effective pretraining methods by self-supervised learn- ing of representations from unlabeled texts, leading to performance enhancement on various downstream tasks. On this account, many works have already applied Transformer on prop- erty predictions of small organic molecules. 40-43 SMILES-BERT was proposed to pretrain the model of BERT-like architecture through a masked SMILES recovery task and then generalize into different molecular property prediction tasks. 44 Similarly, ChemBERTa, 45 a RoBERTa-like model for molecular property prediction, was also introduced, following the pretrain-finetune pipeline. ChemBERTa demonstrated competitive performance on multiple downstream tasks and scaled well with the size of pretraining datasets. Transformer-based models could even be used for processing reactions. Schwaller et al. mimicked machine translation tasks and trained Transformer on reaction sequences represented by SMILES for reaction prediction with high accuracy.46 Recently, Transformer has been further proven to be effective as a structure-agnostic model in material science tasks, for example, predicting MOF properties based on a text string representation. 47 Despite the wide investigation of Transformer for molecules and materials, such models have not yet been leveraged to learn representations of polymers. Compared with small molecules, designing Transformer-based models for polymers is more challenging because the standard SMILES encoding fails to model the polymer structure and misses fundamental factors influencing polymer properties like degree of polymerization and temperature of measurement. Moreover, the polymer se- quences used as input should contain information on not only the definition of monomers but also the arrangement of monomers in polymers. 48 In addition, sequence models for polymers are confronted with an inherent scarcity of handy, well-labeled data, considering the hard

4

work in the characterization process in the laboratory. The situation becomes even worse when some of the polymer data sources are not fully accessible. 49,50

Herein, we propose TransPolymer, a Transformer-based language model for polymer property predictions. To the best of our knowledge, it is the first work to introduce the Transformer-based model to polymer sciences. Polymers are represented by sequences based on SMILES of their repeating units as well as structural descriptors and then tokenized by a chemically-aware tokenizer as the input of TransPolymer, shown in Fig. 1(a). Even though there is still information which cannot be explicitly obtained from input sequences, like bond angles or overall polymer chain configuration, such information can still be learned implicitly by the model. TransPolymer consists of a RoBERTa architecture and a multi- layer perceptron (MLP) regressor head, for predictions of various polymer properties. In the pretraining phase, TransPolymer is trained through Masked Language Modeling (MLM) with approximately 5M augmented unlabeled polymers from the PI1M database. 51 In MLM, tokens in sequences are randomly masked and the objective is to recover the original tokens based on the contexts. Afterward, TransPolymer is finetuned and evaluated on ten datasets of polymers concerning various properties, covering polymer electrolyte conductivity, band gap, electron affinity, ionization energy, crystallization tendency, dielectric constant, refrac- tive index, and p-type polymer OPV power conversion efficiency. 52-55 For each entry in the datasets, the corresponding polymer sequence, containing polymer SMILES as well as useful descriptors like temperature and special tokens are tokenized as input of TransPolymer. The pretraining and finetuning processes are illustrated in Fig. 1(b) and (d). Data augmentation is also implemented for better learning of features from polymer sequences. TransPolymer achieves state-of-the-art (SOTA) results on all ten benchmarks and surpasses other baseline models by large margins in most cases. Ablation studies provide further evidence of what contributes to the superior performance of TransPolymer by investigating the roles of MLM pretraining on large unlabeled data, finetuning both Transformer encoders and the regressor head, and data augmentation. The evidence from visualization of attention scores illustrates

5

that TransPolymer can encode chemical information about internal interactions of polymers and influential factors of polymer properties. Such a method learns generalizable features that can be transferred to property prediction of polymers, which is of great significance in polymer design.

## 2.1 TransPolymer framework

Our TransPolymer framework consists of tokenization, Transformer encoder, pretraining, and finetuning. Each polymer data is first converted to a string of tokens through tokeniza- tion. Polymer sequences are more challenging to design than molecule or protein sequences as polymers contain complex hierarchical structures and compositions. For instance, two polymers that have the same repeating units can vary in terms of the degree of polymeriza- tion. Therefore, we propose a chemical-aware polymer tokenization method as shown in Fig. 1(a). The repeating units of polymers are embedded using SMILES and additional descrip- tors (e.g., degree of polymerization, polydispersity, and chain conformation) are included to model the polymer system. Plus, copolymers are modeled by combining the SMILES of each constituting repeating unit along with the ratios and the arrangements of those re- peating units. Moreover, materials consisting of mixtures of polymers are represented by concatenating the sequences for each component as well as the descriptors for the materials. Besides, each token represents either an element, the value of a polymer descriptor, or a special separator. Therefore, the tokenization strategy is chemical-aware and thus has an edge over the tokenizer trained for natural languages which tokenizes based on single letters. More details about the design of our chemical-aware tokenization strategy could be found in the Methods section.

Transformer encoders are built upon stacked self-attention and point-wise, fully connected layers, 34 shown in Fig. 1(c). Unlike RNN or CNN models, Transformer depends on the

6

a

* C C

... ... 1 = 0 $

NAN_ratio $ S_1 $

124

Polymer

ratio

Type

Tg

*CC1CC(*)C(=O)O[B-]2(OC(=O)C(=O)02)OC1=0

$ NAN_ratio

$ S_1

$

124

Component 1

:unselected: = SMILES

:unselected: = Separator

*

C

...

0 * $ 5.0

I$

S_2 $ NAN_Tg

2.05

$ 69

Copolymer

ratio

Type

Tg

ratio

Temp

*CC(*)(C)C(=O)OCC[^] .* CCO* |$

5.0

$

S_2

$ NAN_Tg

2.05

$ 69

Component 2

Material Descriptors

:unselected: = Component Descriptors

:unselected: = Material Descriptors

b

c

Step 1: Pretrain

TransPolymer

Masked

Language Model

/ Unlabeled I sequences I

Add & Norm

Output

Feed Forward

NX

Step 2: Finetune

TransPolymer

Property Prediction

1 Labeled I sequences I

d

Prediction Head

Token

Embed <s>

Embed 1

Embed Mask

.. ..

:unselected:

:unselected:

:unselected:

:unselected:

:unselected:

:unselected: Embed </s>

Trans Polymer

<s>

Token 1

Mask

..

</s>

Polymer Sequence

Q *

KT

Softmax

*

V

Embedding Size

Add & Norm

Multi-Head Attention

Q

V

K :selected:

Input

Emebedding

1

Regressor Head

Property

Embed <s>

Embed 1

Embed 2

Embed </s>

.. ...

:unselected: :unselected:

:unselected: :unselected:

:unselected:

:unselected:

:unselected: :unselected: Trans Polymer

<s>

Token 1

Token 2

.. ...

</s>

Polymer Sequence

Figure 1: (a) Polymer tokenization. Illustrated by the example, the sequence which com- prises components with polymer SMILES and other descriptors is tokenized with chemical awareness. (b) The whole TransPolymer framework with a pretrain-finetune pipeline. (c) Sketch of Transformer encoder and multi-head attention. (d) Illustration of the pretraining (left) and finetuning (right) phases of TransPolymer. The model is pretrained with Masked Language Modeling to recover original tokens, while the feature vector corresponding to the special token '(s)' of the last hidden layer is used for prediction when finetuning. Within the TransPolymer block, lines of deeper color and larger width stand for higher attention scores.

7

self-attention mechanism that relates tokens at different positions in a sequence to learn representations. Scaled dot-product attention across tokens is applied which relies on the query, key, and value matrices. More details about self-attention can be found in the Methods section. In our case, the Transformer encoder is made up of 6 hidden layers and each hidden layer contains 12 attention heads. The hyperparameters of TransPolymer are chosen by starting from the common setting of RoBERTa 36 and then tuned according to model performance.

To learn better representations from large unlabeled polymer data, the Transformer en- coder is pretrained via Masked Language Modeling (MLM), a universal and effective pre- training method for various NLP tasks. 56-58 As shown in Fig. 1(d) (left), 15% of tokens of a sequence are randomly chosen for possible replacement, and the pretraining objective is to predict the original tokens by learning from the contexts. The pretrained model is then finetuned for predicting polymer properties with labeled data. Particularly, the final hidden vector of the special token '(s)' at the beginning of the sequence is fed into a regressor head which is made up of one hidden layer with SiLU as the activation function for prediction as illustrated in Fig. 1(d) (right).

## 2.2 Experimental settings

PI1M, the benchmark of polymer informatics, is used for pretraining. The benchmark, whose size is around 1M, was built by Ma et al. by training a generative model on polymer data col- lected from the PolyInfo database. 51,59 The generated sequences consist of monomer SMILES and '*' signs representing the polymerization points. The ~1M database was demonstrated to cover similar chemical space as PolyInfo but populate space where data in PolyInfo are sparse. Therefore, the database can serve as an important benchmark for multiple tasks in polymer informatics.

To finetune the pretrained TransPolymer, ten datasets are used in our experiments which cover various properties of different polymer materials, and the distributions of polymer se-

8

Table 1: Summary of datasets for downstream tasks.



|Dataset|Property|# Data|# Augmented train data|# Test data|Data split|
|---|---|---|---|---|---|
|PE-[53|conductivity|9185|34803|146|train-test split by year|
|PE-II52|conductivity|271|8864|55|5-fold cross-validation|
|Egc54|bandgap (chain)|3380|5408|676|5-fold cross-validation|
|Egb 54|bandgap (bulk)|561|6443|113|5-fold cross-validation|
|Eea 54|electron affinity|368|3993|74|5-fold cross-validation|
|E; 54|ionization energy|370|4000|74|5-fold cross-validation|
|Xc54
:selected:|crystallization tendency|432|8837|87|5-fold cross-validation|
|EPS 54|dielectric constant|382|4188|77|5-fold cross-validation|
|Nc54|refractive index|382|4188|77|5-fold cross-validation|
|OPV55|power conversion efficiency|1203|4810|241|5-fold cross-validation|


We apply data augmentation to each dataset that we use by removing canonicalization from SMILES and generating non-canonical SMILES which correspond to the same structure as the canonical ones. For PI1M database, each data entry is augmented to five so that the augmented dataset with the size of ~5M is used for pretraining. For downstream datasets, we limit the numbers of augmented SMILES for large datasets with long SMILES for the following reasons: long SMILES tend to generate more non-canonical SMILES which might alter the original data distribution; we are not able to use all the augmented data for fine- tuning given the limited computation resources. We include the number of data points after augmentation in Table 1 and summarize the augmentation strategy for each downstream dataset in Supplementary Table 1.

9

## 2.3 Polymer property prediction results

The performance of our pretrained TransPolymer model on ten property prediction tasks is illustrated below. We use root mean square error (RMSE) and R2 as metrics for evaluation. For each benchmark, the baseline models and data splitting are adopted from the original literature. Except for PE-I which is trained on data from the year 2018 and evaluated on data from the year 2019, all other datasets are split by five-fold cross-validation. When cross- validation is used, the metrics are calculated by taking the average of those by each fold. We also train Random Forest models using Extended Connectivity Fingerprint (ECFP), 19,20 one of the state-of-the-art fingerprint approaches, to compare with TransPolymer. Besides, we develop long short-term memory (LSTM), another widely used language model, as well as unpretrained TransPolymer trained purely via supervised learning as baseline models in all the benchmarks. TransPolymer unpretrained and TransPolymerpretrained denote unpretrained and pretrained TransPolymer, respectively.

The results of TransPolymer and baselines on PE-I are illustrated in Table 2. The original literature used gated GNN to generate fingerprints for the prediction of polymer electrolyte conductivity by Gaussian process. 53 The fingerprints are also passed to random forest and supporting vector machine (SVM) for comparison. Another random forest is trained based on ECFP fingerprints. The results of most baseline models indicate strong overfitting which is attributed to the introduction of unconventional conductors consisting of conjugated poly- benzimidazole and ionic liquid. For instance, Gaussian Process trained on GNN fingerprints achieves a R2 of 0.90 on the training set but only 0.16 on the test set, and GNN + Ran- dom Forest gets a negative test R2 even the train R2 is 0.91. Random Forest trained on ECFP fingerprints stands out among all the baseline models, whereas its performance on test dataset is still poor. However, TransPolymerpretrained not only achieves the highest scores on the training set but also improves the performance on the test set significantly, which is illustrated by the R2 of 0.69 on the test set. Such information demonstrates that TransPoly- mer is capable of learning the intrinsic relationship between polymers and their properties

10

a

-2

.

-4

PE-I Dataset

train data

test data

b

-2

PE-II Dataset

train data

.

-4

test data

c

Egc Dataset

10

.

train data

.

test data

8

-6

Predicted Conductivity (log S/com)

-8

Predicted Conductivity (log S/com)

-6

6

Predicted Bandgap (chain) (eV)

4

-10

-8

-12

2

0

-12 -10 -8 -6 -4 -2 Experimental Conductivity (log S/com)

-6 Experimental Conductivity (log S/com)

-2

0

d

Egb Dataset

101

. train data

.

test data

e

Eea Dataset

. train data

. test data

f

10

9

2 4

6

8

10

Theoretical Bandgap (chain) (eV)

Ei Dataset

ยท

train data

.

test data

8

'4

8

6

Predicted Bandgap (bulk) (eV)

4

3

Predicted Electron Affinity (eV)

7

Predicted Ionization Energy (eV)

6

5

2

O

g

4 6 2 Theoretical Bandgap (bulk) (eV)

8 10

Xc Dataset

h

9

2

5

Theoretical Electron Affinity (eV)

EPS Dataset

i

3.0

4

5 6

+

8

Theoretical Ionization Energy (eV)

Nc Dataset

100

80

. train data

. test data

. train data . test data

8

7

1 2.5

ยท train data test data

.

Predicted Crystallization Tendency (%)

60

40}

Predicted Dielectric Constant

6

Predicted Refractive Index

2.0

20

20

40

60

80 100

Theoretical Crystallization Tendency (%)

j

3 5

6

7

8 9

Theoretical Dielectric Constant

OPV Dataset

1.54 1.5

2.0

3.0

Theoretical Refractive Index 2.5

10 train data . test data

8

6

Predicted PCE (%)

4

2

0

2

4

6 8 TO

Experimental PCE (%)

Figure 2: Scatter plots of ground truth vs. predicted values by TransPolymer pretrained for downstream datasets: (a) PE-I, (b) PE-II, (c) Egc, (d) Egb, (e) Eea, (f) Ei, (g) Xc, (h) EPS, (i) Nc, and (j) OPV. The dashed lines on diagonals stand for perfect regression.

11

Table 2: Performance of TransPolymer and baseline models on PE-I.



|Model|Train RMSE (S/cm ) (4)|Test RMSE (S/cm ) (4)|Train R2 (1)|Test R2 (1)|
|---|---|---|---|---|
|Gaussian Process (GNN FP)|0.55|0.97|0.90|0.16|
|Random Forest (GNN FP)|0.50|2.23|0.91|-2.64|
|SVM (GNN FP)|1.34|2.12|0.04|-1.94|
|Random Forest (ECFP)|0.15|1.00|0.99|0.32|
|LSTM|1.03|1.36|0.67|-0.25|
|TransPolymer unpretrained|0.88|1.02|0.70|0.30|
|TransPolymer pretrained|0.20|0.67|0.98|0.69|


* The units are in logarithm scale

Table 3: Performance of TransPolymer and baseline models on PE-II.



|Model|Train RMSE (S/cm ) (4)|Test RMSE (S/cm ) (4)|Train R2 (1)|Test R2 (1)|
|---|---|---|---|---|
|Ridge (Chemical descriptors)|0.58|0.67|0.77|0.58|
|Random Forest (Chemical descriptors)|0.26|0.64|0.96|0.71|
|Gradient Boosting (Chemical descriptors)|0.00|0.66|0.99|0.68|
|Extra Trees (Chemical descriptors)|0.10|0.63|0.98|0.72|
|Random Forest (ECFP)|0.22|0.94|0.96|0.27|
|LSTM|1.16|1.18|0.05|0.00|
|TransPolymer unpretrained|0.18|0.80|0.97|0.54|
|TransPolymer pretrained|0.18|0.61|0.96|0.73|


* The units are in logarithm scale

and suffers less from overfitting. Notably, TransPolymer unpretrained also achieves competitive results and shows mild overfitting compared with other baseline models. This indicates the effectiveness of the attention mechanism of Transformer-based models. The scatter plots of ground truth vs. predicted values for PE-I by TransPolymerpretrained are illustrated in Fig. 2(a) and Supplementary Figure 2(a).

As is shown in Table 3, the results of TransPolymer and baselines including Ridge, random forest, gradient boosting, and extra trees which were trained on chemical descriptors gener- ated from polymers from PE-II in the original paper52 are listed, as well as random forest trained on ECFP fingerprints. Although gradient boosting surpasses other models on train- ing sets by obtaining nearly perfect regression outcomes, its performance on test sets drops significantly. In contrast, TransPolymerpretrained, which achieves the lowest RMSE of 0.61 and highest R2 of 0.73 on the average of cross-validation sets, exhibits better generalization.

The scatter plots of ground truth vs. predicted values for PE-II by TransPolymer pretrained are illustrated in Fig. 2(b) and Supplementary Figure 2(b).

12

Table 4: Performance of TransPolymer and baseline models on datasets from literature by Kuenneth et al. 54



|Model|Egc (eV)|Test RMSE (4)||||EPS|Nc|Egc|Egb|Test R2 (1)||||Nc|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||Egb (eV)|Eea (eV)|Ei (eV)|Xc (%)|||||Eea|Ei|Xc|EPS||
|Gaussian Process (PG)|0.48|0.55|0.32|0.42|24.42|0.53|0.10|0.90|0.91|0.90|0.77|<0|0.68|0.79|
|Neural Network (PG)|0.49|0.57|0.32|0.45|20.74|0.54|0.10|0.89|0.89|0.87|0.74|<0|0.71|0.78|
|Random Forest (ECFP)|0.81|0.88|0.56|0.58|25.61|0.75|0.14|0.65|0.66|0.70|0.57|-0.29|0.50|0.56|
|LSTM|0.58|1.94|1.04|0.94|23.67|1.11|0.23|0.86|0.00|0.06|0.10|0.00|-0.02|0.02|
|TransPolymer unpretrained|0.63|0.61|0.36|0.46|20.11|0.59|0.10|0.84|0.90|0.89|0.78|0.27|0.70|0.80|
|TransPolymer pretrained|0.44|0.52|0.32|0.39|16.57|0.52|0.10|0.92|0.93|0.91|0.84|0.50|0.76|0.82|


Table 4 summarizes the performance of TransPolymer and baselines on Egc, Egb, Eea, Ei, Xc, EPS, and Nc datasets from Kuenneth et al.54 In the original literature, both Gaus- sian process and neural networks were trained on each dataset with polymer genome (PG) fingerprints 60 as input, some of which resulted in desirable performance while some of which did not. Meanwhile, PG fingerprints are demonstrated to surpass ECFP fingerprints on the datasets used by Kuenneth et al. For Egc, Egb, and Eea, despite the high scores by other models, TransPolymer pretrained is still able to enhance the performance, lowering RMSE and enhancing R2. In contrast, baseline models perform poorly on Xc whose test R2 scores are less than 0. However, TransPolymerpretrained significantly lowers test RMSE and increases R2 to 0.50. Notably, The authors of the original paper used multi-task learning to enhance model performance and achieved higher scores than TransPolymerpretrained on some of the datasets, like Egb, EPS, and Nc (the average test RMSE and R2 are 0.43 and 0.95 for Egb, 0.39 and 0.86 for EPS, and 0.07 and 0.91 for Nc, respectively). Access to multiple prop- erties of one polymer, however, may not be available from time to time, which limits the application of multi-task learning. In addition, the TransPolymer pretrained still outperforms multi-task learning models on four out of the seven chosen datasets. Hence the improvement by TransPolymer compared with single-task baselines should still be highly valued. The scatter plots of ground truth vs. predicted values for Egc, Egb, Eea, Ei, Xc, EPS, and Nc datasets by TransPolymerpretrained are depicted in Fig. 2(c)-(i) and Supplementary Figure 2(c)-(i), respectively.

TransPolymer and baselines are trained on p-type polymer OPV dataset whose results

13

Table 5: Performance of TransPolymer and baseline models on p-type polymer OPV.



|Model|Train RMSE (%) (4)|Test RMSE (%) (4)|Train R2 (1)|Test R2 (1)|
|---|---|---|---|---|
|Random Forest (ECFP)|0.66|1.92|0.92|0.27|
|ANN (ECFP)|1.58|2.03|0.55|0.20|
|LSTM|2.35|2.34|-0.01|0.00|
|TransPolymer unpretrained|1.91|2.10|0.33|0.19|
|TransPolymer pretrained|1.19|1.92|0.74|0.32|


are shown in Table 5. The original paper trained random forest and artificial neural network (ANN) on the dataset using ECFP fingerprints. 55 TransPolymerpretrained, in comparison with baselines, gives a slightly better performance as the average RMSE is the same as that of ran- dom forest, and the average test R2 is increased by 0.05. Although all the model performance is not satisfying enough, possibly attributed to the noise in data, TransPolymer pretrained still outperforms baselines. The scatter plots of ground truth vs. predicted values for OPV by TransPolymerpretrained are depicted in Fig. 2(j) and Supplementary Figure 2(j).

Table 6 summarizes the improvement of TransPolymerpretrained over the previous best baseline models as well as TransPolymerunpretrained on each dataset. TransPolymer pretrained has outperformed all other models on all ten datasets, further providing evidence for the generalization of TransPolymer. TransPolymerpretrained exhibits an average decrease of eval- uation RMSE by 7.70% (in percentage) and an increase of evaluation R2 by 0.11 (in absolute value) compared with the best baseline models, and the two values become 18.5% and 0.12, respectively, when it comes to comparison with TransPolymer unpretrained. Therefore, the pre- trained TransPolymer could hopefully be a universal pretrained model for polymer property prediction tasks and applied to other tasks by finetuning. Besides, TransPolymer equipped with MLM pretraining technique shows significant advantages over other models in dealing with complicated polymer systems. Specifically, on PE-I benchmark, TransPolymer pretrained improves R2 by 0.37 comparing with the previous best baseline model and by 0.39 comparing with TransPolymer unpretrained. PE-I contains not only polymer SMILES but also key descrip- tors of the materials like temperature and component ratios within the materials. The data

14

Table 6: Improvement of performance of TransPolymerpretrained compared with baselines and TransPolymer unpretrained in terms of decrease of test RMSE (in percentage) and increase of test R2 (in absolute value).



|Dataset|vs. best
RMSE (4)|baselines
R2 (1)|vs. TransPolymer
RMSE (4)|unpretrained
R2 (1)|
|---|---|---|---|---|
|PE-I|-30.9%|+0.37|-52.2%|+0.39|
|PE-II|-3.17%|+0.01|-23.8%|+0.19|
|Egc|-8.33%|+0.02|-30.2%|+0.08|
|Egb|-5.45%|+0.02|-14.8%|+0.03|
|Eea|0.00%|+0.01|-11.1%|+0.02|
|Ei|-7.14%|+0.07|-15.2%|+0.06|
|Xc|-20.1%|+0.50|-17.6%|+0.23|
|EPS|-1.89%|+0.05|-11.9%|+0.06|
|Nc|0.00%|+0.03|0.00%|+0.02|
|OPV|0.00%|+0.05|-8.57%|+0.13|
|Average|-7.70%|+0.11|-18.5%|+0.12|


+0.12

in PE-I is noisy due to the existence of different types of components in the polymer mate- rials, for instance, copolymers, anions, and ionic liquids. Also, models are trained on data from the year 2018 and evaluated on data from the year 2019, which gives a more challenging setting. Therefore it is reasonable to infer that TransPolymer is better at learning features out of noisy data and giving a robust performance. It is noticeable that LSTM becomes the least competitive model in almost every downstream task, such evidence demonstrates the significance of attention mechanisms in understanding chemical knowledge from polymer sequences.

## 2.4 Abaltion Studies

The effects of pretraining could be further demonstrated by the chemical space taken up by polymer SMILES from the pretraining and downstream datasets visualized by t-SNE, 61 shown in Fig. 3. Each polymer SMILES is converted to TransPolymer embedding with the size of sequence length x embedding size. Max pooling is implemented to convert the embedding matrices to vectors so that the strong characteristics in embeddings could be

15

preserved in the input of t-SNE. We use openTSNE library62 to create 2D embeddings via pretraining data and map downstream data to the same 2D space. As illustrated in Fig. 3(a), almost every downstream data point lies in the space covered by the original ~1M pretraining data points, indicating the effectiveness of pretraining in better representation learning of TransPolymer. Data points from datasets like Xc which exhibit minor evidence of clustering in the chemical space cover a wide range of polymers, explaining the phenomenon that other models struggle on Xc while pretrained TransPolymer learns reasonable represen- tations. Meanwhile, for datasets that cluster in the chemical space, other models can obtain reasonable results whereas TransPolymer achieves better results. Additionally, it should be pointed out that the numbers of unique polymer SMILES in PE-I and PE-II are much smaller than the sizes of the datasets as many instances share the same polymer SMILES while differing in descriptors like molecular weight and temperature, hence the visualization of polymer SMILES cannot fully reflect the chemical space taken up by the polymers from these datasets.

Besides, we have also investigated how the size of the pretraining dataset affects the downstream performance. We randomly pick up 5K, 50K, 500K, and 1M (original size) data points from the initial pretraining dataset without augmentation, and pretrain TransPolymer with them and compare the results with those by TransPolymer trained with 5M augmented data. The results are summarized in Supplementary Table 5. Plus, Fig. 4 presents the bar plot of R2 for each experiment we have performed. Error bars are included in the figure if cross-validation is implemented in experiments. As shown in the table and the figure, the results demonstrate a clear trend of enhanced downstream performance (decreasing RMSE and increasing R2) with increasing pretraining size. In particular, the model performance on some datasets, for example, PE-I, Nc, and OPV, are even worse than training TransPolymer from scratch (the results by TransPolymer unpretrained in Table 2-5). A possible explanation is that the small amount of pretraining size results in the limited data space covered by pretraining data, thus making some downstream data points out of the distribution of pre-

16

a

## . Pretrain (1M)

.

PE-I

. PE-II

. Egc

Egb

:selected:

. Pretrain (50K)

b

. PE-I

. PE-II

. Egc

:selected:

ยท Eea

. Xc

Ei

:selected:

:selected:

EPS

:selected:

:selected:

:selected:

. Nc

. OPV

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected: :selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

. Egb

Eea

ยท Ei

. Xc

. EPS

. Nc

. OPV

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected: :selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected: :selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected: :selected:

:selected:

:selected:

:selected:

:selected:

c



|.|Pretrain (5K)|
|---|---|
|.|PE-I|
|.|PE-II|
||Egc|
|.|Egb|
|.|Eea|
|.|Ei|
||Xc|
|.|EPS|
|.|Nc|
|.|OPV|


:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

:selected:

Figure 3: t-SNE visualization of pretraining and downstream data. The embeddings are obtained by first fitting on the (a) 1M (original), (b) 50K, and (c) 5K pretraining data and then transforming downstream data to the corresponding data space.

training data. Fig. 3(b) and (c) visualize the data space by fitting on 50K and 5K pretraining data, respectively, in which a lot of space taken up downstream data points is not covered by pretraining data. Therefore, the results emphasize the effects of pretraining with a large number of unlabeled sequences.

17

5 K 50 K 500 K

1 M

0.8

5 M

0.6}

R2

0.4+

0.2+

0.0

PE-I PE-II Egc Egb Eea Ei Xc EPS Nc OPV

Downstream Dataset

Figure 4: Bar plot of R2 for each downstream task with different pretraining data sizes.

The results from TransPolymer pretrained so far are all derived by pretraining first and then finetuning the whole model on the downstream datasets. Besides, we also consider another setting where in downstream tasks only the regressor head is finetuned while the pretrained Transformer encoder is frozen. The comparison of the performance of TransPolymer pretrained between finetuning the regressor head only and finetuning the whole model is presented in Table 7. Standard deviation is included in the results if cross-validation is applied for down- stream tasks. Reasonable results could be obtained by freezing the pretrained encoders and training the regressor head only. For instance, the model performance on Xc dataset already surpasses the baseline models, and the model performance on Ei, Nc, and OPV datasets is

18

Table 7: Comparison of performance of TransPolymer pretrained between finetuning the regres- sor head only and finetuning the whole model in terms of test RMSE and R2.



|Dataset|Finetuning the regressor head RMSE R2||Finetuning the
RMSE|whole model
R2|
|---|---|---|---|---|
|PE-I|1.14|0.12|0.67|0.69|
|PE-II|0.91 + 0.12|0.40 ยฑ 0.10|0.61 + 0.07|0.73 + 0.04|
|Egc|0.69 + 0.03|0.81 + 0.02|0.44 โค 0.01|0.92 0.00|
|Egb|0.83 ยฑ 0.05|0.81 + 0.02|0.52 + 0.05|0.93 ยฑ 0.01|
|Eea|0.52 ยฑ 0.04|0.76 ยฑ 0.04|0.32 0.02|0.91 + 0.03|
|Ei|0.51 + 0.05|0.73 + 0.05|0.39 + 0.07|0.84 + 0.06|
|Xc|19.18 + 2.15|0.34 0.10|16.57 ยฑ 0.68|0.50 + 0.06|
|EPS|0.72 ยฑ 0.09|0.58 โค 0.07|0.52 0.07|0.76 + 0.11|
|Nc|0.13 0.02|0.70 ยฑ 0.06|0.10 โค 0.02|0.82 0.07|
|OPV|2.04 + 0.06|0.24 โค 0.03|1.92 0.06|0.32 ยฑ 0.05|


slightly worse than the corresponding best baselines. However, the performance on all the downstream tasks increases significantly if both the Transformer encoders and the regressor head are finetuned, which indicates that the regressor head only is not enough to learn task- specific information. In fact, the attention mechanism plays a key role in learning not only generalizable but also task-specific information. Even though the pretrained TransPolymer is transferable to various downstream tasks and more efficient, it is necessary to finetune the Transformer encoders with task-related data points for better performance.

Data augmentation is implemented not only in pretraining but also in finetuning. The comparison between the model performance on downstream tasks with pretraining on the original ~1M dataset and the augmented ~5M dataset (shown in Supplementary Table 5) has already demonstrated the significance of data augmentation in model performance en- hancement. In this part, we use the model pretrained on the ~5M augmented pretraining dataset but finetune TransPolymer without augmenting the downstream datasets to inves- tigate to what extent the TransPolymer model can improve the best baseline models for downstream tasks. The model performance enhancement with or without data augmenta- tion compared with best baseline models is summarized in Table 8. For most downstream tasks, TransPolymer pretrained can improve model performance without data augmentation,

19

Table 8: Improvement of performance of TransPolymerpretrained without and with data aug- mentation in finetuning compared with best baselines in terms of decrease of test RMSE (in percentage) and increase of test R2 (in absolute value).



|Dataset|No Augmentation RMSE (4) R2 (1)||Augmentation||
|---|---|---|---|---|
||||RMSE (4)|R2 (1)|
|PE-I|-15.5%|+0.22|-30.9%|+0.37|
|PE-II|+22.2%|-0.15|-3.17%|+0.01|
|Egc|-4.17%|+0.01|-8.33%|+0.02|
|Egb|+9.09%|-0.02|-5.45%|+0.02|
|Eea|+9.38%|0.00|0.00%|+0.01|
|Ei|0.00%|+0.03|-7.14%|+0.07|
|Xc|-14.5%|+0.43|-20.1%|+0.50|
|EPS|+7.55%|0.00|-1.89%|+0.05|
|Nc|0.00%|+0.02|0.00%|+0.03|
|OPV|+0.52%|+0.03|0.00%|+0.05|
|Average|+1.46%|+0.06|-7.70%|+0.11|


while such improvement would become more significant if data augmentation is applied. For PE-II dataset, however, TransPolymerpretrained is not comparable to the best baseline model without data augmentation since the original dataset contains only 271 data points in total. Because of the data-greedy characteristics of Transformer, data augmentation could be a crucial factor in finetuning, especially when data are scarce (which is very common in chem- ical and materials science regimes). Therefore, data augmentation can help generalize the model to sequences unseen in training data.

## 2.5 Self-attention Visualization

AAttention scores, serving as an indicator of how closely two tokens align with each other, could be used for understanding how much chemical knowledge TransPolymer learns from pretraining and how each token contributes to the prediction results. Take poly(ethylene oxide) (*CCO*), which is one of the most prevailing polymer electrolytes, as an example. The attention scores between each token in the first and last hidden layer are shown in Fig. 5(a) and (b), respectively. The attention score matrices of 12 attention heads generated

20

a

Attention Head 1

* * O C C * < 5>

:unselected: :unselected:

U

U.

0

</s>

<s> * C C O * < /s> Attention Head 5

* O C C * << >

:unselected:

Attention Head 2

Attention Head 3

-0.4

* C C * < >>

-0.8

* * * O C C * < 5>

0

0.3

:unselected:

- 0.6

Attention Head 4

-0.8

* U O C C * < 5>

0.8

- 0.6

0

U

U

0.2

- 0.4

O

0

*

- 0.2

- 0.1

</s> *

</s>

1

<s> * C C O * < /s> Attention Head 6

<5> * CCO * < /s> Attention Head 7

-0.8

* < S>

-0.8

- 0.6

U

* * </s> * O C C * < 5>

:unselected:

U

0.6

U CC

:unselected:

- 0.4

*

0.2

</S>

:unselected: - 0.6

- 0.4

- 0.2

<s> * C C O * < /s> Attention Head 8

-0.8

* 0 O C C * < 5>

-0.8

U

:unselected:

:unselected:

-0.6

- 0.6

*

</s>

</s>

<5> * C C O * < /s> Attention Head 9

-0.4

- 0.4

0

0.2

*

0.2

</S>

- 0.4

0

0.2

:unselected:

:unselected:

</s> * O C C * < 5>

:unselected:

s> * C C O * < /s> Attention Head 10

<s> * C C O * < /s> Attention Head 11

-0.7

* < 5>

0.6

-0.8

-0.5

U

0.6

* O * </s> * O C C * < 5>

U

- 0.4

U

- 0.3

- 0.4

O

-0.2

*

- 0.2

-0.1

1 </s>

<s> * C C O * < /s> Attention Head 12

-0.8

* U * * O C C * < 5>

U

:unselected:

- 0.6

U

-0.4

- 0.2

<>> * C C O * < /s>

b

Attention Head 1

รด * O C C * < 5>

U

O

* C C O * < /S> <5>

<5> *

C C O * < /S>

Attention Head 2

Attention Head 3

-0.30

* < 5>

* U U C C * < 5>

-0.25

-0.25

:unselected:

O

U

:unselected:

-0.20

-0.20

U

- 0.15

0.15

0

*

- 0.10

*

</S>

0.05

</5>

<>> *

C C O * < /S> Attention Head 5

* * * O C C * < 5>

:unselected:

U

O

</s>

<5> * C * O * < /s> Attention Head 9

* O C C * < 5>

:unselected:

0

*

</s>

<5> * C 0 * < /s> O

*

0.10

5/52

<>>

*- C C O * < /s> Attention Head 7

0.6

* CCO * < /s>

Attention Head 6

<>>

*

U

U

O

*

</s>

<< >

* C C O * < /s> Attention Head 10

* < 5>

U

U

0

*

1 </5>

<5> *

CCO

* < /s>

S>

- 0.5

- 0.25

*

- 0.4

0.20

U-

- 0.3

U

-0.15

O -

-0.2

0.10

*

-0.1

/s>

<5>

* C C O * < /S> Attention Head 11

0.30

0.8

<5>

-0.25

*

- 0.6

U-

-0.20

-0.4

C

U

-0.15

O

-0.10

-0.2

*

</5>

<5>

*

CCO * < /s>

- 0.4

- 0.2

- 0.8

-0.6

- 0.4

O

- 0.2

</s>

-0.0

<5> * C C O * < /s>

- 0.5

Attention Head 4

<>>

-0.25

-0,4

* CC *

U

-0.3

:unselected:

U

-0.20

0.15

0.2

O

- 0.1

*

0.10

</5>

<5>

* CC O * < /s>

Attention Head 8

-0.25

* < 5>

U

-0.20

C

:unselected:

:unselected:

U

-0.15

0

:unselected:

-0.10 *

0.250 - 0.225

-0.200

0.175

- 0.150

0.125

0.100

0.075

</5>

<5>

* C CO * < /S> Attention Head 12

0.30

0.225

-0.25 *

C C * < S>

- 0.200

U

-0.20

:unselected:

-0.175

0.150

-0.15

0

0.125

*

- 0.10

</5>

-0.100

0.075

<5>

C

O * < /s>

Figure 5: Visualization of attention scores in (a) the first and (b) the last hidden layer of pretrained TransPolymer.

from the first hidden layer indicate strong relationships between tokens in the neighborhood, which could be inferred from the emergence of high attention scores around the diagonals

21

Layer 1

Layer 2

Layer 3

Layer 4

Layer 5

Layer 6

Attention Head 1

0.5

Layer 1

- 0.4

Layer 2

-0.3

Layer 3

-0.2

Layer 4

Layer 5

-0.1

Layer 6

Attention Head 2

0.5

Layer 1

- 0.4

Layer 2

0.3

Layer 3

0.2

Layer 4

Layer 5

0.1

Layer 6

Attention Head 3

0.6

Layer 1 0.5

Layer 2

0.4

Layer 3

0.3

Layer 4

0.2

Layer 5

-0.1

Layer 6

Attention Head 4

- 0.8

- 0.7

- 0.6

0.5

- 0.4

-0.3

- 0.2

0.1

Layer 1 Layer 2

Layer 3

Layer 4

Layer 5

Layer 6

## Attention Head 5

0.6

Layer 1

-0.5

Layer 2

-0.4

Layer 3

-0.3

Layer 4

-0.2

Layer 5

-0.1

Layer 6

Attention Head 6

Layer 1

- 0.4

Layer 2

- 0.3

Layer 3

-0.2

Layer 4

Layer 5

0.1

Layer 6

Attention Head 7

0.7

Layer 1

0.6

Layer 2

- 0.5

- 0.4

Layer 3

-0.3

Layer 4

0.2

Layer 5

-0.1

Layer 6

Attention Head 8

0.8

-0.6

Attention Head 9

Attention Head 10

Attention Head 11

Attention Head 12

Layer 1 Layer 2

Layer 3

Layer 4

Layer 1

-0.7

ยท0.6

Layer 2

0.5

Layer 3

0.4

Layer 4

0.3

Layer 1

- 0.8

Layer 2

- 0.6

Layer 3

0.4

Layer 4

:unselected:

Layer 1

ยท 0.4

Layer 2

0.3

Layer 3

-0,2

Layer 4

0.8

- 0.6

- 0.4

Layer 5

Layer 6

-0.2

Layer 5

0.1

Layer 6

Layer 5

-0.2

Layer 6

Layer 5

0.2

-0.1

Layer 6

*COC(=O)OC *.* CCO*$F[B-](F)(F)F$0.17$95.2$37.0$-23$S_1

Figure 6: Visualization of attention scores between '(s)' token and other tokens at different hidden layers in each attention head. At the bottom is the sequence used for visualization in which the tokens having high attention scores with '(s)' are marked in red.

of matrices. This trend makes sense because the nearby tokens in polymer SMILES usually represent atoms bonded to each other in the polymer, and atoms are most significantly af- fected by their local environments. Therefore, the first hidden layer, which is the closest layer to inputs, could capture such chemical information. In contrast, the attention scores from the last hidden layer tend to be more uniform, thus lacking an interpretable pattern. Such phenomenon has also been observed by Abnar et al. who discovered that the embeddings of tokens would become contextualized for deeper hidden layers and might carry similar information. 63

When finetuning TransPolymer, the vector of the special token '(s)' from the last hidden state is used for prediction. Hence, to check the impacts of tokens on prediction results, the attention scores between '(s)' and other tokens from all 6 hidden layers in each attention head are illustrated with the example of the PEC-PEO blend electrolyte coming from PE-

22

- 0.4

- 0.2

II whose polymer SMILES is '*COC(=O)OC *.* CCO*'. In addition to polymer SMILES, the sequence also includes 'F[B-](F)(F)F', '0.17', '95.2', '37.0', '-23', and 'S_1' which stand for the anion in the electrolyte, the ratio between lithium ions and functional groups in the polymer, comonomer percentage, molecular weight (kDa), glass transition temperature (Tg), and linear chain structure, respectively. As is illustrated in Fig. 6, the '(s)' token tends to focus on certain tokens, like the '*', '$', and '-23', which are marked in red in the example sequence in Fig. 6. Since Tg usually plays an important role in determining the conductivity of polymers, 64 the finetuned Transpolyemr could understand the influential parts on prop- erties in a polymer sequence. However, it is also widely argued that the attention weights cannot fully depict the relationship between tokens and prediction results because a high attention score does not necessarily guarantee that the pair of tokens is important to the prediction results given that attention scores do not consider Value matrices. 65 More related work is needed to fully address the attention interpretation problem.

## 3 Discussion

In summary, we have proposed TransPolymer, a Transformer-based model with MLM pre- training, for accurate and efficient polymer property prediction. By rationally designing a polymer tokenization strategy, we can map a polymer instance to a sequence of tokens. Data augmentation is implemented to enlarge the available data for representation learn- ing. TransPolymer is first pretrained on approximately 5M unlabeled polymer sequences by MLM, then finetuned on different downstream datasets, outperforming all the baselines and unpretrained TransPolymer. The superior model performance could be further explained by the impact of pretraining with large unlabeled data, finetuning Transformer encoders, and data augmentation for data space enlargement. The attention scores from hidden layers in TransPolymer provide evidence of the efficacy of learning representations with chemical awareness and suggest the influential tokens on final prediction results.

23

Given the desirable model performance and outstanding generalization ability out of a small number of labeled downstream data, we anticipate that TransPolymer would serve as a potential solution to predicting newly designed polymer properties and guiding polymer design. For example, the pretrained TransPolymer could be applied in the active-learning- guided polymer discovery framework, 66,67 in which TransPolymer serves to virtually screen the polymer space, recommend the potential candidates with desirable properties based on model predictions, and get updated by learning on data from experimental evaluation. In addition, the outstanding performance of TransPolymer on copolymer datasets compared with existing baseline models has shed light on the exploration of copolymers. In a nutshell, even though the main focus of this paper is placed on regression, TransPolymer can pave the way for several promising (co)polymer discovery frameworks.

## 4.1 Polymer tokenization

Unlike small molecules which are easily represented by SMILES, polymers are more com- plex to be converted to sequences since SMILES fails to incorporate pivotal information like connectivity between repeating units and degree of polymerization. As a result, we need to design the polymer sequences to take account of that information. To design the polymer sequences, each repeating unit of the polymer is first recognized and converted to SMILES, then '*' signs are added at the places which represent the ends of the repeating unit to indi- cate the connectivity between repeating units. Such a strategy to indicate repeating units has been widely used in string-based polymer representations. 68,69 For the cases of copolymers, ".' is used to separate different constituents, and '^' is used to indicate branches in copolymers. Other information like the degree of polymerization and molecular weight, if accessible, will be put after the polymer SMILES separated by special tokens. Take the example of the sequence given in Fig. 1(a), the sequence describes a polymer electrolyte system including

24

two components separated by the special token '|'. Descriptors like the ratio between re- peating units in the copolymer, component type, and glass transition temperature (Tg for short) are added for each component separated by '$', and the ratio between components and temperature are put at the end of the sequence. Adding these descriptors can improve the performance of property predictions as suggested by Patel et al. 29 Unique 'NAN' tokens are assigned for missing values of each descriptor in the dataset. For example, 'NAN_Tg' indicates the missing value of glass transition temperature, and 'NAN_MW' indicates the missing molecular weight at that place. These unique NAN tokens are added during finetun- ing to include available chemical descriptors in the datasets. Therefore, different datasets can contain different NAN tokens. Notably, other descriptors like molecular weight and de- gree of polymerization are omitted in this example because their values for each component are missing. However, for practical usage, these values should also be included with unique 'NAN' characters. Besides, considering the varying constituents in copolymers as well as components in composites, the 'NAN' tokens for ratios are padded to the maximum possible numbers.

When tokenizing the polymer sequences, the regular expression in the tokenizer adapted from the RoBERTa tokenizer is transformed to search for all the possible elements in poly- mers as well as the vocabulary for descriptors and special tokens. Consequently, the polymer tokenizer can correctly slice polymers into constituting atoms. For example, 'Si' which rep- resents a silicon atom in polymer sequences would be recognized as a single token by our polymer tokenizer whereas 'S' and 'i' are likely to be separated into different tokens when using the RoBERTa tokenizer. Values for descriptors and special tokens are converted to single tokens as well, where all the non-text values, e.g., temperature, are discretized and treated as one token by the tokenizer.

25

## 4.2 Data augmentation

To enlarge the available polymer data for better representation learning, data augmentation is applied to the polymer SMILES within polymer sequences from each dataset we use. The augmentation technique is borrowed from Lambard et al. 70 First, canonicalization is removed from SMILES representations; then, atoms in SMILES are renumbered by rotation of their indices; finally, for each renumbering case, grammatically correct SMILES which preserve isomerism of original polymers or molecules and prevent Kekulisation are reconstructed. 31,71 Also, duplicate SMILES are removed from the expanded list. SMILES augmentation is implemented by RDKit library.72 In particular, data augmentation is only applied to training sets after the train-test split to avoid information leakage.

## 4.3 Transformer-based encoder

Our TransPolymer model is based on Transformer encoder architecture. 34 Unlike RNN-based models which encoded temporal information by recurrence, Transformer uses self-attention layers instead. The attention mechanism used in Transformer is named Scaled Dot-Product Attention, which maps input data into three vectors: queries (Q), keys (K), and values (V). The attention is computed by first computing the dot product of the query with all keys, dividing each by vdk for scaling where dk is the dimension of keys, applying softmax function to obtain the weights of values, and finally deriving the attention. The dot product between queries and keys computes how closely aligned the keys are with the queries. Therefore, the attention score is able to reflect how closely related the two embeddings of tokens are. The formula of Scaled Dot-Product Attention can be written as:

Attention(Q, K, V) = softmax

QKT Vdk

V

(1)

-

Multi-head attention is performed instead of single attention by linearly projecting Q, K, and V with different projections and applying the attention function in parallel. The outputs

26

are concatenated and projected again to obtain the final results. In this way, information from different subspaces could be learned by the model.

The input of Transformer model, namely embeddings, maps tokens in sequences to vec- tors. Due to the absence of recurrence, word embeddings only are not sufficient to encode sequence order. Therefore, positional encodings are introduced so that the model can know the relative or absolute position of the token in the sequence. In Transformer, position encodings are represented by trigonometric functions:

PEpos,2i = sin (pos/100002i/dmodel )

(2)

PEpos,2i+1 = cos (pos/100002i/dmodel)

(3)

where pos is the position of the token and i is the dimension. By this means, the relative positions of tokens could be learned by the model.

## 4.4 Pretraining with MLM

To pretrain TransPolymer with Masked Language Modeling (MLM), 15% of tokens of a sequence are chosen for possible replacement. Among the chosen tokens, 80% of which are masked, 10% of which are replaced by randomly selected vocabulary tokens, and 10% are left unchanged, in order to generate proper contextual embeddings for all tokens and bias the representation towards the actual observed words. 35 Such a pretraining strategy enables TransPolymer to learn the "chemical grammar" of polymer sequences by recovering the original tokens so that chemical knowledge is encoded by the model.

The pretraining database is split into training and validation sets by a ratio of 80/20. We use AdamW as the optimizer, where the learning rate is 5 x 10-5, betas parameters are (0.9, 0.999), epsilon is 1 ร 10-6, and weight decay is 0. A linear scheduler with a warm-up ratio of 0.05 is set up so that the learning rate increases from 0 to the learning rate set in the optimizer in the first 5% training steps then decreases linearly to zero. The batch size is

27

set to 200, and the hidden layer dropout and attention dropout are set to 0.1. The model is pretrained for 30 epochs during which the binary cross entropy loss decreases steadily from over 1 to around 0.07, and the one with the best performance on the validation set is used for finetuning. The whole pretraining process takes approximately 3 days on two RTX 6000 GPUs.

## 4.5 Finetuning for polymer property prediction

The finetuning process involves the pretrained Transformer encoder and a one-layer MLP regressor head so that representations of polymer sequences could be used for property predictions.

For the experimental settings of finetuning, AdamW is set to be the optimizer whose betas parameters are (0.9, 0.999), epsilon is 1 ร 10-6, and weight decay is 0.01. Different learning rates are used for the pretrained TransPolymer and regressor head. Particularly, for some experiments a strategy of layer-wise learning rate the decay (LLRD), suggested by Zhang et al., 73 is applied. Specifically, in LLRD, the learning rate is decreased layer-by- layer from top to bottom with a multiplicative decay rate. The strategy is based on the observation that different layers learn different information from sequences. Top layers near the output learn more local and specific information, thus requiring larger learning rates; while bottom layers near inputs learn more general and common information. The specific choices of learning rates for each dataset as well as other hyperparameters of the optimizer and scheduler are exhibited in Supplementary Table 2. For each downstream dataset, the model is trained for 20 epochs and the best model is determined in terms of the RMSE and R2 on the test set for evaluation.

28

## Data Availability

All data used in this work are publicly available. Original datasets could be found in corre- sponding literature. 51-55 Besides, the original and processed datasets used in this work are also available at https://github. com/ChangwenXu98/TransPolymer.git.

## Code Availability

The codes developed for this work are available at https: //github. com/ChangwenXu98/Tr ansPolymer.git.

## Acknowledgments

We thank the Advanced Research Projects Agency-Energy (ARPA-E), U.S. Department of Energy, under Award No. DE-AR0001221 and the start-up fund provided by the Department of Mechanical Engineering at Carnegie Mellon University.

## Author Contributions

A.B.F., Y.W., and C.X. conceived the idea; C.X. trained and evaluated the TransPolymer model; C.X. wrote the manuscript; A.B.F. supervised the work; all authors modified and approved the manuscript.

## Competing Interests

The authors declare no competing financial or non-financial interests.

29

Supporting Information Available

Downstream data augmentation strategies. Finetuning details. Summary of baseline. Se- quence length distributions of downstream data. Ground truth vs. predictions with aug- mented data. Model performance with varying pretraining size. LSTM Gradient Diminish- ing.

## References

(1) Wang, Y., et al. Toward designing highly conductive polymer electrolytes by machine learning assisted coarse-grained molecular dynamics. Chem. Mater. 2020, 32, 4144- 4151.

(2) Xie, T., et al. Accelerating amorphous polymer electrolyte screening by learning to reduce errors in molecular dynamics simulated properties. Nat. Commun. 2022, 13, 1-10.

(3) St. John, P. C., et al. Message-passing neural networks for high-throughput polymer screening. J. Chem. Phys. 2019, 150, 234111.

(4) Munshi, J .; Chen, W .; Chien, T .; Balasubramanian, G. Transfer learned designer poly- mers for organic solar cells. J. Chem. Inf. Model. 2021, 61, 134-142.

(5) Luo, H., et al. Core shell nanostructure design in polymer nanocomposite capacitors for energy storage applications. ACS Sustain. Chem. Eng. 2018, 7, 3145-3153.

(6) Hu, H., et al. Recent advances in rational design of polymer nanocomposite dielectrics for energy storage. Nano Energy 2020, 74, 104844.

(7) Bai, Y., et al. Accelerated discovery of organic polymer photocatalysts for hydrogen evolution from water through the integration of experiment and theory. J. Am. Chem. Soc. 2019, 141, 9063-9071.

30

(8) Liang, J .; Xu, S .; Hu, L .; Zhao, Y .; Zhu, X. Machine-learning-assisted low dielectric constant polymer discovery. Mater. Chem. Front. 2021, 5, 3823-3829.

(9) Mannodi-Kanakkithodi, A., et al. Scoping the polymer genome: A roadmap for rational polymer dielectrics design and beyond. Mater. Today 2018, 21, 785-796.

(10) Chen, L., et al. Polymer informatics: Current status and critical next steps. Mater. Sci. Eng. R Rep. 2021, 144, 100595.

(11) Rahman, A., et al. A machine learning framework for predicting the shear strength of carbon nanotube-polymer interfaces based on molecular dynamics simulation data. Compos Sci Technol 2021, 207, 108627.

(12) Scarselli, F .; Gori, M .; Tsoi, A. C .; Hagenbuchner, M .; Monfardini, G. The graph neural network model. IEEE trans. neural netw. 2008, 20, 61-80.

(13) Xie, T .; Grossman, J. C. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Phys. Rev. Lett. 2018, 120, 145301.

(14) Duvenaud, D. K., et al. Convolutional networks on graphs for learning molecular fin- gerprints. Advances in neural information processing systems 2015, 28.

(15) Yang, K., et al. Analyzing learned molecular representations for property prediction. J. Chem. Inf. Model. 2019, 59, 3370-3388.

(16) Karamad, M., et al. Orbital graph convolutional neural network for material property prediction. Phys. Rev. Mater. 2020, 4, 093801.

(17) Wang, Y .; Wang, J .; Cao, Z .; Barati Farimani, A. Molecular contrastive learning of representations via graph neural networks. Nat. Mach. Intell. 2022, 4, 279-287.

(18) Park, J., et al. Prediction and Interpretation of Polymer Properties Using the Graph Convolutional Network. ACS polym. Au. 2022,

31

(19) Cereto-Massaguรฉ, A., et al. Molecular fingerprint similarity search in virtual screening. Methods 2015, 71, 58-63.

(20) Rogers, D .; Hahn, M. Extended-connectivity fingerprints. J Chem Inf Model 2010, 50, 742-754.

(21) Aldeghi, M .; Coley, C. W. A graph representation of molecular ensembles for polymer property prediction. Chem. Sci. 2022, 13, 10486-10498.

(22) Cho, K., et al. Learning phrase representations using RNN encoder-decoder for statis- tical machine translation. ACL Anthology 2014, 1724-1734.

(23) Schwaller, P .; Gaudin, T .; Lanyi, D .; Bekas, C .; Laino, T. "Found in Translation": predicting outcomes of complex organic chemistry reactions using neural sequence-to- sequence models. Chem. Sci. 2018, 9, 6091-6098.

(24) Tsai, S .- T .; Kuo, E .- J .; Tiwary, P. Learning molecular dynamics with simple language model built upon long short-term memory neural network. Nat. Commun. 2020, 11, 1-11.

(25) Flam-Shepherd, D .; Zhu, K .; Aspuru-Guzik, A. Language models can learn complex molecular distributions. Nat. Commun. 2022, 13, 3293.

(26) Cadeddu, A .; Wylie, E. K .; Jurczak, J .; Wampler-Doty, M .; Grzybowski, B. A. Organic chemistry as a language and the implications of chemical linguistics for structural and retrosynthetic analyses. Angew. Chem. Int. Ed. 2014, 53, 8108-8112.

(27) Simine, L .; Allen, T. C .; Rossky, P. J. Predicting optical spectra for optoelectronic polymers using coarse-grained models and recurrent neural networks. Proc. Natl. Acad. Sci. U.S.A. 2020, 117, 13945-13948.

(28) Webb, M. A .; Jackson, N. E .; Gil, P. S .; de Pablo, J. J. Targeted sequence design within the coarse-grained polymer genome. Sci. Adv. 2020, 6, eabc6216.

32

(29) Patel, R. A .; Borca, C. H .; Webb, M. A. Featurization strategies for polymer sequence or composition design by machine learning. Mol. Syst. Des. Eng. 2022,

(30) Bhattacharya, D .; Kleeblatt, D. C .; Statt, A .; Reinhart, W. F. Predicting aggregate morphology of sequence-defined macromolecules with recurrent neural networks. Soft Matter 2022, 18, 5037-5051.

(31) Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput. 1988, 28, 31-36.

(32) Lin, T .- S., et al. BigSMILES: a structurally-based line notation for describing macro- molecules. ACS Cent. Sci. 2019, 5, 1523-1531.

(33) Goswami, S .; Ghosh, R .; Neog, A .; Das, B. Deep learning based approach for prediction of glass transition temperature in polymers. Mater. Today: Proc. 2021, 46, 5838-5843.

(34) Vaswani, A., et al. Attention is all you need. Advances in neural information processing systems 2017, 30.

(35) Devlin, J .; Chang, M .- W .; Lee, K .; Toutanova, K. BERT: Pre-training of Deep Bidirec- tional Transformers for Language Understanding. Proceedings of NAACL-HLT. 2019; pp 4171-4186.

(36) Liu, Y., et al. Roberta: A robustly optimized bert pretraining approach. Preprint at https://arxiv.org/abs/1907.11692 2019,

(37) Brown, T., et al. Language models are few-shot learners. Advances in neural information processing systems 2020, 33, 1877-1901.

(38) Peters, M. E .; Neumann, M .; Zettlemoyer, L .; Yih, W .- t. Dissecting Contextual Word Embeddings: Architecture and Representation. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018; pp 1499-1509.

33

(39) Conneau, A .; Lample, G. Cross-lingual language model pretraining. Advances in neural information processing systems 2019, 32.

(40) Honda, S .; Shi, S .; Ueda, H. R. Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. Preprint at https://arxiv.org/abs/1911.04738 2019,

(41) Ying, C., et al. Do transformers really perform badly for graph representation? Ad- vances in Neural Information Processing Systems 2021, 34, 28877-28888.

(42) Irwin, R .; Dimitriadis, S .; He, J .; Bjerrum, E. J. Chemformer: a pre-trained transformer for computational chemistry. Mach. learn .: sci. technol. 2022, 3, 015022.

(43) Magar, R .; Wang, Y .; Barati Farimani, A. Crystal twins: self-supervised learning for crystalline material property prediction. NPJ Comput. Mater. 2022, 8, 231.

(44) Wang, S .; Guo, Y .; Wang, Y .; Sun, H .; Huang, J. SMILES-BERT: large scale unsu- pervised pre-training for molecular property prediction. Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informat- ics. 2019; pp 429-436.

Chithrananda,

S .;

Grand,

G .;

Ramsundar,

B.

ChemBERTa:

large-scale

self-supervised

pretraining

for

molecular

property

prediction.

Preprint at

https://arxiv.org/abs/2010.09885 2020,

(46) Schwaller, P., et al. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS Cent. Sci. 2019, 5, 1572-1583.

(47) Cao, Z .; Magar, R .; Wang, Y .; Barati Farimani, A. MOFormer: Self-Supervised Trans- former Model for Metal-Organic Framework Property Prediction. J. Am. Chem. Soc. 2023, 145, 2958-2967.

34

(48) Perry, S. L .; Sing, C. E. 100th anniversary of macromolecular science viewpoint: Op- portunities in the physics of sequence-defined polymers. ACS Macro Lett. 2020, 9, 216-225.

(49) Le, T .; Epa, V. C .; Burden, F. R .; Winkler, D. A. Quantitative structure property relationship modeling of diverse materials properties. Chem. Rev. 2012, 112, 2889- 2919.

(50) Persson, N .; McBride, M .; Grover, M .; Reichmanis, E. Silicon valley meets the ivory tower: Searchable data repositories for experimental nanomaterials research. Curr Opin Solid State Mater Sci 2016, 20, 338-343.

(51) Ma, R .; Luo, T. PI1M: a benchmark database for polymer informatics. J Chem Inf Model 2020, 60, 4684-4690.

(52) Schauser, N. S .; Kliegle, G. A .; Cooke, P .; Segalman, R. A .; Seshadri, R. Database cre- ation, visualization, and statistical learning for polymer Li+-electrolyte design. Chem. Mater. 2021, 33, 4863-4876.

(53) Hatakeyama-Sato, K .; Tezuka, T .; Umeki, M .; Oyaizu, K. AI-assisted exploration of superionic glass-type Li+ conductors with aromatic structures. J. Am. Chem. Soc. 2020, 142, 3301-3305.

(54) Kuenneth, C., et al. Polymer informatics with multi-task learning. Patterns 2021, 2, 100238.

(55) Nagasawa, S .; Al-Naamani, E .; Saeki, A. Computer-aided screening of conjugated poly- mers for organic solar cell: classification by random forest. J. Phys. Chem. Lett. 2018, 9, 2639-2646.

(56) Salazar, J .; Liang, D .; Nguyen, T. Q .; Kirchhoff, K. Masked Language Model Scor-

35

ing. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020; pp 2699-2712.

(57) Bao, H., et al. Unilmv2: Pseudo-masked language models for unified language model pre-training. International conference on machine learning. 2020; pp 642-652.

(58) Yang, Z .; Yang, Y .; Cer, D .; Law, J .; Darve, E. Universal Sentence Representation Learning with Conditional Masked Language Model. Proceedings of the 2021 Confer- ence on Empirical Methods in Natural Language Processing. 2021; pp 6216-6228.

(59) Otsuka, S .; Kuwajima, I .; Hosoya, J .; Xu, Y .; Yamazaki, M. PoLyInfo: Polymer database for polymeric materials design. 2011 International Conference on Emerging Intelligent Data and Web Technologies. 2011; pp 22-29.

(60) Kim, C .; Chandrasekaran, A .; Huan, T. D .; Das, D .; Ramprasad, R. Polymer genome: a data-powered polymer informatics platform for property predictions. J. Phys. Chem. C 2018, 122, 17575-17585.

(61) Van der Maaten, L .; Hinton, G. Visualizing data using t-SNE. J Mach Learn Res 2008, 9.

(62) Poliฤar, P. G .; Straลพar, M .; Zupan, B. openTSNE: a modular Python li- brary for t-SNE dimensionality reduction and embedding. Preprint at https://www.biorxiv.org/content/10.1101/7318773.abstract 2019,

(63) Abnar, S .; Zuidema, W. Quantifying Attention Flow in Transformers. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020; pp 4190-4197.

(64) Schauser, N. S., et al. Glass transition temperature and ion binding determine conduc- tivity and lithium-ion transport in polymer electrolytes. ACS Macro Lett. 2020, 10, 104-109.

36

(65) Hao, Y .; Dong, L .; Wei, F .; Xu, K. Self-attention attribution: Interpreting informa- tion interactions inside transformer. Proceedings of the AAAI Conference on Artificial Intelligence. 2021; pp 12963-12971.

(66) Reis, M., et al. Machine-learning-guided discovery of 19F MRI agents enabled by au- tomated copolymer synthesis. J. Am. Chem. Soc. 2021, 143, 17677-17689.

(67) Tamasi, M. J., et al. Machine Learning on a Robotic Platform for the Design of Polymer-Protein Hybrids. Adv. Mater. 2022, 34, 2201809.

(68) Batra, R., et al. Polymers for extreme conditions designed using syntax-directed vari- ational autoencoders. Chem. Mater. 2020, 32, 10489-10500.

(69) Chen, G .; Tao, L .; Li, Y. Predicting polymers' glass transition temperature by a chem- ical language processing model. Polymers 2021, 13, 1898.

(70) Lambard, G .; Gracheva, E. Smiles-x: autonomous molecular compounds characteriza- tion for small datasets without descriptors. Mach. learn .: sci. technol. 2020, 1, 025004.

(71) Eyben, F .; Wรถllmer, M .; Schuller, B. Opensmile: the munich versatile and fast open- source audio feature extractor. Proceedings of the 18th ACM international conference on Multimedia. 2010; pp 1459-1462.

(72) Landrum, G., et al. RDKit: Open-source cheminformatics. https://www.rdkit.org 2006,

(73) Zhang, T .; Wu, F .; Katiyar, A .; Weinberger, K. Q .; Artzi, Y. Revisiting Few-sample BERT Fine-tuning. International Conference on Learning Representations. 2021.

37

Supplementary Information for TransPolymer: a Transformer-based Language Model for Polymer Property Predictions

Changwen Xu, ++ Yuyang Wang, +,1 and Amir Barati Farimani*, 1.$, 1.8 + Department of Materials Science and Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA $Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA TMachine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA ยง Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA E-mail: barati@cmu.edu

arXiv:2209.01307v4 [cs.LG] 26 Apr 2023

1

Supplementary Methods

## Downstream Data Augmentation Strategies

The data augmentation strategies for downstream datasets are summarized in Supplemen- tary Table 1. Copolymer data are augmented by generating equivalent SMILES of each repeating unit respectively. Different limitations on the amount of augmented data are ap- plied considering the capacity of the computational resources we have. For example, each repeating unit in the PE-I dataset is allowed to be augmented to two, and each SMILES in the OPV dataset are allowed to generate five in total. For datasets like Ei which consist of a small amount of data (usually hundreds of data) with limited sequence length (around or even smaller than 100), each polymer sequence is allowed to generate as many augmented ones as possible.

Supplementary Table 1: Summary of data augmentation strategies for downstream datasets.



|Dataset|Augmentation strategy|
|---|---|
|PE-I1|augmenting each repeating unit of (co)polymer to twice|
|PE-II2|augmenting each repeating unit of (co)polymer|
|Egc3|augmenting each polymer to twice|
|Egb3|augmenting each polymer without upper bound limit|
|Eea 3|augmenting each polymer without upper bound limit|
|Ei3|augmenting each polymer without upper bound limit|
|Xc3|augmenting each polymer without upper bound limit|
|EPS3|augmenting each polymer without upper bound limit|
|Nc3|augmenting each polymer without upper bound limit|
|OPV4|augmenting each polymer to five times|


## Finetuning Details

When finetuning TransPolymer, we search the hyperparameters so that the model can give the best performance on the validation set. We include the set of hyperparameters for optimizers and schedulers for each downstream task which gives the best performance in Supplementary Table 2. The LR annealing strategy we use is cosine-annealing. Besides,

2

Supplementary Table 3 summarizes the hyperparameters we use in experiments.

Supplementary Table 2: Summary of implementation details of optimizers and schedulers for downstream tasks.



|Dataset|Regressor LR|Last Hidden State LR|LR Decay Factor|Weight Decay|Warm-up Ratio|
|---|---|---|---|---|---|
|PE-I1|1 ร 10-4
:selected:|5ร10-5
:selected:|1.0|0.01|0.05|
|PE-II2|5ร10-5
:selected:|1 ร 10-4
:selected:|1.0|0.00001|0.1|
|Egc3|1ร 10-4
:selected:|1 ร 10-4
:selected:|0.9|0.01|0.1|
|Egb3|5ร10-5
:selected:|5ร10-5
:selected:|0.9|0.01|0.1|
|Eea 3|5ร10-5
:selected:|5ร10-5
:selected:|0.9|0.01|0.1|
|Ei3|5ร10-5|5ร10-5
:selected:|0.9|0.01|0.1|
|Xc3|5ร10-5
:selected:|5ร10-5
:selected:|0.9|0.01|0.1|
|EPS3|5ร10-5
:selected:|5ร10-5
:selected:|0.9|0.01|0.1|
|Nc3|5ร10-5
:selected:|5ร10-5
:selected:|0.9|0.01|0.1|
|OPV4|1 ร 10-4
:selected:|1 ร 10-4
:selected:|0.9|0.01|0.1|




|Hyperparameter|Range|
|---|---|
|Batch size|{32,64}|
|Last Hidden State LR|{5ร10-5,1ร10-4}|
|Regressor LR|{5ร10-5,1ร10-4}|
|LR Decay Factor|{0.9,1.0}|
|Weight Decay|{0.01,0.00001}|
|Warm-up Ratio|{0.05,0.1}|
|Hidden State Dropout|{0.1, 0.5}|
|Attention Dropout|{0.1, 0.5}|
|Regressor Dropout|{0.1, 0.5}|


## Summary of Baseline

The implementations of baseline models follow the original architectures used in the orig- inal paper. 1-4 The codes for all the baseline models except those for the OPV dataset are available on https: //github. com/KanHatakeyama/ion_predictor.git, 1 https://github . com/nschauser/PolymerElectrolyte.git,2 and https: //github. com/Ramprasad-Group /multi-task-learning.git,3 respectively. We cloned the repositories and reproduced the results reported in the paper using the same architecture described in the original paper. The

3

baseline models for the OPV dataset are realized by ourselves by first grid-searching the hy- perparameters for random forest and neural network then performing 5-fold cross-validation during training. In addition, we have trained bi-directional LSTM as an additional baseline for each downstream task to provide a direct comparison between Transformer with other language models. We search the hyperparameters to obtain the best performance, which is summarized in Supplementary Table 4. Besides, we have also trained random forest with ECFP6 fingerprints5 to provide a direct comparison with SOTA fingerprinting strategies. The implementation of this baseline is similar to that for the OPV dataset: we first grid- searched the hyperparameters and then performed 5-fold cross-validation during training.

Supplementary Table 4: Hyperparameters for LSTM.



|Hyperparameter|Range|
|---|---|
|# of Layers|{2,3,6}|
|Learning Rate|{5ร10-5,1ร10-4,5ร10-4}|
|Hidden Size|{256,768}|
|LSTM Dropout|{0, 0.1, 0.3}|
|Freq Mask Param|{0, 6,15}|
|Time Mask Ratio|{0, 0.25}|
|Add Convolution Layer|{True, False}|
|Input of Regressor|{Last hidden states, All hidden states}|


4

Supplementary Discussion

## Sequence Length Distributions of Downstream Data

The distributions of polymer sequences for the datasets we use for downstream tasks are illustrated in Supplementary Figure 1. The varying distributions provide evidence that TransPolymer is transferable to various polymer datasets and can learn meaningful repre- sentations from both long and short sequences.

## Ground Truth vs. Predictions with Augmented Data

Supplementary Figure 2 presents the scatter plots of ground truth vs. predicted values for the downstream tasks. In the plots, we not only include training and test data contained in the original datasets provided by the literature for reference1-4 but also the augmented data generated from training data. The true value of the augmented data is the same as that of the corresponding original training data point, while the predicted property values of augmented data are clustered around the predicted value of the original training data point.

## Model Performance with Varying Pretraining Size

Supplementary Table 5 summarizes the model performance on downstream tasks where the model is pretrained on data with different sizes. Standard deviation is included when cross- validation is applied in downstream tasks. As is shown in the table, model performance on downstream tasks increases with increasing pretraining data size.

5

Supplementary Table 5: Effects of the size of pretraining dataset on model performance on downstream datasets.



|Dataset|5 K|50 K|Test RMSE 500 K|1 M|5 M|5 K|50 K|Test R2 500 K|1 M|5 M|
|---|---|---|---|---|---|---|---|---|---|---|
|PE-I|1.18|1.15|1.08|1.1|0.67|0.05|0.10|0.21|0.18|0.69|
|PE-II|0.76 = 0.09|0.67 + 0.06|0.67 ยฑ 0.07|0.66 + 0.05|0.61 = 0.07|0.57 + 0.10|0.67 ยฑ 0.06|0.68 + 0.04|0.68 = 0.02|0.73 + 0.04|
|Egc|0.57 = 0.02|0.49 ยฑ 0.02|0.46 ยฑ 0.02|0.45 = 0.01|0.44 + 0.01|0.87 + 0.01|0.90 + 0.01|0.91 ยฑ 0.01|0.92 ยฑ 0.01|0.92 ยฑ 0.00|
|Egb|0.62 = 0.05|0.54 0.07|0.53 ยฑ 0.05|0.54 = 0.05|0.52 0.05|0.90 ยฑ 0.01|0.92 ยฑ 0.01|0.93 + 0.01|0.92 + 0.01|0.93 ยฑ 0.01|
|Eea|0.35 = 0.02|0.33 0.04|0.31 0.02|0.32 = 0.04|0.32 = 0.02|0.89 + 0.01|0.90 ยฑ 0.03|0.92 0.02|0.90 + 0.03|0.91 ยฑ 0.03|
|Ei|0.44 + 0.06|0.43 = 0.07|0.43 ยฑ 0.06|0.41 = 0.05|0.39 + 0.07|0.79 + 0.07|0.80 0.07|0.80 = 0.07|0.82 + 0.06|0.84 0.06|
|Xc|19.89 + 1.84|18.16 + 1.09|17.43 + 0.18|17.44 = 0.96|16.57 + 0.68|0.29 + 0.10|0.40 ยฑ 0.06|0.45 ยฑ 0.07|0.45 ยฑ 0.07|0.50 ยฑ 0.06|
|EPS|0.59 + 0.07|0.55 ยฑ 0.07|0.58 = 0.07|0.58 ยฑ 0.07|0.52 0.07|0.71 + 0.09|0.74 ยฑ 0.08|0.71 = 0.09|0.71 0.10|0.76 โบ 0.11|
|Nc|0.11 ยฑ 0.02|0.10 0.02|0.10 ยฑ 0.02|0.10 ยฑ 0.02|0.10 ยฑ 0.02|0.78 = 0.07|0.81 0.07|0.81 = 0.08|0.81 ยฑ 0.09|0.82 0.07|
|OPV|2.12 + 0.07|2.07 ยฑ 0.06|1.96 ยฑ 0.04|1.94 ยฑ 0.05|1.92 ยฑ 0.06|0.18 ยฑ 0.04|0.21 ยฑ 0.04|0.30ยฑ 0.04|0.31 ยฑ 0.04|0.32 ยฑ 0.05|


## LSTM Gradient Diminishing

From Table 2-5 in the manuscript, LSTM gives the worst performance on most of the down- stream tasks. Plus, we even observe that the model performance is not improved if we reduce the model size from 6 layers, 768 hidden size to 2 layers, 256 hidden size, and add a convolution layer before the LSTM layers.

In order to investigate the reason for the poor performance of LSTM on the property prediction tasks, we provide the visualization of gradient flow across LSTM layers in Supple- mentary Figure 3 to check whether the models suffer from the diminishing gradient problem. The models are trained on the Ei dataset for which five-fold cross-validation is applied. We plot the gradient of each layer for every back-propagation step and present them in a sin- gle figure for each LSTM architecture. Darker colors in the figures indicate that there are more bars at that place. The mean gradient suffers from the diminishing problem for all three models, while the maximum gradient does not diminish significantly for smaller models (two LSTM layers). Even though LSTM is better at remembering information from past hidden states in long sequences than regular RNNs, it is still confronted with the gradient diminishing problem in this situation.

Besides, we include the train and test loss versus epoch plots in Supplementary Figure 4 for different model architectures. We can tell from the loss vs. epoch plots that in many folds the training is early stopped before reaching 20 epochs due to the non-decreasing test

6

loss, which means it is very easy for LSTM to overfit even though dropout and time and frequency masking are used.

The experiment results strongly suggest that LSTM is not capable of representation learning from polymer sequences. In comparison, the good performance of Transformer on the same dataset highlights the advantage of the attention mechanism in understanding chemical knowledge from polymer sequences.

7

a

PE-I Dataset

b

PE-II Dataset

1200

60

1000

50

800

40

Number of Sequences

6001

Number of Sequences

30

400

20

200

10

0

100

200 300 Sequence Length

400

040

60 80

100

Sequence Length

d

Egb Dataset

e

Eea Dataset

80}

60

50

60

Number of Sequences

40}

40

Number of Sequences

30

20

201

c

Egc Dataset

300

250

200

Number of Sequences

150

100

50

20

40

60 Sequence Length 80

100 120

f

Ei Dataset

60

50

40

Number of Sequences

30

20

10

10

0

20

40

60

Sequence Length

0

20 30

40 Sequence Length

50

60

g

Xc Dataset

h

EPS Dataset

80

70

70

60

260

50

Number of Sequences

5 40

40

Number of Sequences

ลซ 30

30

20

20

10

10

0

20

40 60

80 100 120

Sequence Length

0

20 30

40

Sequence Length

20

30

40

Sequence Length

50

60

i

Nc Dataset

70

60

50

40

Number of Sequences

20

10

50

60

j

140

OPV Dataset

120

100

80

Number of Sequences

60

40

20

0

100

200

300

Sequence Length

0

10

20

30 Sequence Length 40

50

60

Supplementary Figure 1: Sequence length distributions of downstream datasets before data augmentation: (a) PE-I, (b) PE-II, (c) Egc, (d) Egb, (e) Eea, (f) Ei, (g) Xc, (h) EPS, (i) Nc, and (j) OPV. 8

a

-2

-4

PE-I Dataset

. augmented data

. train data

ยท test data

b

-2

-4

PE-II Dataset

augmented data

.

train data

test data

c

10

.

.

8

.

Egc Dataset

augmented data

train data

test data

-6

Predicted Conductivity (log S/com)

-8

Predicted Conductivity (log S/com)

-6

6

Predicted Bandgap (chain) (eV)

4

....

-10

-8

-121

2

0

-12 -10 -8 -6 -4 -2 Experimental Conductivity (log S/com)

-6

-4

-2

0

d

Egb Dataset

10 ยท augmented data

8

. train data

ยท test data

Experimental Conductivity (log S/com)

e

Eea Dataset

5 augmented data

. train data

test data

f

10

9 .

8

2 4

6

8

IO

Theoretical Bandgap (chain) (eV)

Ei Dataset

. augmented data

train data

.

test data

6

Predicted Bandgap (bulk) (eV)

4

3

Predicted Electron Affinity (eV)

2

7

Predicted Ionization Energy (eV)

6

5

2

4

0

2

4

6

8 10

2

5

4

6

+

8

9

Theoretical Bandgap (bulk) (eV)

Theoretical Electron Affinity (eV)

Theoretical Ionization Energy (eV)

Xc Dataset

ยท augmented data

100

. train data

ยท test data

80

Predicted Crystallization Tendency (%)

60

h

EPS Dataset

9

augmented data

train data

test data

7

i

3.0

Nc Dataset

ยท augmented data

.

train data

.

test data

1 2.5

Predicted Dielectric Constant

6

Predicted Refractive Index

40

2.0

20

20

40

60

80 100

Theoretical Crystallization Tendency (%)

j

3

5 6 +

8

Theoretical Dielectric Constant

OPV Dataset

1.54 1.5

2.0 Theoretical Refractive Index 2.5

3.0

augmented data

10}

train data

test data

8

6

Predicted PCE (%)

4

2

0

2

6

8 TO

Experimental PCE (%)

Supplementary Figure 2: Scatter plots of ground truth vs. predicted values by TransPolymer pretrained for downstream datasets: (a) PE-I, (b) PE-II, (c) Egc, (d) Egb, (e) Eea, (f) Ei, (g) Xc, (h) EPS, (i) Nc, and (j) OPV. Augmented data points are also included in the plots. The dashed lines on diagonals stand for perfect regression.

and convolution layer.

average gradient

conv.weight 0.00

Istm.weight_ih_10

Istm.weight_hh_10

Istm.weight_ih_10_reverse

Istm.weight_hh_10_reverse

0.05

0.10

0.15

0.20

average gradient

a

0.000 0.025 -

0.050

0.075 -

0.100

0.125

0.150

0.175 0.200

Istm.weight_ih_10 Istm.weight_hh_10 Istm.weight_ih_10_reverse -

Istm.weight_hh_10_reverse Istm.weight_ih_11 Istm.weight_hh_11

Istm.weight_ih_11_reverse

Istm.weight_hh_11_reverse

Istm.weight_ih_12

Istm.weight_hh_12

Istm.weight_ih_12_reverse Istm.weight_hh_12_reverse Istm.weight_ih_13

c

Gradient flow

mean-gradient zero-gradient Istm.weight_hh_13

0.25

Istm.weight_ih_13_reverse

max-gradient

Istm.weight_hh_13_reverse Istm.weight_ih_14 Istm.weight_hh_14 Istm.weight_ih_14_reverse Istm.weight_hh_14_reverse

Istm.weight_ih_15 Istm.weight_hh_15

Istm.weight_ih_15_reverse Istm.weight_hh_15_reverse -

regressor.1.weight - regressor.3.weight

Layers

Supplementary Figure 3: Visualization of gradient flow across LSTM layers for different model architectures: (a) 6 LSTM layers, 768 hidden size, and no convolution layer; (b) 2 LSTM layers, 256 hidden size, and no convolution layer;(c) 2 LSTM layers, 256 hidden size,

10

Istm.weight_ih_11

Layers

Gradient flow

Istm.weight_hh_11

Istm.weight_ih_11_reverse

Istm.weight_hh_11_reverse

regressor.1.weight zero-gradient mean-gradient regressor.3.weight

average gradient

b

0.00 + 0.02

0.06 0.04

0.10 0.08

0.12

0.14

0.16

Istm.weight_ih_10

Istm.weight_hh_10

Istm.weight_ih_10_reverse

zero-gradient

mean-gradient max-gradient

max-gradient

Istm.weight_hh_10_reverse

Istm.weight_ih_11

Layers

Istm.weight_hh_11

Gradient flow

Istm.weight_ih_11_reverse

Istm.weight_hh_11_reverse

regressor.1.weight

regressor.3.weight

a

Fold 1

1.2

1.0

0.8

train

Fold 2

1.2

1.0

0.8

train

Fold 3

1.0

0.8

train test

Fold 4

1.4

1.2

1.0

train test

Fold 5

0.8

0.7

0.6

0.5

train test

test

0.6

test

0.6

0.6

0.6

0,4

0.4

0.4

0.2

0.2

0.4 -

0,2

0.2

0.2

0 1

7

4

0 1

7

0

7 A

6

10

n

1 7

m

4

5

6

0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5

b

Fold 1

1.2

Fold 2

1.2

train

test

1.0

Fold 3

1.6

1.4

Fold 4

1.2

1.0

Fold 5

0.9

0.8

1.0

1.2

0.7

0.8

train test

0.6

0.4.

0.8

1.0

0.8

0.6

0.6

0.4

0.4

train

test

0.8

train test

0.6

0.4

0.6

train

test

0.5

0.4

0.3

0 1

7 3

4

5

1

2

3

4

5

0.2

0

1

7

3

4

In

0

1 7 3

4

n

1

7

c

Fold 1

1.4

Fold 2

1.0

0.9

train test

Fold 3

1.0

Fold 4

train

1.4

test

Fold 5

1.1

1.0

train test

1.2

0.8

0.9

1.2

0.9

1.0

train

test

0.8

0.8

0.7-

0.7

0.6

0.6

0.5

train

test

0.8

1.0

0.7

0.8

0.6

0.6

0.4

0.5

0.6

0.5

0.4

0

A

3

4

0.3 -

6

0

5

10

0.4

15

0

0.4

0

1

3

4

5

6

0.4

0.3

7

4

6

8 10

Supplementary Figure 4: Train and test loss versus epoch for different model architectures: (a) 6 LSTM layers, 768 hidden size, and no convolution layer; (b) 2 LSTM layers, 256 hidden size, and no convolution layer; (c) 2 LSTM layers, 256 hidden size, and convolution layer. Five-fold cross-validation is applied for each experiment and the model is trained for 20 epochs for each fold. The training process is early stopped if the test loss is not decreasing for successive 5 epochs.

11

## LSTM Gradient Diminishing

From Table 2-5 in the manuscript, LSTM gives the worst performance on most of the down- stream tasks. Plus, we even observe that the model performance is not improved if we reduce the model size from 6 layers, 768 hidden size to 2 layers, 256 hidden size, and add a convolution layer before the LSTM layers.

In order to investigate the reason for the poor performance of LSTM on the property prediction tasks, we provide the visualization of gradient flow across LSTM layers in Supple- mentary Figure 3 to check whether the models suffer from the diminishing gradient problem. The models are trained on the Ei dataset for which five-fold cross-validation is applied. We plot the gradient of each layer for every back-propagation step and present them in a sin- gle figure for each LSTM architecture. Darker colors in the figures indicate that there are more bars at that place. The mean gradient suffers from the diminishing problem for all three models, while the maximum gradient does not diminish significantly for smaller models (two LSTM layers). Even though LSTM is better at remembering information from past hidden states in long sequences than regular RNNs, it is still confronted with the gradient diminishing problem in this situation.

Besides, we include the train and test loss versus epoch plots in Supplementary Figure 4 for different model architectures. We can tell from the loss vs. epoch plots that in many folds the training is early stopped before reaching 20 epochs due to the non-decreasing test

6

loss, which means it is very easy for LSTM to overfit even though dropout and time and frequency masking are used.

The experiment results strongly suggest that LSTM is not capable of representation learning from polymer sequences. In comparison, the good performance of Transformer on the same dataset highlights the advantage of the attention mechanism in understanding chemical knowledge from polymer sequences.

7

a

PE-I Dataset

b

PE-II Dataset

1200

60

1000

50

800

40

Number of Sequences

6001

Number of Sequences

30

400

20

200

10

0

100

200 300 Sequence Length

400

040

60 80

100

Sequence Length

d

Egb Dataset

e

Eea Dataset

80}

60

50

60

Number of Sequences

40}

40

Number of Sequences

30

20

201

c

Egc Dataset

300

250

200

Number of Sequences

150

100

50

20

40

60 Sequence Length 80

100 120

f

Ei Dataset

60

50

40

Number of Sequences

30

20

10

10

0

20

40

60

Sequence Length

0

20 30

40 Sequence Length

50

60

g

Xc Dataset

h

EPS Dataset

80

70

70

60

260

50

Number of Sequences

5 40

40

Number of Sequences

ลซ 30

30

20

20

10

10

0

20

40 60

80 100 120

Sequence Length

0

20 30

40

Sequence Length

20

30

40

Sequence Length

50

60

i

Nc Dataset

70

60

50

40

Number of Sequences

20

10

50

60

j

140

OPV Dataset

120

100

80

Number of Sequences

60

40

20

0

100

200

300

Sequence Length

0

10

20

30 Sequence Length 40

50

60

Supplementary Figure 1: Sequence length distributions of downstream datasets before data augmentation: (a) PE-I, (b) PE-II, (c) Egc, (d) Egb, (e) Eea, (f) Ei, (g) Xc, (h) EPS, (i) Nc, and (j) OPV. 8

a

-2

-4

PE-I Dataset

. augmented data

. train data

ยท test data

b

-2

-4

PE-II Dataset

augmented data

.

train data

test data

c

10

.

.

8

.

Egc Dataset

augmented data

train data

test data

-6

Predicted Conductivity (log S/com)

-8

Predicted Conductivity (log S/com)

-6

6

Predicted Bandgap (chain) (eV)

4

....

-10

-8

-121

2

0

-12 -10 -8 -6 -4 -2 Experimental Conductivity (log S/com)

-6

-4

-2

0

d

Egb Dataset

10 ยท augmented data

8

. train data

ยท test data

Experimental Conductivity (log S/com)

e

Eea Dataset

5 augmented data

. train data

test data

f

10

9 .

8

2 4

6

8

IO

Theoretical Bandgap (chain) (eV)

Ei Dataset

. augmented data

train data

.

test data

6

Predicted Bandgap (bulk) (eV)

4

3

Predicted Electron Affinity (eV)

2

7

Predicted Ionization Energy (eV)

6

5

2

4

0

2

4

6

8 10

2

5

4

6

+

8

9

Theoretical Bandgap (bulk) (eV)

Theoretical Electron Affinity (eV)

Theoretical Ionization Energy (eV)

Xc Dataset

ยท augmented data

100

. train data

ยท test data

80

Predicted Crystallization Tendency (%)

60

h

EPS Dataset

9

augmented data

train data

test data

7

i

3.0

Nc Dataset

ยท augmented data

.

train data

.

test data

1 2.5

Predicted Dielectric Constant

6

Predicted Refractive Index

40

2.0

20

20

40

60

80 100

Theoretical Crystallization Tendency (%)

j

3

5 6 +

8

Theoretical Dielectric Constant

OPV Dataset

1.54 1.5

2.0 Theoretical Refractive Index 2.5

3.0

augmented data

10}

train data

test data

8

6

Predicted PCE (%)

4

2

0

2

6

8 TO

Experimental PCE (%)

Supplementary Figure 2: Scatter plots of ground truth vs. predicted values by TransPolymer pretrained for downstream datasets: (a) PE-I, (b) PE-II, (c) Egc, (d) Egb, (e) Eea, (f) Ei, (g) Xc, (h) EPS, (i) Nc, and (j) OPV. Augmented data points are also included in the plots. The dashed lines on diagonals stand for perfect regression.

and convolution layer.

average gradient

conv.weight 0.00

Istm.weight_ih_10

Istm.weight_hh_10

Istm.weight_ih_10_reverse

Istm.weight_hh_10_reverse

0.05

0.10

0.15

0.20

average gradient

a

0.000 0.025 -

0.050

0.075 -

0.100

0.125

0.150

0.175 0.200

Istm.weight_ih_10 Istm.weight_hh_10 Istm.weight_ih_10_reverse -

Istm.weight_hh_10_reverse Istm.weight_ih_11 Istm.weight_hh_11

Istm.weight_ih_11_reverse

Istm.weight_hh_11_reverse

Istm.weight_ih_12

Istm.weight_hh_12

Istm.weight_ih_12_reverse Istm.weight_hh_12_reverse Istm.weight_ih_13

c

Gradient flow

mean-gradient zero-gradient Istm.weight_hh_13

0.25

Istm.weight_ih_13_reverse

max-gradient

Istm.weight_hh_13_reverse Istm.weight_ih_14 Istm.weight_hh_14 Istm.weight_ih_14_reverse Istm.weight_hh_14_reverse

Istm.weight_ih_15 Istm.weight_hh_15

Istm.weight_ih_15_reverse Istm.weight_hh_15_reverse -

regressor.1.weight - regressor.3.weight

Layers

Supplementary Figure 3: Visualization of gradient flow across LSTM layers for different model architectures: (a) 6 LSTM layers, 768 hidden size, and no convolution layer; (b) 2 LSTM layers, 256 hidden size, and no convolution layer;(c) 2 LSTM layers, 256 hidden size,

10

Istm.weight_ih_11

Layers

Gradient flow

Istm.weight_hh_11

Istm.weight_ih_11_reverse

Istm.weight_hh_11_reverse

regressor.1.weight zero-gradient mean-gradient regressor.3.weight

average gradient

b

0.00 + 0.02

0.06 0.04

0.10 0.08

0.12

0.14

0.16

Istm.weight_ih_10

Istm.weight_hh_10

Istm.weight_ih_10_reverse

zero-gradient

mean-gradient max-gradient

max-gradient

Istm.weight_hh_10_reverse

Istm.weight_ih_11

Layers

Istm.weight_hh_11

Gradient flow

Istm.weight_ih_11_reverse

Istm.weight_hh_11_reverse

regressor.1.weight

regressor.3.weight

a

Fold 1

1.2

1.0

0.8

train

Fold 2

1.2

1.0

0.8

train

Fold 3

1.0

0.8

train test

Fold 4

1.4

1.2

1.0

train test

Fold 5

0.8

0.7

0.6

0.5

train test

test

0.6

test

0.6

0.6

0.6

0,4

0.4

0.4

0.2

0.2

0.4 -

0,2

0.2

0.2

0 1

7

4

0 1

7

0

7 A

6

10

n

1 7

m

4

5

6

0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5

b

Fold 1

1.2

Fold 2

1.2

train

test

1.0

Fold 3

1.6

1.4

Fold 4

1.2

1.0

Fold 5

0.9

0.8

1.0

1.2

0.7

0.8

train test

0.6

0.4.

0.8

1.0

0.8

0.6

0.6

0.4

0.4

train

test

0.8

train test

0.6

0.4

0.6

train

test

0.5

0.4

0.3

0 1

7 3

4

5

1

2

3

4

5

0.2

0

1

7

3

4

In

0

1 7 3

4

n

1

7

c

Fold 1

1.4

Fold 2

1.0

0.9

train test

Fold 3

1.0

Fold 4

train

1.4

test

Fold 5

1.1

1.0

train test

1.2

0.8

0.9

1.2

0.9

1.0

train

test

0.8

0.8

0.7-

0.7

0.6

0.6

0.5

train

test

0.8

1.0

0.7

0.8

0.6

0.6

0.4

0.5

0.6

0.5

0.4

0

A

3

4

0.3 -

6

0

5

10

0.4

15

0

0.4

0

1

3

4

5

6

0.4

0.3

7

4

6

8 10

Supplementary Figure 4: Train and test loss versus epoch for different model architectures: (a) 6 LSTM layers, 768 hidden size, and no convolution layer; (b) 2 LSTM layers, 256 hidden size, and no convolution layer; (c) 2 LSTM layers, 256 hidden size, and convolution layer. Five-fold cross-validation is applied for each experiment and the model is trained for 20 epochs for each fold. The training process is early stopped if the test loss is not decreasing for successive 5 epochs.

11

(1) Hatakeyama-Sato, K .; Tezuka, T .; Umeki, M .; Oyaizu, K. AI-assisted exploration of superionic glass-type Li+ conductors with aromatic structures. J. Am. Chem. Soc. 2020, 142, 3301-3305.

(2) Schauser, N. S .; Kliegle, G. A .; Cooke, P .; Segalman, R. A .; Seshadri, R. Database creation, visualization, and statistical learning for polymer Li+-electrolyte design. Chem. Mater. 2021, 33, 4863-4876.

(3) Kuenneth, C .; Rajan, A. C .; Tran, H .; Chen, L .; Kim, C .; Ramprasad, R. Polymer informatics with multi-task learning. Patterns 2021, 2, 100238.

(4) Nagasawa, S .; Al-Naamani, E .; Saeki, A. Computer-aided screening of conjugated poly- mers for organic solar cell: classification by random forest. J. Phys. Chem. Lett. 2018, 9, 2639-2646.

(5) Cereto-Massaguรฉ, A .; Ojeda, M. J .; Valls, C .; Mulero, M .; Garcia-Vallvรฉ, S .; Pujadas, G. Molecular fingerprint similarity search in virtual screening. Methods 2015, 71, 58-63.

12

