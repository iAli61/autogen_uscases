Title: Deep Neural Networks for Ab Initio Quantum Chemistry

david david is a senior research scientist at deepmind his research stands ai ml and computational science he received his phd from colombia in 2015 working at the center for theoretical neuroscience he's been doing a lot of nice work on applying ml to computational physics and we're excited to hear from him today on deep neural networks for ab initio quantum chemistry thank you very much all right so yeah it's uh it's a pleasure to be able to speak here it seems like a you know like a great audience a great mix of people across a lot of backgrounds and so this will be a talk that combines together things from a lot of different areas i'll just uh i'll just dive right in uh so the basic idea uh about this talk is how can we use methods from deep learning to solve the elec uh solve the many electron schrodinger equation and so why do we care about that uh basically if you can solve that you've kind of solved all of chemistry you can predict from first principles which chemical reactions will happen uh but also quite a lot of condensed matter physics as well uh it comes down to basically just studying the collective properties of electrons if you want to uh to understand what things are metals or insulators or superconductors or you know even more exotic topological states of matter and things like this it all just comes down to the collective quantum behavior of electrons so many electron systems and even 100 years ago paul durock nicely summed up the summed up the situation uh when he said basically at the level of atoms and molecules and materials we understand all of the laws of uh physics and chemistry uh now all we just need to do is solve them and 100 years later we're still trying to solve them and that's still a very difficult problem so uh since then you know quant uh computational uh quantum mechanics and quantum chemistry has grown into a whole field of its own there have been nobel prizes awarded in this field uh but pretty much all of these methods there's there's sort of a whole menagerie of them and they all have different advantages and disadvantages so at one extreme you have methods like like archery fog which are basically mean field methods which are very fast and scale cubicle in the number of electrons but but are not very accurate uh at the other extreme you have you know progressively more uh more complicated and more accurate polynomially scaling methods and at the very very far extreme you have what are called full configuration inter interaction methods which actually scale factorially but up to a certain level of discrete uh discretization are technically exact uh let me see if i can pull up a pointer here okay cool and basically for all of these methods they're just different ways of solving this seemingly simple equation right so h here is your hamiltonian so linear operator psi is the wave function it's a possibly complex valued function that takes electron positions and in principal electron spins as inputs so each r here is just a three-dimensional vector one for each electron and to solve the time-independent version you want to find eigenfunctions of this operator and so you have states of constant so the eigenvalue is the energy so you have states of constant energy right and so all of these methods are basically trying to do variations on this and you also have a density functional methods which sort of exist in a class of their own where the idea there is that rather than actually trying to solve for the full wave function which is a function of all electrons individually you're actually trying to solve just for the density of electrons and there's beautiful theory showing that in principle this can be arbitrarily accurate but it depends on knowledge of what's called the exchange correlation functional and though you can show that that exists there's no real way to uh to compute it some practice to use approximations that are not uh not as accurate as some of these methods okay so what we want to do is we want to make it more accurate with deep learning and so again to this audience you're probably already familiar with most of these but in the last decade you know neural networks have sort of gone from strength to strength they've shown amazing amazing results in object recognition uh for game playing like beating the game of go or atari games for natural language like these gpt3 type language models where you know the more data you throw at them the more sort of uncannily realistic they start to become so we know that neural networks are fantastic at function approximation at least if you have enough data so why not use them for approximating a wave function uh and so this is the idea behind uh a group of methods that are combining one of these methods called quantum monte carlo with deep learning some people have taken to calling this deep qmc and so uh the fermionic neural network that i'll be telling you about today is one method within these within these classic methods uh that works quite well uh and as i said in the title this is specifically about ab initio methods so for those of you who are more familiar with the machine learning what does this really mean what kind of learning am i talking about here so in the in the space of machine learning there are a lot of analogies between between machine learning methods and sort of existing methods in the world of computational uh computational chemistry and computational physics so what gets called empirical methods in computational chemistry pretty much maps onto supervised learning so most of the time when somebody says they're doing machine learning for quantum physics they're usually doing something like this where they use either experimental data or really accurate ab initio data and maybe train a neural network uh to fit that data so that you can say run run things faster or generalize out of domain the quantum mechanical analog of unsupervised learning would probably be something like quantum tomography where you don't have any sort of labeled data you just have a bunch of observations that comes from some quantum system and you want to reconstruct what the wave function is that generated that that's a bit like generative models in machine learning but what we're talking about quantum monte carlo is a type of abenitio method and the best analog i can maybe think of in um in machine learning is maybe something a bit like self-play and reinforcement learning where the data is generated by the model itself so you don't even have training data really so there isn't really a term in this for machine learning i've taken to calling this self-generative learning so if if anybody likes that term and wants to use it please go ahead um but at least in the in the computational chemistry world it's typically called ab initio so this i think distinguishes it from a lot of other work uh using at the intersection of chemistry and machine learning that's more in the supervised vein okay uh so then as i said how do we actually learn a wave function what what would self-generative or ab initio learning look like uh so again what we're what we're trying to do is find eigen functions of this operator the hamiltonian and you can break this up into a kinetic energy term that basically is the laplacian of the wave function and then a potential energy term where you have terms for interaction it's just you know a one over r potential where you have you know basic electrostatics you just have uh electrons and ions interacting and the convention is to use lowercase r for electron positions and capital r for nuclear positions so this just gives you the potential energy uh so psi squared you have some constraints on it or sorry psi the wave function you're trying to learn you have some constraints on it psi squared should be a probability density function um because h is a um a symmetric operator or a hermitian operator then for psi to be an eigenfunction has to be real valued so we don't have to worry about complex values in this talk and also for electrons i'll get to this later but the thing that makes this complicated is that this has to be anti-symmetric with respect to exchange of electrons and you can solve this exactly for hydrogen but nothing else basically in continuous space pretty much nothing else no real systems at least uh so quantum monte carlo is a computational method for solving this uh it's considered a fairly uh in some ways a sort of cheap and not very accurate method at least variational quantum monte carlo uh the objective function you're trying to solve for is given by this is basically the rayleigh quotient so the minimum of this is going to be the smallest eigen function of h hat and because psi in general we might not necessarily use a normalized wave function you have to add this normalization term here if we expand this out as an integral it looks like this and you can see this term here almost looks like a probability distribution if this were a psi squared rather than a psi this would be a normalized probability distribution so what you can do is you can basically write this as an expectation if you pull out psi squared uh and so you can write out another way that might be more familiar to a machine learning audience to write this objective function is like this where p is a probability distribution proportional to psi squared we have to put in this psi inverse because you know the squared is missing here and this is a term that's called the local energy and that's basically called that because if psi is actually an eigenfunction uh then the psi and the psi inverse will cancel out and this will just be a constant everywhere that's equal to the energy uh so it's quan uh and so basically what makes it quantum monte carlo is that you evaluate this expectation using uh using monte carlo methods to sample from from psi squared okay and this was originally originally this was not actually minimized you would just plug in a fixed functional form for psi and just evaluate these integrals this is used in the study of liquid helium originally uh was extended to fermions in the 70s um and there are basically two main types of quantum monte carlo one called variational quantum monte carlo where you directly minimize this this was called variational quantum monte carlo because this is an upper bound on the ground state energy so even if psi doesn't give you an exact solution you know it's at least an upper bound so that gives you a way of comparing different methods and uh evaluating which one's better and a diffusion quantum monte carlo which actually is very very closely related to um sequential monte carlo or particle filtering in machine learning they're they're it's basically the same idea with a few extra tricks where you have a set of particles undergoing diffusion and you update some weights on these particles uh and you also do a sort of birth death thing to make sure that the weights don't get too skewed on the particles um so diffusion quantum monte carlo is generally much more much more accurate but it's also difficult more difficult to do things like evaluate forces while a variational quantum monte carlo historically has been thought of as a sort of cheap but low accuracy method but what we found is that by using deep learning we can make this as accurate as many of these much more much more computationally demanding and much more complicated methods uh okay and so when i say accurate or you know or not so accurate uh in computational chemistry the the standard level of accuracy that people go for is usually what's called chemical accuracy or about one kilocalorie per mole and just for some context as i said these heartry fog methods are a type of mean field method those will usually capture about 99 99.5 percent of the energy in a chemical system the leftover half a percent of energy is what's called the correlation energy and chemical accuracy depending on the system will typically be in the range of 99 of the correlation energy so for a system that has a um in what are called atomic units the unit of energy is heart trees and so for a a small say hydrocarbon system the total energy might be 100 200 trees but the energy in the bonds will be on the order of thousandths of archery of the order of millihar trees so this is really an exceedingly high standard of accuracy especially compared to the kind of things you know if you get 87 top one accuracy on imagenet you know you have the best object recognition system in the world uh but here you need you know 99.9995 accuracy okay so everyone following so far i feel like on these talks it's sort of hard to hard to gauge if people are following along but if anybody has any questions just jump in can everybody hear me yeah i can hear you okay great um so that's the objective function that we're trying to minimize uh but i mentioned that you have to you know directly optimize psi how do we actually represent psi uh so as i said what distinguishes a fermionic wave functions or uh wave functions for electrons uh from other systems is that it has to be anti-symmetric you exchange the position of two electrons and you get a minus sign so what this means is that if you have two electrons with the same state the wave function will be zero the probability of finding that configuration is zero so two electrons can never have the same configuration this is the pauli exclu pauli exclusion principle okay so one way uh one very natural way of representing an anti-symmetric function is to use determinants because determinants have the nice property that they're anti-symmetric under exchange of rows and exchange of columns and in fact i think they're the only function or the most general function that is multi-linear and anti-symmetric or there's some there's some way of formulating things where you can show the determinant is the most general anti-symmetric function that has a certain property so there are natural reasons that you want to work with these and it's also just kind of a mathematical miracle in some sense that determinants can be computed in cubic time whereas you know the the equivalent for symmetric functions the permanent can only be computed in factorial time so in some ways it's actually much harder to deal with bosonic systems than fermionic systems uh and so the starting point for most of these uh these wave functions are what's called the slater determinant where you have a set of functions that are a function of single electrons so each of these functions is just is not a function of all electrons it's just a function that maps you know a point in uh in three-dimensional space to a scalar and you construct a matrix where you know each row is one function and each column is one electron and the determinant is known as a slater determinant so the minimum energy solution for uh if you represent your wave function as a slater determinant is known as the hartree fox solution so it's equivalent to mean field methods in statistical physics in many ways uh but then there are certain things that are very hard to represent in this representation for instance electron electron correlations are hard to encode because these are a function of single uh single electrons so you can include a multiplicative factor that has uh basically a term for every pair of electrons and so this is just a rotationally symmetric scalar term for every pair of electrons you multiply them all together and it's still an anti-symmetric function and this is called the jastrow factor and you can also so this sort of changes things after this later determinant you can also add correlations between electrons before they go into this later determinant so you can do a coordinate transform called the backflow transform where you you take another scalar function rotation rotationally symmetric function of electron positions just add up all of them for every other electron add it to this electron position and use that this new transformed coordinate as the input to the slater determinants this actually was originally developed by feynman and feynman and cohen for studying superfluid helium but it when you put all these terms together and you can also take a large linear combination of many of these determinants so putting these all together and plugging this into this variational quantum monte carlo you get the sort of standard slater jastro backflow function and this is just the this is like the go-to off-the-shelf method that everybody in in variational qmc uses um there are also fancy fancier things called like um using what are called geminals and faffians that have some advantages and so there instead of functions in a single electron you can plug in functions of two electrons but the performance you get is sort of in the same ballpark uh so this is this is kind of the baseline that we're comparing against and so the idea is that rather than rather than this fairly restricted wave function form can we actually write down a neural network that generalizes this and that's much more expressive than this and therefore can be expressed much more compactly because in theory if you have a large enough linear combination of slater determinants if you have enough of these coefficients to make k you will eventually get a universal representation for antisymmetric functions but uh as i mentioned where you do the case where you do a sort of arbitrarily large linear combination then you arrive at full configuration interaction and that is that scales factorially and often you'll need hundreds of thousands or millions of these determinants uh with these slater gastro functions to get good performance so it might seem like a neural network is in some ways more complicated might have a lot more parameters but if you want to actually make this accurate just the number of weights that you need here can go into the millions so it actually can be a huge huge number of parameters so as we know from a lot of machine learning replacing a big linear combination of things with a non-linear function approximation can often work a lot better okay so now we have to build an anti-symmetric neural network uh and that's that's kind of an unusual constraint you know a lot of work has gone into designing neural networks with things like um permutation equivariance translation equivariance or invariance a spherical equivalence is you know i guess max is at his meetings he knows all about that um in fact i believe taco spoke at one of these about not specifically about that so the fermionic neural network uh is specifically a neural network that is meant to be antisymmetric and so how do we how do we build that in in a neural network so the key idea is that we want to take these orbitals that we're plugging into these determinants these functions of a single electron or maybe a transformed coordinate of a single electron and generalize it so it becomes a function of all electron positions because the resulting determinant will still be anti-symmetric as long as the orbitals are symmetric in every input except one so you want the inputs to be permutation equivariant you basically put in n electron positions as input and you want to make it so that if you swap any two of these electrons the output will still be at the out the two outputs will be swapped and then you can take those output vectors pack them into matrices and take the determinants okay so then we have to say alright how do we represent a permutation equivalent function and so you can start with just a sort of standard neural network uh one for every single one for every single electron and so this is basically just a neural networky way of representing like cartridge fog if you pack these all into a determinant uh each of these phi's will just be a function of one electron so that's not doing anything novel or interesting but you can also add some kind of permutation equivalent interaction between all of these layers and so at each layer we can take the activation at one layer sum them all uh some all of them together and then concatenate those in as an input to the next layer and so this is actually an old idea for how to build equivariance permutation equivariance internal networks i think it dates back to uh john shaw taylor in like the late 80s or early 90s and using this to represent um anti-symmetric functions adding this determinant at the end although it's very simple it is in theory universal so marcus header had a nice paper showing that following our following our paper on the fermi net uh in practice you need to make a few extensions even though in theory it's universal in practice you have to add a few extensions to get good behavior and so at the moment a bit like these um a bit like these gastro factors we need some way or we found that adding in some way to represent correlation between pairs of electrons improves performance a lot and so we actually add in a second stream to the network where it's basically for every pair of electrons we have just a single mlp all running in parallel and then we basically sum up the activations for for one of these rows and concatenate those in as well as sort of a a second set of features going in at every layer and so that encodes features of pairs of electrons and allows you to capture things with electron correlation that's very difficult to do with this first part of the network uh and we also find that a single determinant isn't quite enough but a pretty small sum of determinants like a few dozen rather than tens of thousands is good enough to get very good behavior so we can answer the question on this yes absolutely so how about so in graph neural networks you would also include the adjacency matrix in some sense right and when you're yes could you do something similar here that you can you definitely could the systems we've looked at are only going up to maybe a few dozen electrons and they tend to be all pretty close to each other so basically you can think of this as sort of like a graph neural network that's just all to all um but i think for larger systems where you have a lot of sort of distant inter electrons that aren't likely to be interacting with one another so if we're scaling us to you know hundreds or thousands that's exactly what we'd want to do with some kind of more sparse graph neural network type thing yeah so that's interesting future thanks hello i also have a question yeah so here you have uh um the wave function does that mean every time you uh do back propagation you have to sample once from this wave function for the obvious wave function okay so that's very that's every time consuming um that's not the most time consuming step because uh basically to update the samples that you're putting into this network we don't uh because we're only changing the network a little bit every step we're not running the mcmc chain like all the way to equilibrium so we tend to just do a few steps and so it just needs a few network evaluations the real limiting step is actually evaluating the kinetic energy operator because there you need to evaluate the laplacian with respect to the inputs so that means you need to evaluate it basically has the same cost as evaluating the hashing of the network right but uh what are you talking about is that you are taking the second derivative of this of this but you're doing that numerically or you're doing that using autographs of stuff uh we're doing that with automatic differentiation okay cool thank you um but yeah in practice like something like 90 percent of one iteration is especially for larger systems is going to be the um the kinetic energy evaluation okay so yeah so implementation details um i hope this loads there's a nice uh animated gif that should pop up here but it's um i probably should downsample that um but yeah we're not doing anything very fancy on the mcmc side it's just conventional metropolis hastings sometimes for larger systems we're doing essentially gibb sampling or what they call one electron moves uh uh here it is so you can actually see uh the particles or what they call walkers uh this is for a nitrogen molecule and uh yeah so for larger systems gibbs sampling can be helpful we tried uh fancier fancier methods we tried you know hmc and other gradient based methods but found that oftentimes what happened was you'd make a proposal very far away it would get rejected the network will get updated and what was a good region becomes basically a sort of a bad low probability region and it's actually harder for hmc to escape that than mcmc and so you would get like huge spikes in your energy and it was sort of you have an interaction between optimization and sampling here so you're not just trying to sample from a fixed objective but you're sampling updating your network and those interact and sort of complicated ways that can sometimes lead to pathological behavior and i haven't seen a lot of research on sort of the interaction between sampling and optimization i've seen some on sort of two-time scale optimization so where you have two coupled optimization problems but not a lot on a coupled optimization and sampling problem so i wonder if there are sort of theoretical advances that could be made there that might be useful for this sometimes people will do things like um like use heuristics like things called space warp coordinates where they'll try to um to change the position of the walkers in a way that conforms with how the network is changing but those are more sort of heuristic so that's one uh one implementation detail that you have to get right another is that the optimization because we need to get really really high accuracy and we're less worried about generalization than we are about asymptotic performance so it's a little different from um for most of machine learning where if you under optimize things a little bit it's actually good uh you're more likely to get uh get good generalization here we actually really want to like optimize things like to the bone uh so we use um an approximation to a natural gradient descent called chronic or factored approximate curvature or kfap so this is works very well with neural networks because you basically assume that your fischer information matrix has this block diagonal structure and that the blocks themselves have this chronic or factorized structure that matches the structure of the neural network and we just find that you get you know you cut the error by like an order of magnitude even on small systems this is just showing optimization on a log log scale for single atoms so just lithium up to neon and the larger the system gets the larger the improvement gets um and we're not really sure why there's this big discrepancy i think some of it is that the fermi net is sort of an unusual architecture so a lot of networks like resnets and things like this are kind of designed to work well with adam and we're not we're designing our network to work well at representing a wave function so these sort of approximate second order methods might um uh might be the the model itself might be sort of ill-conditioned and so these sec approximate second quarter methods don't struggle with that as much but yeah in a lot of machine learning kfac has gotten a reputation for being like very fancy but doesn't give you much of an advantage but on this application at least we find a huge advantage um that's the other thing is that there's a as i said a lot of kind of connections between these fields you could almost write a glossary kind of going from machine learning terminology to computational chemistry terminology and computational like quantum monte carlo terminology and in the electronic structure community uh the stochastic reconfiguration method is almost exactly the same as natural gradient descent or at least you end up with the same algorithm there are some things that you can do in the quantum setting if you're looking at say time evolution of a system and so this matrix then becomes um complex so in some sense it's actually more general than just uh just natural gradient descent but there's a really beautiful mathematical connection between these fields um so that's that's how we implemented it and how well does it do so getting to these results so we wanted to look at really apples to apples comparison against these slater jastro backflow networks and so that involved looking at things with the exact same number of determinants so that things should be in some sense equally equally expressive uh so there's very good benchmark data for single atoms from this uh from this paper from a decade ago uh first of all typically these orbitals are represented by discretizing the space by picking something a bit like a fourier basis or like spherical harmonics to represent functions in r3 and that's called the basis set and so just by using a neural network and place it rather than a basis set we can already get some improvement in the in the energy but we find using the fermi net we cut this error dramatically and the gray area here is um that's chemical accuracy the standard of one kilocalorie per mole i mentioned so uh one determinant isn't quite enough to get chemical accuracy but it's far more accurate than a slater gastro wave function uh if we start adding determinants so this is now for a larger system so this is carbon monoxide and the nitrogen molecule so this we're looking at systems with about 14 electrons here uh you can see that this slater gastro network uh gets you know more and more accurate as you add more determinants um but it also gets a lot harder to optimize which is why you see a bit of a bump here is actually difficulty in optimization so you should see this as an upper bound but the performance of the of the fermi net you know is always better and basically gets down almost to chemical accuracy just with a few dozen a few dozen rather than tens of thousands or hundreds of thousands of determinants and it's also substantially easier to optimize these optimization difficulties that we have here we're totally absent with the fermi net the vermin that gave you you know pretty identical results every time that's a question about uh what exactly you're optimizing yeah sure so um i understand that function that you're looking at you're finding states that are closer and closer and energy to the true ground state yeah you have a similar sense of whether they actually look anything like the true ground state because uh yeah situations where you have a highly degenerate ground space and a true ground state is or almost degenerate ground space where the true ground state looks very different from some low energy state uh yeah i mean i think systems like that i think i guess i think that's often what is called referred to as like multi-configurational uh and typically you need a lot of a lot of determinants to represent them uh we found i guess like the nitrogen dimer when you're sort of in the bond breaking region because that's uh that's a very complicated bond you have sort of three electrons from each atom and in that region you sort of got these two states one that looks a lot like two isolated atoms and one that looks like two bonded atoms very close together we find the energies are quite good in that region and i think of slides somewhere in there um we you know we've looked in some systems at things like the energy density and um even the uh the exchange correlation holes so basically looking at the difference between the energy density uh on average and the energy density conditional on another electron being nearby and you do see like exactly the kind of holes that you expect the electron densities for the for the hydrogen chain look reasonable but of course there's always the question of like well what's the ground truth that you're comparing against so if you have any ideas of systems that would be useful to look at i you know we're always we're always open if you run this on the hydrogen atom for which you have an analytical solution to what the function yeah well so this is just for ground states um i've sort of run similar i ran a similar method that we were calling spectral inference networks uh where you're not actually sampling from psi squared but you can get excited states but because you have to do like it you're not getting unbiased gradients in that case so the optimization is very very very tricky it's very um very delicate and so it was a lot uh it was a lot more difficult to scale but what we found there was for like p orbitals you know you would get three orthogonal p orbitals um but they would you know they would sort of rotate uh over the course of um over the course of optimization so they would sort of uh you would get sort of an arbitrary as you said they're degenerate so you would get like an arbitrary rotation because there's nothing in the nothing in the loss function that you're minimizing that picks a certain rotation over the other so if it's degenerate it'll just sort of arbitrarily pick one of them yeah i guess what i'm suggesting is in situations where you independently have an analytical form for the ground state you can calculate various overlap or state fidelity kind of functions yeah yeah i think you would if you have a for anything we're looking at you wouldn't really have an analytical solution you would have like you would have sort of other computational solutions so you might be able to get like a solution in a particular in a particular basis set so maybe you could try comparing it against that um do i think i mean the most what we've been using is the sort of ground truth reference for a lot of these has been coupled cluster methods which work well for equilibrium systems but i think you don't actually get an explicit onsauce out i think i think the wave function ends up being represented implicitly i'm less of an expert on coupled cluster there but yes maybe maybe for like uh some of these model spin systems heisenberg systems and stuff like that you could get analytical solutions but you know part of the reason we're doing this is we're interested in solutions where there are no analytical methods yeah i guess i'm it's just a little unclear whether you'll actually get the true ground state if that's the specific thing that you want if there are degeneracies you don't know about yeah i think uh yeah what you see there is that well there n2 is also a good system for this because um i don't have the figures in here but we've been looking a little at um spin states and so there is actually kind of a transition where the ground state goes from having um zero uh spin spin magnitude to a non-zero spin magnitude and you do see the energies are like the area where it struggles the most where the energies are highest relative to that's small enough that you can do something like full configuration interaction and what you see is that the specifically where that transition happens is also where the errors are the biggest but the errors are still on the order of you know five or ten heart trees so it's still pretty accurate it's still better than coupled cluster okay so i'll try to get through some of these results in the time that we have so as i said for ground state atoms we do a lot better uh and in fact so diffusion monte carlo which i said was this it's pretty much always more accurate than a variational monte carlo if you do it right uh isn't red here uh this is this is variational monte carlo using the standard slater jaspro backflow wave functions and this is the fermi nut here and since these are small systems you can also do these exact full con these really really accurate exact methods uh coupled cluster because it's not actually variational actually overshoots on some of these systems um but we're getting it pretty much smack on so better than better than diffusion monte carlo on many systems which is which was surprising to us and i think was surprising to many people uh we can also generalize this to bigger molecules so as i said uh the correlation you know the level of accuracy that we're looking for is this tiny fraction of the total energy and looking at these molecules from you know lithium hydride which is four atoms up to bicycle butane which is like 30 atoms uh we do get um [Music] some degradation and performance we're using coupled cluster as the baseline here because these systems are just too big to do exact calculations so we get better than 99 of correlation up to some pretty big systems um and even bicycle butane is still i would say impressively good for a system that big uh and the results we're comparing against we're actually doing some extrapolation so this is comparing the error to the extrapolated results but these are the different unextrapolated coupled cluster methods at a certain point these systems get so big that the most accurate ones just went out of memory uh whereas we use networks of a fixed size and so we are more accurate than coupled cluster in a fixed basis set basically and so we think that if we were to use sort of bigger and bigger networks and do a similar extrapolation uh we hope we think that we would also find results this good but um but this is this is still you know impressively accurate and some sense because you know we're comparing different computational methods to one another it's not always clear what the what the standard of accuracy is it's not clear what the ground truth is uh so what you really want to do is compare against really accurate experimental results so on this system this largest system by cyclobutane that we've looked at with 30 electrons the transition from bicycle butane to butadiene has multiple possible conformations this what they're what are called the conrotatory and disrotatory pathways and this is also a system that coupled cluster struggles with uh it predicts i believe that the disrotatory pathway is the lower energy and preferred pathway while actually experiments show that the conrotatory pathway is uh is the true solution uh so just comparing the difference in the uh fermi net energy uh for the different systems and comparing against a very accurate diffusion monte carlo computations which uh which are consistent with with other even more sophisticated coupled cluster methods uh we do find that for all of the different transition states we get basically um we get within chemical accuracy of these other methods so this worked pretty much out of the box we didn't have to do any sort of fancy tuning which is often very rare for um for some of these fancier quantum chemistry methods so this is this is really encouraging results that even on bicyclobutane where we don't know how much to trust the absolute energy the relative energy seems to really nail it okay and you know we've also looked at some of these benchmark systems like the hydrogen chain uh which is a really good benchmark system because there was this paper just with you know a million authors on it where they compared every kitchen sink computational electronic structure method um and so this is one of these exact methods as the baseline uh you know just looking at the energy curve they're all so close that you can't even see it without zooming in standard variational monte carlo is here in um here in yellow but the fermi net is in is in light blue is again much much much more accurate and looking at network ablations we can get a sense of what's actually what's actually working for these networks so adding layers helps up to a point but actually past three or four layers it doesn't make a huge difference um adding more units to this two electron part of the network it's part of the network meant to capture electron correlation helps up to a bit but again it tops out with pretty small layers like you only need about 32 units to get things pretty accurate where you see the biggest improvement is in improving the is in making the layers wider wider not deeper which surprised us in the one electron part of the network and there you see sort of an ex exponentially decreasing errors as you make things wider okay and as i said there's sort of this burgeoning field of deep qmc a lot of a lot of ideas bouncing around about how to combine uh deep learning and quantum monte carlo methods how to do ab initio calculations self-generative calculations and sort of around the same time that our paper came out another method called the paulinet was published uh that was using like this chernat or sknat is one of these um equivalent neural networks or permutation equivariant neural networks or graph neural networks developed originally i think for um [Music] originally for potential energy surfaces but you can use it also to do this the same idea that we said of constructing permutation equivariant inputs to the to the determinants their networks are a little smaller so i think the biggest difference is basically that their networks are smaller so they train faster but are less accurate in terms of overall energy so this is looking at the cyclobutadiene which has two two different configurations that'll sort of oscillate between and in terms of absolute energy uh the fermi nut gets um you know something like 40 50 ml arches lower looking at the relative energies between the uh the rectangular and the square state we get pretty similar results and these results are all basically at the high end of the experimental range and again this is yet another system that coupled cluster struggles with because anything that's sort of away from the ground state anything that's stretched ccsdt is not very good at but these multi-reference coupled cluster methods tend to tend to work much better for and so we're pretty much in agreement with all of them and again this works pretty much out of the box or they both work pretty much out of the box so there's sort of a there's sort of a whole class of models like these uh neural network backflow was maybe one of the first but that was exclusively for discrete systems so they looked at hubbard models and things like that we were one of the first for continuous systems um there was a method called deep wf that did not use determinants it used something that scaled quadratically rather than cubically but the results were just not very accurate so we were also one of the first methods to reach better accuracy than archie fox so archery is kind of you know table stakes in quantum chemistry if you can't do better than archery fog there isn't much of a point uh so we were one of the first to show that um and there was another paper that came out around the same time as ours in the palinet that rather than representing things in first quantization where you have positions of the electrons as input they represent things in second quantization where you're actually counting the occupancy of different states um so that's sort of an interesting alternative approach and there's a lot of active work in that direction as well and i think there's even there was even a recent paper on something called deep irwin that's yet another one of these methods that's also sort of a small small low accuracy method like the palinet so this is uh you know i'm not gonna say exploding field but there is there's a lot of work in this field and so we hope that sort of newer more advanced architectures will come along and it will do even better than even better than the fermi net to wrap up i guess i'm pretty much out of time uh so we've designed a new type of neural network that is anti-symmetric with respect to its inputs uh specifically to use as a way of representing wave functions combining that with powerful approximate second order optimization is a really effective strategy and gets you know for for especially for using deep learning uh you know incredibly accurate results uh scientifically you know levels of accuracy that are good enough for scientific computation um there's some difficulties with uh interaction between optimization sampling that i think are actually counter-intuitive um but we've actually shown that this fairly cheap and less accurate variational quantum monte carlo method can be made competitive with and in some cases even better than things that are typically considered the gold standard for for all electron calculations um but there are a lot of a lot more things to do in terms of scaling to bigger systems adding in periodic boundary conditions if you want to extend this to condensed matter physics uh there you have to treat you know systems which have periodic boundary conditions uh we'd like to extend this to heavier atoms uh i sort of breezed through this but we've actually got really accurate results all the way up to argon now so all the way up to electrons with um with 18 uh all the way up to atoms with 18 electrons atomic weight 18 but getting beyond that into things like transition metals becomes a lot harder and just um just lots of room for improving the optimization things like that so that i want to wrap up and say you know thank you to my collaborators uh and this was actually a real you know a real joint collaboration between sort of machine learning people and quantum chemistry people uh so james spencer and i have you know worked uh worked together on this from the very start uh he he comes from a quantum chemistry background and i come from a machine learning background and matthew folks at imperial college was uh was very kind to advise us on many of the finer points of quantum monte carlo and you know the papers are up online um and the code is online as well uh including uh a new version in jaxx so please take this play play with it take it for spin you know find bugs in it give us pull requests uh and if you have any more questions don't hesitate to get in touch thank you thank you very much for the very nice talk david um questions hello i have a question so um is is the gestural factor necessary in your for me now uh so we don't have a gastro factor in the fermi net what we have is this part of the network yeah yeah yeah yeah we have this part of the network that takes pairs of electrons as input and so that's sort of like a generalized combination of backflow and gastro factor um but yeah so one of the things that gastrofactors are really good at is capturing what are called the cusp conditions and that's what happens when basically two electrons overlap or an electron and a nucleus overlaps and then you have a divergence in the in the potential energy the potential energy goes to infinity and what that means for the what you can show is analytically the ground state wave function it has to be non-smooth there so it's well defined but it uh the derivative isn't defined and we found that if you all you have to do is actually put the the absolute value of the distance between electrons in as an input and then that gives you a function that is non-smooth at the cusp as an input feature to the network so the network itself is smooth but the inputs are a non-smooth function of the electron positions and that is enough for the network to learn the cusp conditions but if you don't include that the energy is way off it really struggles so the in your network determines like exponential minus the distance so that term is very important in your inner structure yeah so we we don't put in an exponential we just put in the absolute value of the uh of the distance but we probably could put in some some monotonic function of that but we sort of let the network itself learn what that monotonic function is um and you're saying that even you have that uh kind of like a gesture style thing your hmt still fails hmc still doesn't work yeah hmc hmc just really uh really struggles i don't think it's because of the cusps we thought it might be like going crazy when it goes near the nucleus um but we're actually we're not doing hmc on the potential energy okay we're doing hmc on psi squared and psi squared is pretty well behaved uh so the problem is how it interacts with the optimization it's basically because it'll make a far away step step gets rejected um and i think with if you're using metropolis hastings it makes lots of little steps so it might not mix as fast but it's less likely to get stuck something like that so probably with more work we could fix that but metropolis hastings seems to work well enough that it's not killing us it doesn't seem like a limiting factor okay thank you all right so i have a question on um why you think that this is like doing a better job than the jastro factor so like it seems like with machine learning you're getting some kind of nonlinear approximation or something like that like you're using like a low it's almost like you're getting a low rank approximation of the solution which you're able to represent in the fewer uh determinants so if the hamiltonian is linear why are you getting that to happen um i feel like there were a few different questions there when the hamiltonian is linear but you know it's a linear operator on a very high dimensional function so it's still a very very big space of you know i mean it's an infinite dimensional space but it's a big hilbert space of things that you're trying to find a solution in um so yeah so you want to build in as much as you know about the solutions possible uh and so like the powly nut builds in the cusp conditions whereas we learn them so we build in the non-smoothness of the costs but we don't build in what the actual like slope of the cusps is um i mean just empirically we we see i think we've tried we've played around a little bit with building an exact cusp conditions uh we haven't really gotten it to work very well but again i think that's just a matter of not putting in as much effort because empirically it seems like it learns them fairly quickly and so it seems like there's just other stuff that takes much longer to learn that uh so much so that this doesn't seem like a limiting step okay but i don't know it's the finding side that's the that's the yeah it's like you know one thing with the cost condition about the shape of psi in you know a set of points of measure zero basically but you know what psi looks like in the entire rest of the the input space you don't know okay connor but yeah i think an exact cusp version of the fermi net probably would work it's just uh just a question of just a question of uh finding the time to do it i have a question uh yeah go ahead yeah um could you talk about the computational performance of firminet uh so as far as i understand the network is growing with the number of electrons yeah so how does the computation perform scale and how um could you also mention like the scaling bottlenecks of the code you mentioned calculating the kinetic terms but could you give more details about the computational terms that are costly sure yeah so the network does does grow so you can have a network with a fixed number of parameters um and there's a question of how many parameters do you need to reach a certain level of accuracy but let's just set that aside and say for a fixed number of parameters the uh the com the overhead in terms of space will grow linearly or no actually for one part of the network will grow linearly but for another part will grow quadratically because the number of pairs of electrons grows quadratically so overall the space requirements grow quadratically um evaluating a forward pass grows cubically because the determinant actually is a cubic function um we're t at the moment we're dealing with small enough systems but that isn't really the bottleneck in practice the constant factor from other stuff tends to be bigger but in a certain you know as things become big enough the determinant computation will dominate there's another reason you might want to do something graph nutty for bigger systems um because then you could do like sparse determinants um the then in terms of actually computing an update step uh the kfac will scale cubically in the width of the layers because you have to invert a matrix that's the size of the width of the layers but again that's not the bottleneck the real bottleneck is this kinetic energy computation because then basically computing a second derivative has like four times the cost of doing a forward pass um because you basically do a grad of a grad and then you have to do that n times you have to do that for every direction or three n times you have to do that for every direction in space uh so if a value if a forward evaluation scales is n cubed computing the kinetic energy will scale is n to the fourth so that really uh that really dominates um which is why you will need to do some kind of sparse thing to to scale to truly large systems uh in practice what that means is like for these biggest systems i showed like the 30 electron systems like bicycle butane and butadiene we're talking like eight v-100 gpus maybe running for a few days so it's not cheap um but you know if you look if you go on some of these like big uk national super computers and look at what they're actually spending most of their compute time on it's basically either fluid dynamics or computational electronic structure stuff so people are certainly burning lots of compute cycles with these other methods as well okay thank you i think we have a few text questions have you done anyone to benchmark with diffusionq results um we're looking at that a little bit there was a paper recently from um max wilson that looked at diffusion qmc on some very small systems like first row atoms and i think they were maybe able to get an improvement of well you should look up the paper yourself because i might be misrepresenting this but um they got a very small improvement of maybe like a millihar tree or a fraction of that uh because on the really small systems as i said comparing against the exact methods we're almost nailing them it's a fraction of a fraction of a molar tree uh some of the larger systems that are interesting um i hope that answers your question uh can you compute forces on the nuclei so um the forces on the nuclei you mean like i mean like if you change the geometry of the system if you like stretch it what are the forces like moving the nuclei around where it is yeah yeah yeah uh in theory yeah there's the you know the feynman hellmann theorem says that you can do it at an equilibrium in practice there are um there are some error terms if you're not exactly at the at the equilibrium so to get accurate results you typically have to include those error terms as well one of the really nice things about this uh is like computing forces and diffusion quantum monte carlo is really hard um but there are ways to do it in variational quantum monte carlo so you can do it with the fermi net the same as you could do it with a slaver jastrow onsites so this is we we haven't published anything with that but you can you can do it the same as you can with any other vmc method so yes i actually have a question on this case back so yeah have you actually compared the research from kpec to this natural gradient descent or stochastic configuration so yeah have you compared the research between keratin so numerically the only real difference between kfac and stochastic reconfiguration is that there's a normalization term yeah yeah yeah but what i was asking is that the difference between tape back and yeah oh even if i wasn't using the approximation if i was doing the same yeah then how much it would be accurate well that's not practical to do on a network of this size because we're talking about hundreds of thousands of parameters yeah sure so we actually can't uh we can't do exact stochastic reconfiguration um and you know i guess in principle you could try kfac on like a very very small neural network that has only a few parameters but we haven't done that and also if you're looking at a tiny neural network you sort of know tiny neural networks don't work that well anyway yeah sure so yeah okay yeah another question is that uh about how many stampers actually uh it actually consumes to be trained to how many samples so we typically do we're using batch sizes of a few thousand walkers and so i think all the results i showed is with 4096 and we typically run it for a few hundred thousand iterations yeah i know typically i think in vmc they'll they'll really go for trying to get exact gradients so you'll run the sampler for hundreds of thousands of steps okay and then do one so you'll only do like maybe 100 uh parameter updates but we wanted to do something a bit more like sort of best practice in in deep learning there's you know lots and lots of little steps instead of a few really big steps okay so what you mean is that raspberry size actually should be used yeah large batch sizes do make a difference also yeah yeah that helps uh dude is there any reason why do you think this is the case large batch sizes some of it i think comes down to actually the derivatives getting less noisy derivatives so when you compute the derivatives uh there's a term that's basically the difference between the local energy and the expected energy and the way you compute the expected energy is just by averaging over a batch so you'll get less noisy estimates of the expected energy if you use bigger batches so you'll get less noisy gradients that's my guess yeah well yeah but in user machine learning things what people do is actually large vegetables leads towards generalization and they want to avoid certificates but we don't really need to worry about generalization the same way because we're doing self-generative learning okay so we can just generate more samples so that's what's an issue for us thank you welcome uh i think there's word inside delta psi equals into now the size squared oh um is it saying would it would it be easier if we were using um a normalized wave function is that what the question is from david hold steeler not not sure i get the question for international or is this about the derivatives sorry i think you're muted david to remove the second order groups um for computing forces i'd have to i'd have to go through the the math a little bit more about what you mean for what do you mean for optimization anyway i'd have to uh oh yeah um i'm not sure how that would work i mean we have a fairly complicated neural network wave function so i don't know how you could write down a form of the wave function where uh where you could use or are you saying using um using finite differences to compute the laplacian instead of uh instead of doing it with auto grad so i think if we tried finite differences it wouldn't be accurate enough we'd we'd see big numerical errors and it would end up having about the same cost anyway because you'd have to do about the same number of using integral equality um well uh oh oh well so i think uh i think that integral equality does get when we're computing the derivatives there is a term that we can remove thanks to that integral quality but i don't think you can you still end up having local energies somewhere in the uh somewhere the derivative i don't think you can get rid of them entirely i'd be happy to go through the through the math on on the derivatives in more detail or we have an expression for the derivatives in the paper um offline if you want but yeah yes david berman yeah hi um i was interested by your comment about putting in periodic boundary conditions so yeah i i i guess you could do things like metals and get block waves if you did that basically is it difficult because it seems like it be a relatively easy thing to do what's the obstruction i guess i'm asking there are there are some things like um like making sure the input conditions are correct uh you have to when you're computing the potential energy you have to do things like you all order called sums um i think when you start looking at like block waves you have to start worrying about complex wave functions again things like that so there are a few details but you know it's not um it's not like you'd have to totally go back to scratch just got one other sort of perhaps more general question you set this up with anti-symmetry because you wanted to mimic electrons are there any other problems that you've thought of some general problem where you want an anti-symmetric neural net well what other what other fermions are there um the only case i've been able to think of and i've not really been able to get the people who work on this interested in it which i think is a shame um is uh is game theory is because there you have if you have an n basically you have a value function and for a two player game if it's a zero sum game the value of one player's position is the negative of the other player's position and so in an end the generalization to an end player game is that the value from the a zero-sum n player game without an anti-symmetric value function yeah and there's also so all sorts of interesting stuff there um but i think so there's some people in like multi-agent reinforcement learning who are interested in this stuff but they tend to do things a bit more like this deep wf wave function where you just have pairwise terms and take a product of pairwise terms and i think that's that's what they've done but that's the only other case i've been able to think of but this might be useful yeah okay thanks all right is that everything i think so um thanks again for super interesting talk um thank you for having me well i think we're in there