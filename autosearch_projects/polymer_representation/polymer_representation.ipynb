{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alibina/miniconda3/envs/autogen/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autosearch.functions.text_analysis import chunk_pdf\n",
    "\n",
    "from autosearch.database.paper_database import PaperDatabase\n",
    "from autosearch.analysis.document_analyzer import DocumentAnalyzer\n",
    "from autosearch.research_project import ResearchProject\n",
    "from autosearch.write_blog import WriteBlog\n",
    "\n",
    "import autogen\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve Azure credentials from environment variables\n",
    "config={\n",
    "    'doc_api_key': os.getenv(\"DOCUMENT_INTELLIGENCE_KEY\"),\n",
    "    'doc_endpoint': os.getenv(\"DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Exploring the Intricacies of Polymer Representation: Unraveling Complexity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging session ID: 1b798eeb-c767-4a62-8ce0-3330aca3150c\n",
      "\u001b[92m    Location = ./polymer_representation/0.3/db/uid_text_dict.pkl\u001b[0m\n",
      "Equipping function 'academic_search' to agent 'topic_expert'\n",
      "Equipping function 'academic_retriever' to agent 'topic_expert'\n",
      "\u001b[92m    Location = ./polymer_representation/0.3/db/uid_text_dict.pkl\u001b[0m\n",
      "Equipping function 'academic_search' to agent 'research_resource_expert'\n",
      "Equipping function 'academic_retriever' to agent 'research_resource_expert'\n",
      "Equipping function 'get_pdf' to agent 'research_resource_expert'\n",
      "\u001b[92m    Location = ./polymer_representation/0.3/db/uid_text_dict.pkl\u001b[0m\n",
      "Equipping function 'academic_retriever' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'academic_search' to agent 'blog_editor-in-chief'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The return type of the function 'wrapper' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equipping function 'get_pdf' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'get_pdfs' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'write_section' to agent 'blog_editor-in-chief'\n",
      "Equipping function 'factual_check' to agent 'content_strategist'\n",
      "Equipping function 'academic_retriever' to agent 'content_strategist'\n",
      "Equipping function 'academic_search' to agent 'content_strategist'\n",
      "Equipping function 'get_pdf' to agent 'content_strategist'\n",
      "Equipping function 'get_pdfs' to agent 'content_strategist'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The return type of the function 'wrapper' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equipping function 'write_section' to agent 'writing_coordinator'\n",
      "\u001b[92m    Location = ./polymer_representation/0.3/db/uid_text_dict.pkl\u001b[0m\n",
      "Equipping function 'factual_check' to agent 'content_review_specialist'\n",
      "Equipping function 'academic_retriever' to agent 'content_review_specialist'\n",
      "Equipping function 'academic_search' to agent 'content_review_specialist'\n",
      "Equipping function 'get_pdf' to agent 'content_review_specialist'\n",
      "Processing local PDFs...\n",
      "Skipping 2205.13757v1.pdf as it is already in the database.\n",
      "Skipping 2102.08134v2.pdf as it is already in the database.\n",
      "Skipping 2311.15481v3.pdf as it is already in the database.\n",
      "Skipping 2205.08619v1.pdf as it is already in the database.\n",
      "Skipping 2406.04727v1.pdf as it is already in the database.\n",
      "Skipping 2312.04013v3.pdf as it is already in the database.\n",
      "Skipping ijms-10-05135.pdf as it is already in the database.\n",
      "Skipping 1809.08090v1.pdf as it is already in the database.\n",
      "Skipping schmid-2022-understanding-and-modeling-polymers-the-challenge-of-multiple-scales.pdf as it is already in the database.\n",
      "Skipping 2311.14744v1.pdf as it is already in the database.\n",
      "Skipping 2109.02794v1.pdf as it is already in the database.\n",
      "Skipping 1812.11212v1.pdf as it is already in the database.\n",
      "Skipping 2011.00508v1.pdf as it is already in the database.\n",
      "Skipping 1805.11924v3.pdf as it is already in the database.\n",
      "Skipping 2105.05278v1.pdf as it is already in the database.\n",
      "Skipping 2010.07683v1.pdf as it is already in the database.\n",
      "Skipping 2209.01307v4.pdf as it is already in the database.\n",
      "Skipping 2209.14803v1.pdf as it is already in the database.\n",
      "Skipping polymers-05-00751.pdf as it is already in the database.\n",
      "Processed 0 files\n",
      "Skipped 19 already processed files\n",
      "\n",
      "1b798eeb-c767-4a62-8ce0-3330aca3150c\n"
     ]
    }
   ],
   "source": [
    "blog_project = WriteBlog(\n",
    "    project_id = \"polymer_representation\",\n",
    "    version= \"0.3\",\n",
    "    config=config,\n",
    "    config_file=\"../OAI_CONFIG_LIST-sweden-505\",\n",
    "    initiate_db= False,\n",
    "    funcClsList = [\"FactualCheck\", \"GetPDF\", \"GetPDFs\", \"UrlCheck\", \"AcademicRetriever\", \"AcademicSearch\", \"WriteSection\"],\n",
    "    communiteList = [\"instructor_agents\", \"outline_agents\", \"write_section_agents\"],\n",
    "    local_papers_dir=\"./papers\",\n",
    "    title=title,\n",
    "    target_audience=\"expert in experimental polymer science and machine learning experts\",\n",
    "    # models = [\"gpt-35-turbo\", \"gpt-35-turbo-16k\"],\n",
    ")\n",
    "project_config = blog_project.ProjectConfig\n",
    "print(project_config.logging_session_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mtopic_expert\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "        We need to create comprehensive instructions for the Editor-in-Chief to write an article titled:\n",
      "        \"Exploring the Intricacies of Polymer Representation: Unraveling Complexity\"\n",
      "\n",
      "        The target audience for this article is: expert in experimental polymer science and machine learning experts\n",
      "\n",
      "        Each agent should contribute their expertise to craft a well-rounded instruction,\n",
      "        keeping in mind the specific needs and background of the target audience.\n",
      "        Topic Expert, please start by analyzing the title and providing key insights,\n",
      "        considering how they should be presented to this audience.\n",
      "        \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: structure_specialist\n",
      "\u001b[0m\n",
      "\u001b[33mstructure_specialist\u001b[0m (to chat_manager):\n",
      "\n",
      "As a Structure Specialist Agent, I will outline an effective structure for the article \"Exploring the Intricacies of Polymer Representation: Unraveling Complexity,\" targeting experts in experimental polymer science and machine learning. Since the audience is highly knowledgeable, the structure will delve deeply into technical methodologies while ensuring the content remains focused and engaging.\n",
      "\n",
      "1. **Introduction to Polymer Complexity**\n",
      "   - Word Count: 300-400\n",
      "   - This section should set the stage for the article by introducing the complexity of polymers and the importance of accurate representation in scientific and computational contexts.\n",
      "\n",
      "2. **Historical Overview of Polymer Representation Techniques**\n",
      "   - Word Count: 400-500\n",
      "   - A brief history illustrating the evolution of polymer representation, touching on key methodologies that have been traditional in experimental polymer science.\n",
      "\n",
      "3. **Theoretical Foundations of Polymer Modeling**\n",
      "   - Word Count: 700-900\n",
      "   - Deep dive into the theoretical frameworks used in polymer representation. Strategies can include various scaling concepts, statistical mechanics, and computational methods.\n",
      "\n",
      "4. **Modern Approaches in Polymer Representation**\n",
      "   - Word Count: 800-1000\n",
      "   - Focusing on recent advances in polymer representation, particularly those driven by machine learning. This section can outline the cutting-edge of how complex polymers can be modeled and understood.\n",
      "\n",
      "5. **Case Studies: Machine Learning in Polymer Representation**\n",
      "   - Word Count: 600-800\n",
      "   - Presenting real-world examples of how machine learning has been used to represent polymers. This will make the content more relatable and demonstrate the practical application of these methodologies.\n",
      "\n",
      "6. **Challenges and Limitations in Current Methodologies**\n",
      "   - Word Count: 500-600\n",
      "   - Discussing current challenges faced by experimentalists and computational scientists in this field. These may include issues with data availability, complexity of systems, and computational limitations.\n",
      "\n",
      "7. **Future Directions and Theoretical Implications**\n",
      "   - Word Count: 400-500\n",
      "   - Speculating on how emerging technologies and theories might impact the field going forward. Also addressing how these future techniques could unravel even more intricacies of polymer systems.\n",
      "\n",
      "**Overall Word Count**: 3700 - 4700 words\n",
      "\n",
      "This proposed structure gives a comprehensive view, balancing historical context, current methodologies, practical applications, and future directions.\n",
      "\n",
      "**Visual Aids**:\n",
      "- **Diagrams of Polymer Structures**: To visually represent the complex structures discussed.\n",
      "- **Charts/Graphs**: Visualization of computational models, machine learning algorithms' performance, and case study results.\n",
      "- **Flowcharts**: To elucidate machine learning pipelines used in the representation of polymers.\n",
      "\n",
      "The content should aim to challenge and inform the target audience, with an assumption that readers are already familiar with the basic concepts of polymer science and machine learning. The article should weave in high-level information and avoid explaining foundational concepts that the intended readers would already know.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: audience_adaptation_expert\n",
      "\u001b[0m\n",
      "\u001b[33maudience_adaptation_expert\u001b[0m (to chat_manager):\n",
      "\n",
      "As a Topic Expert, here are the specific insights that should be considered for the article \"Exploring the Intricacies of Polymer Representation: Unraveling Complexity\" for an audience of experts in experimental polymer science and machine learning:\n",
      "\n",
      "1. **Nature of Polymer Systems**: \n",
      "   - Address the complexity of polymer systems, especially focusing on the wide range of monomers, varied polymerization processes, and the consequent structural diversity.\n",
      "   - Discuss the implications of this complexity in terms of properties, behaviors, and the challenges they present for accurate representation.\n",
      "\n",
      "2. **Data-Driven Approaches**:\n",
      "   - Explore how machine learning models require high-quality data for training, and how experimental polymer science provides this through meticulous experimentation.\n",
      "   - Analyze the synergistic relationship between the data generated by experimental scientists and the computational power of machine learning.\n",
      "\n",
      "3. **Algorithmic Developments**:\n",
      "   - Present the latest machine learning algorithms and how they are catered to deal with the high dimensionality and complexity of polymer representation.\n",
      "   - Discuss examples like convolutional networks for image-based polymer representations or recurrent neural networks to understand polymer sequencing.\n",
      "\n",
      "4. **Interdisciplinarity Application**:\n",
      "   - Emphasize the necessity for a multidisciplinary approach that melds machine learning with polymer science for the sophisticated task of polymer representation. \n",
      "   - Highlight case studies where such an integrated approach has led to breakthroughs in understanding or novel material discovery.\n",
      "\n",
      "Since the target audience is highly specialized, the article should present concepts using the specific jargon of both fields without over-explaining basic principles. Use case studies to provide concrete examples of abstract concepts and explore the cutting-edge aspects of both disciplines' contributions to polymer representations. The tone should be professional and academic, yet accessible, avoiding too dense or unreadable segments that might disengage even knowledgeable readers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: research_resource_expert\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 17\n",
      "Add of existing embedding ID: 19\n",
      "Add of existing embedding ID: 20\n",
      "Add of existing embedding ID: 21\n",
      "Add of existing embedding ID: 23\n",
      "Add of existing embedding ID: 23\n",
      "Add of existing embedding ID: 24\n",
      "Add of existing embedding ID: 25\n",
      "Add of existing embedding ID: 25\n",
      "Add of existing embedding ID: 29\n",
      "Add of existing embedding ID: 32\n",
      "Add of existing embedding ID: 38\n",
      "Add of existing embedding ID: 54\n",
      "Add of existing embedding ID: 62\n",
      "Add of existing embedding ID: 65\n",
      "Add of existing embedding ID: 75\n",
      "Add of existing embedding ID: 79\n",
      "Add of existing embedding ID: 84\n",
      "Add of existing embedding ID: 84\n",
      "Add of existing embedding ID: 86\n",
      "Add of existing embedding ID: 88\n",
      "Add of existing embedding ID: 91\n",
      "Add of existing embedding ID: 92\n",
      "Add of existing embedding ID: 95\n",
      "Add of existing embedding ID: 103\n",
      "Add of existing embedding ID: 103\n",
      "Add of existing embedding ID: 114\n",
      "Add of existing embedding ID: 121\n",
      "Add of existing embedding ID: 122\n",
      "Add of existing embedding ID: 128\n",
      "Add of existing embedding ID: 131\n",
      "Add of existing embedding ID: 135\n",
      "Add of existing embedding ID: 136\n",
      "Add of existing embedding ID: 140\n",
      "Add of existing embedding ID: 147\n",
      "Add of existing embedding ID: 149\n",
      "Add of existing embedding ID: 185\n",
      "Add of existing embedding ID: 185\n",
      "Add of existing embedding ID: 186\n",
      "Add of existing embedding ID: 186\n",
      "Add of existing embedding ID: 187\n",
      "Add of existing embedding ID: 197\n",
      "Add of existing embedding ID: 198\n",
      "Add of existing embedding ID: 199\n",
      "Add of existing embedding ID: 199\n",
      "Add of existing embedding ID: 199\n",
      "Add of existing embedding ID: 203\n",
      "Add of existing embedding ID: 208\n",
      "Add of existing embedding ID: 218\n",
      "Add of existing embedding ID: 220\n",
      "Add of existing embedding ID: 222\n",
      "Add of existing embedding ID: 222\n",
      "Add of existing embedding ID: 237\n",
      "Add of existing embedding ID: 241\n",
      "Add of existing embedding ID: 242\n",
      "Add of existing embedding ID: 242\n",
      "Add of existing embedding ID: 242\n",
      "Add of existing embedding ID: 253\n",
      "Add of existing embedding ID: 255\n",
      "Add of existing embedding ID: 255\n",
      "Add of existing embedding ID: 255\n",
      "Add of existing embedding ID: 263\n",
      "Add of existing embedding ID: 279\n",
      "Add of existing embedding ID: 281\n",
      "Add of existing embedding ID: 281\n",
      "Add of existing embedding ID: 281\n",
      "Add of existing embedding ID: 286\n",
      "Add of existing embedding ID: 287\n",
      "Add of existing embedding ID: 287\n",
      "Add of existing embedding ID: 288\n",
      "Add of existing embedding ID: 323\n",
      "Add of existing embedding ID: 324\n",
      "Add of existing embedding ID: 327\n",
      "Add of existing embedding ID: 327\n",
      "Add of existing embedding ID: 328\n",
      "Add of existing embedding ID: 328\n",
      "Add of existing embedding ID: 330\n",
      "Add of existing embedding ID: 332\n",
      "Add of existing embedding ID: 337\n",
      "Add of existing embedding ID: 349\n",
      "Add of existing embedding ID: 350\n",
      "Add of existing embedding ID: 351\n",
      "Add of existing embedding ID: 352\n",
      "Add of existing embedding ID: 352\n",
      "Add of existing embedding ID: 353\n",
      "Add of existing embedding ID: 354\n",
      "Add of existing embedding ID: 354\n",
      "Add of existing embedding ID: 354\n",
      "Add of existing embedding ID: 354\n",
      "Add of existing embedding ID: 355\n",
      "Add of existing embedding ID: 355\n",
      "Add of existing embedding ID: 357\n",
      "Add of existing embedding ID: 358\n",
      "Add of existing embedding ID: 358\n",
      "Add of existing embedding ID: 360\n",
      "Add of existing embedding ID: 360\n",
      "Add of existing embedding ID: 360\n",
      "Add of existing embedding ID: 360\n",
      "Add of existing embedding ID: 361\n",
      "Add of existing embedding ID: 361\n",
      "Add of existing embedding ID: 361\n",
      "Add of existing embedding ID: 362\n",
      "Add of existing embedding ID: 363\n",
      "Add of existing embedding ID: 367\n",
      "Add of existing embedding ID: 369\n",
      "Add of existing embedding ID: 369\n",
      "Add of existing embedding ID: 370\n",
      "Add of existing embedding ID: 375\n",
      "Add of existing embedding ID: 376\n",
      "Add of existing embedding ID: 377\n",
      "Add of existing embedding ID: 377\n",
      "Add of existing embedding ID: 378\n",
      "Add of existing embedding ID: 378\n",
      "Add of existing embedding ID: 378\n",
      "Add of existing embedding ID: 378\n",
      "Add of existing embedding ID: 382\n",
      "Add of existing embedding ID: 393\n",
      "Add of existing embedding ID: 394\n",
      "Add of existing embedding ID: 395\n",
      "Add of existing embedding ID: 396\n",
      "Add of existing embedding ID: 397\n",
      "Add of existing embedding ID: 400\n",
      "Add of existing embedding ID: 400\n",
      "Add of existing embedding ID: 403\n",
      "Add of existing embedding ID: 406\n",
      "Add of existing embedding ID: 407\n",
      "Add of existing embedding ID: 408\n",
      "Add of existing embedding ID: 418\n",
      "Add of existing embedding ID: 427\n",
      "Add of existing embedding ID: 431\n",
      "Add of existing embedding ID: 431\n",
      "Add of existing embedding ID: 433\n",
      "Add of existing embedding ID: 435\n",
      "Add of existing embedding ID: 436\n",
      "Add of existing embedding ID: 440\n",
      "Add of existing embedding ID: 440\n",
      "Add of existing embedding ID: 454\n",
      "Add of existing embedding ID: 467\n",
      "Add of existing embedding ID: 489\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "blog_project.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
