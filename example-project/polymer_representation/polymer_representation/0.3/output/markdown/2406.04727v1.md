# Predicting Polymer Properties Based on Multimodal Multitask Pretraining

## 1 Introduction



As illustrated in Figure 1, polymers are constructed by bonding numerous identical or similar monomers based on various polymerization operations (e.g., addition, ring-opening, and condensation) [8]. In the past few decades, polymers have played a crucial role in many scientific fields, such as chemistry [7], material science [6], drug design [33], and bioinformatics [38]. In the applications mentioned above, achieving accurate prediction of polymer properties has garnered more and more attention due to its significance [3, 9]. For example, predicting polymer properties like plasticity and conductivity helps guide the design and development of polymer-based materials with specific functionalities [5]. Besides, predicting polymeric drug carriers' bio-compatibility and release kinetics is essential for designing effective and safe drugs [34].

Traditional studies on polymers rely on experiments or simulations to assess their properties [16, 10]. While these techniques yield precise outcomes, they suffer from poor scalability due to their high costs,

*Corresponding authors: Hongteng Xu (hongtengxu@ruc.edu.cn) and Zhifeng Gao (gaozf@dp.tech)

1



|Polymer|Monomer||Polymer|
|---|---|---|---|
||:unselected: :unselected: C=C|Polymerization|:unselected: :unselected: :unselected: :unselected: :unselected:|
||:unselected:|addition|:unselected: :unselected: :unselected: :unselected: :unselected: :unselected:|
|Monomer||Polymerization ring-opening|O M :unselected:|
|||||
||OH HO|Polymerization condensation||


1

] = repeating units

Figure 1: An illustration of monomer polymerization and typical polymerization operations.

extensive time requirements, and the need for specialized tools and knowledge. As a result, they fail to meet the rapidly increasing demands of polymer property prediction. Subsequently, although some learning-based methods [36, 11, 2, 46] have been proposed for polymer property prediction, their performance is often unsatisfactory due to the scarcity of high-quality polymer property data. Recently, some attempts have been made to mitigate the data insufficiency based on model pretraining techniques [53, 20]. However, they merely rely on the information from polymer SMILES sequences (i.e., P-SMILES strings) while ignoring the fact that the properties of a polymer are highly correlated with its 3D structure, which leads to sub-optimal performance as well.

To overcome the limitations of current polymer property prediction methods, we propose a novel multimodal multitask pretraining framework, MMPolymer, leveraging 3D structural information to enhance the predictive capabilities of polymer properties. As illustrated in Figure 2, MMPolymer leverages a two-tower architecture to represent polymer SMILES sequences (i.e., P-SMILES strings) and 3D conformations in the latent space respectively. Aligning latent representations of the two modalities further leads to semantically meaningful polymer representations for downstream polymer property prediction tasks. Notably, considering the regularity of polymer 3D structure and scarcity of polymer 3D data, we leverage the 3D conformation of the corresponding repeating unit 1 to approximate the whole polymer 3D conformation in our work. Specifically, we first convert the corresponding P-SMILES string through our "Star Substitution" strategy, where the "*" symbol in the P-SMILES string is replaced by the neighboring atom symbol of another "*" symbol. Then we use the RDKit [21] tool to generate the 3D conformation based on the converted P-SMILES string. In this way, the generated 3D conformation can not only reflect 3D structural features within repeating units but also reflect 3D structural features between the repeating units, thus contributing to extracting polymer 3D structural information effectively.

During pretraining, we mask the P-SMILES string randomly, add noise to the atom coordinates of corresponding 3D conformation, and pass them through the 1D and 3D representation networks, respectively. We learn the proposed representation networks based on three pretraining tasks, including predicting masked tokens, recovering clear 3D coordinates (i.e., coordinate denoising), and aligning the latent representations across the two modalities by contrastive learning [24]. Solving these three tasks jointly leads to the proposed multimodal multitask pretraining paradigm, which helps our model fully incorporate both polymer 1D sequential and 3D structural information. In the fine-tuning phase, we further fine-tune the pretrained MMPolymer in the supervised learning paradigm for downstream polymer property prediction tasks.

To the best of our knowledge, MMPolymer is the first work that incorporates 3D structural

1 Not equivalent to the monomer, which lacks polymerization site information.

2

P-SMILES String *CC(C1=CC=CC=C1)*

Masking Masked P-SMILES String *_ C(_1=C __ C _=_ 1 _*

Multimodal Multitask Pretraining

1D

Representation Network

Masking Prediction

Reconstructed P-SMILES String

*CC(C1=CC=CC=C1)*

Aligned Latent Representations

Downstream Prediction Task

Cross-modal Alignment

Polymer

3D

Conformation

Adding Noise

Noisy 3D Coordinates

3D

Representation Network

Fine-tuning

Coordinate Denosing

Reconstructed 3D Coordinates

:selected:

Figure 2: The scheme of the proposed method. Here, the red arrows indicate the pipeline of our multimodal multitask pretraining paradigm, and the blue arrows indicate the pipeline of fine-tuning steps for downstream polymer property prediction tasks. The blue modules are designed for 1D sequences (i.e., P-SMILES strings), and the red modules are designed for 3D conformations. The modules shared by 1D and 3D representations are labeled in green.

information into polymer property prediction. The experimental results on various polymer property datasets consistently demonstrate that MMPolymer significantly outperforms existing methods, achieving state-of-the-art performance. Moreover, even merely relying on single-modal information (either polymer 1D sequential information or 3D structural information) during fine-tuning, MMPolymer still outperforms existing methods, showcasing its promising capability of extracting and utilizing polymer features. Besides, comprehensive ablation studies offer valuable insights into the performance of our method, further supporting its rationality and effectiveness.

## 2 Related Work



## 2.1 Polymer Property Prediction



Polymer property prediction is one of the fundamental problems in polymer informatics, and various machine learning and deep learning algorithms have been widely used as promising tools for polymer property prediction [15], offering efficient alternatives to traditional experimental or simulational approaches [16, 10].

Early works [5, 54] in polymer property prediction employed classic machine learning algorithms (e.g., multiple linear regression) to predict corresponding polymer properties from polymer sequences. The work in [22] further applied kernel support vector machines (kernel SVMs) to deal with non-linear relationships between polymer sequences and their properties, achieving great predictive performance. For deep learning examples, the work in [35] predicted mechanical properties of polymer-carbon nanotube surfaces using convolutional neural networks (CNNs) while recurrent neural networks (RNNs) were also used in [40, 51] to learn the latent relationships between polymer sequences and their properties. However, all these prediction methods are significantly limited by the scarcity of high-quality polymer property data.

Recently, inspired by the exceptional performance of various pretrained models in natural language processing (NLP) tasks [28, 23, 58], some Transformer-based pretraining methods have been proposed for polymer property prediction. These methods like Transpolymer [53] and polyBERT [20] treat polymers as character sequences (i.e., P-SMILES strings) and undergo pretraining on extensive unlabeled polymer sequences, so they heavily rely on the information learned from polymer sequences. However, the latest studies [41, 59, 49] have revealed that properties are mainly determined by the 3D structure, thus highlighting the crucial need for a paradigm shift towards integrating 3D structural information into property prediction.

3

## 2.2 Pretraining for Molecular Modeling



A closely related domain to polymer science is molecular science [26, 13, 42], where numerous pretraining methods have been developed for molecular modeling. These methods have greatly contributed to improving the performance of various downstream tasks in molecular science, especially molecular property prediction.

SMILES-BERT [48] and ChemBERTa [1], were first proposed and achieved great predictive performance on various molecular property prediction tasks by undergoing pretraining on extensive molecular sequences (e.g., SMILES strings). Then, due to the rapid development of graph neural networks [37, 55, 52], subsequent works were further extended to molecular graphs. For example, MolCLR [50] was pretrained on 10 million molecular graphs by contrastive learning strategy, and the work in [4] pretrained graph neural networks by utilizing inherent properties of molecular graphs.

Recently, considering that intramolecular interactions are fundamentally three-dimensional [12, 45, 61], many works tend to incorporate molecular 3D structural information into pretraining to acquire more comprehensive molecular representation. For example, the work in [60] proposed a unified 2D and 3D pretraining framework for more informative representation, while 3D Infomax [41] proposed to maximize the mutual information between learned 3D representation and 2D representation during pretraining to improve the performance of molecular property prediction. Furthermore, several 3D pretraining methods [59, 49] have been proposed and demonstrated significant performance gains across various downstream tasks in molecular science, thus underscoring the critical role of 3D structural information.

## 3 Method



## 3.1 Overview



Given a polymer, we denote it as a tuple {S, C}. Here, S represents its SMILES sequence (i.e., P- SMILES string), and C = (A, P) represents the 3D conformation, where A = [ai] E NA is atom types, P = [pi] E RNx3 is atom 3D coordinates, and N is the number of atoms.

To achieve our goal that acquiring a comprehensive polymer representation for downstream polymer property prediction tasks, we pretrain MMPolymer through the multimodal multitask paradigm. As shown in Figure 2, the 1D representation network fld : S +> X' is pretrained via the masked prediction task. Meanwhile, to effectively incorporate 3D structural information, the 3D representation network f3d : C +> X is pretrained via the coordinate denoising task. Besides, we further align the latent representations across the two modalities via the cross-modal alignment task during pretraining. Here, S is the collection of P-SMILES strings, C is the collection of 3D conformations, and & C Rd is the latent space.

## 3.2 Model Architecture



## 3.2.1 1D Representation Network



The 1D representation network f 1d , based on Transformer architecture [44, 25], takes polymer sequences as input, and outputs the corresponding 1D sequential representation X1d, aiming to encode as much information as possible about polymer sequences.

Unlike small molecules, which can be easily represented by corresponding sequences (e.g., SMILES string), converting polymers to sequences is not straightforward due to their complex structures and compositions. Here, we use the P-SMILES string [17], a modified SMILES string for polymers, as input polymer sequences. Specifically, the P-SMILES string is formed by combining the SMILES string [32] of the corresponding monomer with two "*" symbols which are used to indicate the polymerization sites (i.e., the connected atoms between repeating units). For example, polyethylene is represented by "*CC*" and polypropylene is represented by "*CC(*)C". Besides, if other descriptors like the polymerization degree are also available, they can be added to the end of the corresponding P-SMILES string and fed into our 1D representation network together.

4

1D Representation Network

3D Representation Network

Token and Position Embedding

Token

Embedding

Attention-based Backbone

L-layer

- :selected:

Linear

Atom-level and Pair-level Embedding

Embedding

Attention-based Backbone

L-layer

Linear

:selected:

*CC(*)F-

Tokenization

Position

Embedding

Linear

Softmax

:selected: X

Linear

Gaussian Kernel

Linear

Softmax

:selected:

Linear

L-layer

Figure 3: Left: the architecture of our 1D representation network, which takes polymer SMILES sequences (i.e., P-SMILES strings) as input, and outputs corresponding 1D sequential representation. Right: the archi- tecture of our 3D representation network, which takes 3D conformation as input, and outputs corresponding 3D structural representation.

As shown in Figure 3, given a polymer sequence represented by the corresponding P-SMILES string "*CC(*)F", we first use chemical-aware tokenization from [53] to get corresponding tokens. Here, the P- SMILES string "*CC(*)F" is converted into ['*', 'C', 'C', '(', '*', ')', 'F'] along with special tokens (i.e., [CLS] and [SEP]). After tokenization, these tokens are further converted into continuous vector representations by adding each token's embedding to its corresponding position embedding. As a result, these vectorized tokens can be effectively interpreted and handled by the attention-based backbone of our 1D representation network. Specifically, segment embeddings are removed and we use the typical trigonometric function to acquire corresponding position embedding, i.e.,

Position_Embedding(pos, 2i) = sin 100002i/d ) pos

(1)

Position_Embedding(pos, 2i +1) = cos 100002i/d pos

-

(2)

where pos is the position in the token sequence, d is the total embedding dimension, and i is the index of the embedding dimension, ranging from 0 to 1 - 1.

Finally, the embedding vectors are fed into the attention-based backbone of our 1D representation network to get the corresponding 1D sequential representation X1d. Here we utilize the output [CLS] representation as the corresponding 1D sequential representation X1d for the input polymer sequence, i.e.,

X1d = Rep[CLS].

(3)

## 3.2.2 3D Representation Network



The 3D representation network f3d, based on SE(3)-Transformer architecture [14, 59], takes 3D conformation as input and outputs corresponding SE(3)-invariant representation vector as 3D structural representation X3d, aiming to encode comprehensive polymer 3D structural information.

"Star Substitution" strategy. Notably, due to the regularity of polymer 3D structure and scarcity of polymer 3D data, we approximate the whole polymer 3D conformation by using the 3D conformation of its corresponding repeating unit. Specifically, as shown in Figure 4, we first generated the converted P-SMILES string through our "Star Substitution" strategy, where the "*" symbol in the P-SMILES string is replaced by the neighboring atom symbol of another "*" symbol. For example, "*NCCCCCC(*)=O" is converted to "CNCCCCCC(N)=O" and "*Oc1ccc(CC(*)=O)cc1" is converted to "COc1ccc(CC(O)=O)cc1". Then we further generate the corresponding 3D conformation by RDKit [21]

5

*Oc1ccc(CC(*)=O)cc1

P-SMILES:

*NCCCCCC(*)=O

Same

Same

I

## * Substitution:



O

*

Same

Same

Converted P-SMILES:

CNCCCCCC(N)=O

COc1ccc(CC(O)=O)cc1

Figure 4: Visualization of our "Star Substitution" strategy.

based on the converted P-SMILES string. In this way, the generated 3D conformation can not only reflect 3D structural features within repeating units but also reflect 3D structural features between the repeating units, thus contributing to approximating the whole polymer 3D conformation as much as possible.

As illustrated in Figure 3, the 3D conformation C = (A, P) of the corresponding repeating unit is used as input of our 3D representation network, where A = [ai] E NA is atom types, P = [pi] E RNx3 is atom 3D coordinates and N is the number of atoms. Besides, the atom do is a virtual atom, analogous to the role of [CLS] in natural language processing tasks, and its 3D coordinate p0 is located at the space center of the whole 3D conformation.

Here, we first use an embedding layer to encode atom types A to the initial atom-level representation X ERNxda as follows:

X(0) = fa(A),

(4)

where fa embeds each atom type to a da-dimensional embedding.

Meanwhile, a pair-type aware Gaussian kernel [39] encodes the Euclidean distances of each atom pair (ai, aj) as the initial pair-level representation Xp E RNXNxdp, i.e.,

X(o) [i, j] = G (Va;a; ||Pi - Pill + Ma;a; - μ,σ).

(5)

where Vaja; E Rdp and uaia; E Rdp represent the weights and biases, which are learnable parameters for each atom pair (aj, aj), G is a Gaussian basis kernel function with mean value u E Rdp and standard deviation o E Rdp

Then, the initial representation X) and X(0) are fed into the attention-based backbone of our 3D representation network to get the final representation X, and X(-) :selected: after passing L encoder layers. To fully capture intricate 3D structure information, the multi-head self-attention mechanism is applied to the encoder layers, and the number of attention heads is determined by the dimension of pair-level embedding dp. Besides, to update atom-level and pair-level representation effectively, we use atom-to-pair communication to further improve the traditional self-attention mechanism. The specific implementation of the h-th attention head in the l-th encoder layer can be expressed as follows:

Atten(l,h)

(6)

Vdh Q(L,h)(K(1,h))T + X(1-1,h),

x(,h) = o(Atten(1,h))V(L,h), X(L,h) = Atten(1,h),

(7)

6

where Q(L,h) = x(-1)w(I,h), K(l,h) = x(1-1)w(lh), and V(I,h) = x(-1)w(n) are query, key, and value matrices generated by corresponding linear maps Way VK, WO) E Rdaxdr. Beisdes, X( -1,h) E RNXN is the h-th slice of X(1) E RNXNxdp, o(.) is the row-wise softmax operator, and dh = da/dp is the hidden dimension of each attention head.

By concatenating the outputs of each attention head, we can further get the outputs of the whole l-th layer as follows:

X() = Concat({ x(h) }dp ),

(8)

X = Concat ({ Xa .(l,h)1dp Sh=1.

20

a

,

(9)

where Concat(.) represents the concatenation operator, and W E Rdaxda represents the feed-forward neural network.

After L encoder layers, we finally get the 3D structural representation X3d from the final atom-level representation of the corresponding virtual atom do as follows:

X3d = X(L)[0, :].

(10)

## 3.3 Learning Paradigm



As shown in Figure 2, we pretrain our MMPolymer with a multimodal multitask paradigm, including the masked prediction task for pretraining the 1D representation network f 1d based on polymer 1D sequence data in Sec. 3.3.1, the coordinate denoising task for pretraining the 3D representation network f3d based on polymer 3D structure data in Sec. 3.3.2, and the cross-modal alignment task for aligning the learned latent representations across the two modalities in Sec. 3.3.3, to acquire a comprehensive polymer representation for downstream polymer property prediction tasks. The total loss during pretraining is expressed as follows:

Lpretrain = L1D + L3D + Lcontrastive.

(11)

After pretraining, we further fine-tune the pretrained MMPolymer on various polymer property datasets for the corresponding polymer property prediction tasks in the supervised learning paradigm, as described in Sec. 3.3.4.

## 3.3.1 Masked Prediction Task



During the masked prediction task (i.e., masked language modeling task in NLP), approximately 15% of tokens within a polymer sequence are randomly selected for masking. Subsequently, these chosen tokens are subjected to three possible replacement options: they may be replaced with a special token [MASK], a random token, or left unchanged. Then, the 1D representation network f 1d is trained to recover the original identity of these masked tokens based on the contextual information provided by the surrounding sequence. This task encourages our 1D representation network f 1d to capture the nuanced structures and interactions present in polymer sequences.

Specifically, V is the vocabulary set, M is the set of masked positions, Y = [y] E RIM|X|V| is the predicted probability distribution over these masked positions, and Y = [y] E RM|X|V| represents the corresponding label in the one-hot format. Here, we utilize the cross-entropy loss [57] as our loss function for the masked prediction task, i.e.,

L1D = M diEM DiEn Iyi[j] . log (y;[j]),

1

(12)

Through the above process, our 1D representation network f 1d acquires valuable insights into the complex patterns and relationships within polymer sequences, thereby greatly benefiting downstream polymer property prediction tasks.

7

## 3.3.2 Coordinate Denoising Task



During the coordinate denoising task, we first randomly add noise to the atom coordinates of the given 3D conformation C, which are combined by atom types A E NN and atom 3D coordinates P E RNx3. This process generates corrupted 3D conformation C with noisy 3D atom coordinates P, which serves as input of our 3D representation network f3d. Then the 3D representation network f3d is trained to recover the original 3D atom coordinates P from the corrupted 3D conformation C.

As described Sec. 3.2.2, the corrupted 3D conformation C is first converted to initial atom-level representation Xa E RNxda and pair-level representation Xp E RNXNxdp. Then Xa and Xp are fed into our attention-based backbone to get the final atom-level representation Xa- and pair-level representation X -through L encoder layer. Finally, a pair-level decoder is used to recover the original 3D atom coordinates based on the learned pair-level representation. Here, we utilize the smooth L1 loss [47] as our loss function for the coordinate denoising task, i.e.,

₱i = ₱i + N p,ij j=1 (oc (L) N i - pij) (Pi - Pj) ,

(13)

1 N 3 i=1 j=1 - Pi[j] - pi[j]| - 0.5, otherwise. (0.5|pi[j] - pi[j]|2, if |pi[j] - pi[j]| < 1,

L3D = N

where N is the number of atoms, pi E R3 is the noisy 3D coordinate of i-th atom, pi E R3 is the predicted 3D coordinate of i-th atom, v is an MLP layer that maps the update of pair-level representation to a scalar as weight, while 2(L). Cp,ij · E Rdp and Ip,ij :(0), E Rdp are final and initial pair-level representations of atom pair (i, j). Generally, the coordinate denoising task enables our 3D representation network f3d to accurately capture essential polymer 3D structural information, essential for downstream polymer property prediction tasks.

## 3.3.3 Cross-modal Alignment Task



In the pretraining phase, in addition to predicting masked tokens and recovering clear 3D coordinates, we also employ constructive learning to align 1D sequential representation X12 learned by 1D representation network fld and 3D structural representation X3d learned by 3D representation network f3d

Specifically, we derive the corresponding multimodal representations {Xid, X3af for a batch of K polymers. Our goal is to maximize the similarity between positive multimodal representation pairs from the same polymer and minimize the similarity between negative multimodal representation pairs from different polymers. Here, we utilize the classical InfoNCE loss [29] as our loss function for the cross-modal alignment task, i.e.,

1 K

Lcontrastive K i=1

log exp(sim(Xia, X3a) /T) Di-1 exp(sim(Xid, X3d) /T) ,

(14)

where K is the batch size, sim(.) is the cosine similarity function, and T is the temperature parameter.

Through the contrastive learning task during pretraining, the 1D sequential representation and 3D structural representation of polymers are aligned into a shared space, enhancing the coherence and mutual informativeness of these multimodal representations.

## 3.3.4 Fine-tuning



Benefiting from our multimodal multitask pretraining paradigm, MMPolymer can learn comprehensive polymer representation after pretraining. Subsequently, we further fine-tune the pretrained MMPolymer through a prediction head (i.e., a collection of multi-layer perceptions), to achieve accurate polymer property prediction in the supervised learning paradigm.

8

Table 1: The summary of our datasets, where the pretraining dataset is unlabeled and the fine-tuning datasets are regression-type datasets with corresponding property labels.



|Dataset|Property|Data Range|Data Size|
|---|---|---|---|
|PI1M|/|/|~1M|
|Egc|bandgap (chain)|[0.02, 8.30]|3380|
|Egb|bandgap (bulk)|[0.39, 10.05]|561|
|Eea|electron affinity|[0.39, 4.61]|368|
|Ei|ionization energy|[3.55, 9.61]|370|
|Xc|crystallization tendency|[0.13, 98.41]|432|
|EPS|dielectric constant|[2.61, 8.52]|382|
|Nc|refractive index|[1.48, 2.58]|382|
|Eat|atomization energy|[-6.83, -5.02]|390|


Moreover, although the pretraining phase is multi-modal, the fine-tuning phase allows for flexible modality information choice by using either a single modality representation (1D sequential representation X1d or 3D structural representation X3d) or combining both modalities to predict corresponding polymer properties. This choice depends on the characteristics of the property being predicted and the specific requirements of the polymer property prediction task at hand. Regardless of the chosen modality representation, the multimodal multitask pretraining paradigm has already equipped our MMPolymer with adequate general knowledge for achieving accurate polymer property prediction.

## 4 Experiments



In this section, to demonstrate the effectiveness of our proposed MMPolymer, we compare it with several state-of-the-art prediction methods on various polymer property datasets. Besides, comprehensive ablation studies are also conducted to offer valuable insights into the performance of our proposed method.

## 4.1 Experimental Setup



## 4.1.1 Dataset



We use the PI1M dataset [27], which contains about one million unlabeled polymer data, to pretrain our MMPolymer. Then, we employ eight open-source polymer property datasets (denoted as Egc, Egb, Eea, Ei, Xc, EPS, Nc, and Eat, respectively) provided in [56] as our fine-tuning datasets. These property datasets, obtained through density functional theory (DFT) calculations [30], encompass a broad range of typical polymer properties. Besides, they are the only publicly available polymer property datasets2 to date and have been extensively utilized in previous works [56, 53]. More details are presented in Table 1.

## 4.1.2 Baselines



To demonstrate the effectiveness of our method, several state-of-the-art methods have been compared, including SML [56], PLM [56], polyBERT [20] and Transpolymer [53], which are all pretraining-based methods for polymer property prediction. Methods lacking pretraining, like GP [19], have already been surpassed by current pretraining-based methods [53], hence we no longer include them in our comparisons. Besides, we also compare our method with four representative molecular pretraining methods, including ChemBERTa [1], MolCLR [50], 3D Infomax [41] and Uni-Mol [59], to reveal the differences between

2 Although some other polymer property datasets have been mentioned in previous works [19, 20], these datasets like PolyInfo database [31], are not publicly available, thus making them inaccessible for our experiments.

9

Table 2: The performance comparison of different methods on eight polymer property datasets, and the best result for each polymer property dataset has been bolded.



|Metric|Method|Egc|Egb|Eea|Ei|Xc|EPS|Nc|Eat|
|---|---|---|---|---|---|---|---|---|---|
|RMSE (1)|ChemBERTa [1]|0.539+0.049|0.664+0.079|0.350+0.036|0.4850.086|18.711+1.396|0.603+0.083|0.1400.010|0.219+0.056|
||MolCLR [50]|0.587+0.024|0.644+0.072|0.404+0.017|0.533+0.053|21.719+1.631|0.631+0.045|0.117+0.015|0.094+0.033|
||3D Infomax [41]|0.494+0.039|0.553+0.032|0.335+0.055|0.449+0.086|19.483 2.491|0.582+0.054|0.101+0.018|0.094+0.039|
||Uni-Mol [59]|0.489+0.028|0.531+0.055|0.332+0.027|0.407+0.080|17.414+1.581|0.5360.053|0.095+0.013|0.084+0.034|
||SML [56]|0.489+0.056|0.547+0.110|0.313+0.016|0.432+0.060|18.981+1.258|0.576+0.020|0.102+0.010|0.062+0.014|
||PLM [56]|0.459+0.036|0.528+0.081|0.322+0.037|0.444+0.062|19.181+1.308|0.576+0.060|0.1000.010|0.050+0.010|
||polyBERT [20]|0.553+0.011|0.759+0.042|0.363+0.037|0.526+0.068|18.437+0.560|0.618+0.049|0.113+0.003|0.172+0.016|
||Transpolymer [53]|0.453+0.007|0.576+0.021|0.326+0.040|0.397+0.061|17.740 0.732|0.547+0.051|0.0960.016|0.147+0.093|
||MMPolymer (ours)||0.431+0.017|0.496+0.031|0.286+0.029|0.390+0.057|16.814+0.867|0.511+0.035|0.087+0.010|0.061+0.016|
|R2 (1)|ChemBERTa [1]|0.8800.023|0.881+0.028|0.888+0.035|0.745+0.102|0.365+0.098|0.682+0.123|0.643+0.076|0.590+0.078|
||MolCLR [50]|0.858+0.010|0.882+0.027|0.854+0.038|0.689+0.037|0.176+0.026|0.683+0.020|0.764+0.037|0.885+0.104|
||3D Infomax [41]|0.900+0.016|0.898+0.018|0.891+0.045|0.766+0.086|0.274+0.122|0.690+0.063|0.797+0.086|0.869+0.097|
||Uni-Mol [59]|0.901+0.013|0.9250.011|0.901+0.027|0.820+0.075|0.454+0.079|0.751+0.085|0.828+0.072|0.937+0.032|
||SML [56]|0.901+0.022|0.920+0.029|0.915+0.015|0.802+0.051|0.3400.125|0.726+0.038|0.812+0.058|0.967+0.015|
||PLM [56]|0.911+0.014|0.925+0.021|0.9100.019|0.791+0.049|0.330+0.105|0.726+0.058|0.817+0.056|0.980+0.008|
||polyBERT [20]|0.875=0.006|0.844+0.034|0.880+0.035|0.705+0.085|0.384+0.066|0.681+0.058|0.769+0.034|0.672+0.119|
||Transpolymer [53]|0.916+0.002|0.911+0.008|0.902+0.036|0.830+0.059|0.430 0.058|0.744+0.075|0.826+0.071|0.800+0.172|
||MMPolymer (ours)||0.924+0.006|0.934+0.008|0.925+0.025|0.836+0.053|0.488+0.072|0.779+0.052|0.864+0.036|0.961+0.018|


small molecules and polymers. Besides, these baselines are implemented based on their default settings in the corresponding references.

## 4.1.3 Evaluation Metircs



Since polymer property prediction tasks on the fine-tuning datasets are all regression tasks, we also choose root mean squared error (RMSE) and R-squared (R2) as evaluation metrics based on the 5-fold cross- validation in line with previous works [53, 20, 56], thus guaranteeing a comprehensive and fair assessment of the predictive performance.

## 4.1.4 Model architecture and hyperparameter settings



Our 1D representation network consists of 6 encoding layers, with 12 attention heads each, whereas our 3D representation network comprises 15 encoding layers, with 64 attention heads each. Besides, our model is pretrained on the eight Tesla V100 GPUs using a batch size of 16 and further fine-tuned on the single Tesla V100 GPU using a batch size of 32. Meanwhile, the adam optimizer [18] is utilized with a learning rate of 1e-4, betas of (0.9, 0.99), and eps of 1e-6.

## 4.2 Comparisons



## 4.2.1 Demonstration of effectiveness



Table 2 presents the performance of different methods on the eight polymer property datasets, which are evaluated by RMSE and R2. In general, our proposed method, MMPolymer, stands out by achieving state-of-the-art performance on seven datasets and comparable performance on the remaining Eat dataset, underscoring its effectiveness on polymer property prediction tasks. Notably, several previous works, such as Transpolymer [53], have excluded the Eat dataset from their original paper because it fails to capture the complete polymer structure3 and therefore leads to lower reliability compared to other property datasets. Besides, even if considering the Eat dataset, the superiority of our MMPolymer is still obvious.

3 Eat is a localized property, which depends on the local atomic environment.

10

8

8

6

5

- 4

3

:selected: :selected:

:selected: :selected:

:selected:

4.5

:selected:

:selected: :selected:

- 4.0

:selected:

..

3.5

:selected:

:selected:

:selected:

:selected: :selected:

:selected:

2.0

· 2

~

1

1

1.5

- 1.0

5

0.5

(a) Egc

(b) Egb

(c) Eea

(d) Ei

-5.2

:selected:

-80

-7

2.4

-- 5.4

:selected:

:selected:

- 60

:selected:

40

20

:selected:

0

5

4

2.2

-5.6

-5.8

- 2.0

-6.0

- 1.8

- - 6.2

1.6

(e) Xc

(f) EPS

(g) Nc

(h) Eat

Figure 5: t-SNE visualization of MMPolymer on eight polymer property datasets, where the color is determined by the corresponding ground truth.

Moreover, the comparison with the four representative molecular pretraining methods reveals insightful trends. On the one hand, the 3D molecular pretraining method Uni-Mol consistently outperforms the 1D molecular pretraining method ChemBERTa across all eight polymer property datasets, emphasizing the crucial role of 3D structural information in enhancing predictive performance. On the other hand, all molecular pretraining methods exhibit inferior performance when compared with our MMPolymer and the best polymer pretraining baseline. These observations indicate the inherent limitations of directly applying molecular pretraining methods to polymer-specific tasks, thereby emphasizing the significance of tailored methods like our MMPolymer for achieving accurate polymer property prediction.

As shown in Figure 5, we conduct t-SNE visualization [43] based on the polymer representations learned by our MMPolymer and corresponding ground truth for each polymer property dataset. The t-SNE visualization illustrates that the chemical space defined by the representations accurately captures the differences in the ground truth for each data point, as reflected by the color gradations. These colors are determined by the corresponding ground truth, and the spread of colors across the datasets indicates a diverse range of values. This variation confirms that the representations learned by our MMPolymer are sensitive to the underlying ground truth differences among data points. The distinct clusters, especially in datasets like Egb, Eea, and Eat, where the color intensity marks out separate regions, serve as a testament to the fidelity with which the representations map out the actual property landscape of the dataset. The linear color variation further emphasizes that the representations are not only capturing distinct property values but are also able to reflect a continuum in the property and chemical space, showing the superiority of our MMPolymer.

## 4.2.2 Adaptability for various downstream settings



We also explore the predictive capacity of our MMPolymer when only utilizing single modality information (either polymer 1D sequential information or 3D structural information) during fine-tuning. As shown in Table 3, compared to the best baseline on each polymer property dataset, both MMPolymer-1D (i.e., only utilize polymer 1D sequential information during fine-tuning) and MMPolymer-3D (i.e., only utilize polymer 3D structural information during fine-tuning) consistently achieve lower RMSE and higher R2 on most datasets. This finding highlights the adaptability of our MMPolymer, demonstrating that it has acquired adequate general knowledge through our multimodal multitask pretraining paradigm, enabling MMPolymer

11

- 3.0

- 2.5

Table 3: The performance improvement of MMPolymer-1D, which only utilizes polymer 1D sequential information during fine-tuning, and MMPolymer-3D, which only utilizes 3D structural information during fine-tuning, compared with the best baseline on each polymer property dataset.



|Dataset|MMPolymer-1D||MMPolymer-3D||
|---|---|---|---|---|
||ARMSE (4)|AR2 (1)|| ARMSE (4)|AR2 (1)|
|Egc|-0.008|+0.003|-0.004|+0.001|
|Egb|-0.015|+0.005|-0.025|+0.007|
|Eea|-0.009|+0.002|-0.027|+0.010|
|Ei|+0.016|-0.011|-0.003|+0.004|
|Xc|-0.219|+0.010|+0.449|-0.028|
|EPS|-0.011|+0.016|-0.025|+0.028|
|Nc|-0.005|+0.024|-0.006|+0.025|
|Eat|+0.028|-0.046|+0.011|-0.019|


Table 4: The performance of MMPolymer under different data processing strategies, where the "Star Keep" strategy refers to directly using the original P-SMILES string, the "Star Remove" strategy refers to removing "*" symbols in the original P-SMILES string and the "Star Substitution" strategy refers to replacing the "*" symbol in the original P-SMILES string with the neighboring atom symbol of another "*" symbol.



|Metric|Strategy|Egc|Egb|Eea|Ei|Xc|EPS|Nc|Eat|
|---|---|---|---|---|---|---|---|---|---|
|RMSE (+)|Star Keep|0.433+0.015|0.506-0.055|0.310+0.022|0.392+0.047|16.836 1.272|0.514+0.047|0.089+0.011|0.077+0.042|
||Star Remove Star Substitution|0.478+0.023|0.527+0.012|0.299+0.016|0.403+0.041|16.962+0.803|0.514+0.032|0.091+0.011|0.083+0.040|
|||0.431+0.017|0.496+0.031|0.286+0.029|0.390+0.057|16.814+0.867|0.511+0.035|0.087+0.010|0.061+0.016|
|R2 (1)|Star Keep Star Remove|0.921+0.006
0.905=0.009|0.927+0.008
0.9250.013|0.914+0.022
0.921+0.015|0.833+0.049
0.828+0.043|0.4830.102 0.471+0.093|0.778+0.055
0.776+0.050|0.855+0.038
0.845=0.053|0.9450.041
0.938+0.040|
||Star Substitution|0.924+0.006|0.934+0.008|0.925+0.025|0.836+0.053|0.488+0.072|0.779+0.052|0.864+0.036|0.961+0.018|


to flexibly choose modality information based on the characteristics of the property being predicted and the specific requirements of the corresponding fine-tuning task.

## 4.3 Ablation Studies



## 4.3.1 Star Substitution Strategy



As mentioned in Sec. 3.2.2, considering the regularity of polymer 3D structure and scarcity of polymer 3D data, we apply the 3D conformation of its repeating unit (represented by the P-SMILES string) to approximate the whole polymer 3D conformation. Particularly, before generating this 3D conformation, we first convert the original P-SMILES string through our "Star Substitution" strategy, where the "*" symbol in the original P-SMILES string is replaced with the neighboring atom symbol of another "*" symbol, to reflect the structural features of the whole polymer 3D conformation as more as possible. Here, comprehensive ablation studies have been conducted to evaluate the effectiveness of our "Star Substitution" strategy.

Table 4 displays the performance of MMPolymer under different data processing strategies, including the default "Star keep" strategy (i.e., directly uses the original P-SMILES string), the "Star Remove" strategy (i.e., remove "*" symbols in the original P-SMILES string), and our "Star Substitution" strategy (i.e., replace the "*" symbol in the original P-SMILES string with the neighboring atom symbol of another "*" symbol). In general, compared with the default "Star keep" strategy and the "Star Remove" strategy, the performance of MMPolymer can be consistently improved on all eight polymer property datasets by using our "Star Substitution" strategy. This indicates that the 3D conformation of the repeating unit can capture more structural features of the whole polymer 3D conformation through our "Star Substitution" strategy, thus emphasizing the effectiveness of our "Star Substitution" strategy.

In particular, the performance of MMPolymer degrades when using the "Star Remove" strategy. We

12

Table 5: The performance of MMPolymer under different pretraining settings, where "1D-pre" refers to the masking prediction task on the 1D representation network, "3D-pre" refers to the coordinate denoising task on the 3D representation network, and "Contr" refers to the cross-modal alignment task.



|Metric|1D-pre||3D-preContr|Egc|Egb|Eea|Ei|Xc|EPS|Nc|Eat|
|---|---|---|---|---|---|---|---|---|---|---|---|
|RMSE (4)|×
:selected:|×
:selected:|×
:selected:|0.596+0.022|0.5750.031|0.343+0.029|0.432+0.062|20.152+1.624|0.569+0.053|0.102+0.010|0.122+0.051|
||✓
:selected:|×
:selected:|×
:selected:|0.438+0.010|0.510+0.034|0.311+0.028|0.394+0.058|16.818+0.779|0.5350.052|0.091+0.008|0.112+0.085|
||×
:selected:|✓
:selected:|×
:selected:|0.492+0.026|0.542+0.042|0.337+0.023|0.424+0.080|18.542+0.849|0.532+0.049|0.096+0.015|0.086+0.030|
||×
:selected:|×
:selected:|✓
:selected:|0.618+0.013|0.716+0.058|0.392+0.041|0.456+0.079|19.624+2.003|0.632+0.031|0.119+0.015|0.162+0.094|
||✓
:selected:|×
:selected:|✓
:selected:|0.459+0.010|0.519-0.044|0.298+0.032|0.417+0.048|17.033 0.482|0.545+0.062|0.092+0.012|0.124+0.073|
||×
:selected:|✓
:selected:|✓
:selected:|0.593+0.018|0.574+0.037|0.347+0.032|0.421+0.053|17.816 1.417|0.560+0.075|0.104+0.011|0.140+0.056|
||✓
:selected:|✓
:selected:|×
:selected:|0.4400.017|0.502+0.051|0.308+0.030|0.400+0.062|17.177+0.554|0.532+0.055|0.092+0.011|0.115+0.084|
||✓
:selected:|✓
:selected:|✓
:selected:|0.431+0.017|0.496+0.031|0.286+0.029|0.390+0.057|16.814+0.867|0.511+0.035|0.087+0.010|0.061+0.016|
|R2 (1)|×
:selected:|×
:selected:|×
:selected:|0.854+0.010|0.910+0.019|0.8950.025|0.799+0.064|0.269+0.090|0.725+0.079|0.811+0.040|0.873+0.082|
||✓
:selected:|×
:selected:|×
:selected:|0.921+0.005|0.931+0.003|0.914+0.020|0.833+0.054|0.488+0.061|0.757+0.065|0.847+0.050|0.878+0.137|
||×
:selected:|✓
:selected:|×
:selected:|0.900+0.012|0.921+0.013|0.897+0.029|0.803+0.079|0.378+0.077|0.758+0.069|0.824+0.080|0.937+0.026|
||×
:selected:|×
:selected:|✓
:selected:|0.843+0.002|0.862+0.022|0.862+0.037|0.773+0.090|0.303+0.125|0.6650.060|0.744+0.058|0.753+0.158|
||✓
:selected:|×
:selected:|✓
:selected:|0.913+0.004|0.928+0.006|0.919+0.024|0.816+0.047|0.474+0.067|0.741+0.094|0.837+0.066|0.865+0.107|
||×
:selected:|✓
:selected:|✓
:selected:|0.855+0.007|0.910+0.021|0.892+0.026|0.810+0.056|0.422+0.105|0.730+0.093|0.803+0.042|0.827+0.087|
||✓
:selected:|✓
:selected:|×
:selected:|0.921+0.005|0.933+0.009|0.915+0.020|0.827+0.060|0.466+0.060|0.7580.072|0.847+0.034|0.871+0.135|
||✓
:selected:|✓
:selected:|✓
:selected:|0.924+0.006|0.934+0.008|0.925+0.025|0.836+0.053|0.488+0.072|0.779+0.052|0.864+0.036|0.961+0.018|


## 4.3.2 Multimodal Multitask Pretraining Paradigm



As mentioned in Sec. 3.3, MMPolymer is pretrained through our multimodal multitask paradigm, including the masked prediction task for pretraining the 1D representation network, the coordinate denoising task for pretraining the 3D representation network, and the cross-modal alignment task for aligning the learned multimodal representation. Here, to evaluate the effectiveness of our multimodal multitask pretraining paradigm, assessing the specific impact of 1D pretraining (i.e., the masked prediction task), 3D pretraining (i.e., the coordinate denoising task), and contrastive learning (i.e., the cross-modal alignment task) on the model performance, comprehensive ablation studies have been conducted.

Table 5 displays the performance of MMPolymer under different pretraining settings, thus providing valuable insights into our proposed multimodal multitask pretraining paradigm. In general, when all three pretraining tasks (i.e., masked prediction task, coordinate denoising task, and cross-modal alignment task) are combined, MMPolymer achieves its optimal performance. This underscores the synergistic effect of our multimodal multitask pretraining paradigm, resulting in a robust and powerful model capable of capturing diverse aspects of polymer data.

Specifically, we observe that incorporating 3D structural information greatly enhances the performance of MMPolymer, especially when combining both 1D sequential and 3D structural information, showcasing the critical role of 3D structural information in polymer property prediction. Moreover, the utilization of contrastive learning further enhances the performance of MMPolymer. This suggests that aligning learned representations across different modalities contributes to better multimodal integration, thus improving the predictive performance of polymer properties. Notably, we aim to leverage complementary 1D and 3D

13

information for polymer property prediction, rather than matching the two modalities. In this case, purely applying the contrastive learning task is insufficient and leads to unsatisfactory performance.

Besides, we observe that the performance of MMPolymer with only 1D pretraining surpasses that with 3D pretraining on most datasets. We attribute this phenomenon to the fact that we approximate the whole polymer 3D conformation with the 3D conformation of its corresponding repeating unit. Although through our "Star Substitution" strategy, as analyzed in Sec. 4.3.1, the 3D conformation of the repeating unit can reflect the structural features of the whole polymer 3D conformation to some extent, it is still not accurate enough to describe all natures. This observation highlights the current issue of insufficient high-precision polymer 3D structure data and indicates that pure 3D pretraining methods still face significant challenges in the polymer domain, further emphasizing the benefits of our multimodal multitask pretraining paradigm.

## 5 Conclusions



In this work, we present MMPolymer, a multimodal multitask pretraining framework, to achieve accurate polymer property prediction. By effectively combining polymer 1D sequential and 3D structural information through our multimodal multitask pretraining paradigm, MMPolymer can fully capture diverse aspects of polymer data, creating a robust and promising model for the downstream property prediction tasks. Besides, through our "Star Substitution" strategy, the 3D structural information can be extracted effectively, overcoming the scarcity of polymer 3D data to some extent. The extensive experiments demonstrate that MMPolymer achieves state-of-the-art performance on various polymer property prediction tasks, significantly outperforming existing methods. Even if a single modality (P-SMILES string or 3D conformation) is utilized during fine-tuning, the pretrained MMPolymer can still surpass existing methods, showcasing its exceptional capability in polymer feature extraction and utilization. Moreover, comprehensive ablation studies are also conducted, providing valuable insights into the rationality of our method. As polymer research continues to advance, our proposed MMPolymer provides a valuable strategy for accurate polymer property prediction, paving the way for further developments in this field. For future work, we will continue to explore the intrinsic relationship between polymer structures and their properties, achieving better modeling of polymer structures to benefit its many downstream applications.

## References



[1] Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta- 2: Towards chemical foundation models. arXiv preprint arXiv: 2209.01712, 2022.

[2] Yoshifumi Amamoto. Data-driven approaches for structure-property relationships in polymer science for prediction and understanding. Polymer Journal, 54(8):957-967, 2022.

[3] Debra J Audus and Juan J de Pablo. Polymer informatics: Opportunities and challenges. ACS macro letters, 6(10):1078-1082, 2017.

[4] Roy Benjamin, Uriel Singer, and Kira Radinsky. Graph neural networks pretraining through inherent supervision for molecular property prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 2903-2912, 2022.

[5] Jozef Bicerano. Prediction of polymer properties. cRc Press, 2002.

[6] Christopher S Brazel and Stephen L Rosen. Fundamental principles of polymeric materials. John Wiley & Sons, 2012.

[7] Charles E Carraher Jr. Introduction to polymer chemistry. CRC press, 2017.

[8] Morgan M Cencer, Jeffrey S Moore, and Rajeev S Assary. Machine learning for polymeric materials: an introduction. Polymer International, 71(5):537-542, 2022.

14

[9] Lihua Chen, Ghanshyam Pilania, Rohit Batra, Tran Doan Huan, Chiho Kim, Christopher Kuenneth, and Rampi Ramprasad. Polymer informatics: Current status and critical next steps. Materials Science and Engineering: R: Reports, 144:100595, 2021.

[10] Scott PO Danielsen, Haley K Beech, Shu Wang, Bassil M El-Zaatari, Xiaodi Wang, Liel Sapir, Tetsu Ouchi, Zi Wang, Patricia N Johnson, Yixin Hu, et al. Molecular characterization of polymer networks. Chemical reviews, 121(8):5042-5092, 2021.

[11] Huan Doan Tran, Chiho Kim, Lihua Chen, Anand Chandrasekaran, Rohit Batra, Shruti Venkatram, Deepak Kamal, Jordan P Lightstone, Rishi Gurnani, Pranav Shetty, et al. Machine-learning predictions of polymer properties with polymer genome. Journal of Applied Physics, 128(17), 2020.

[12] Xiaomin Fang, Lihang Liu, Jieqiong Lei, Donglong He, Shanzhuo Zhang, Jingbo Zhou, Fan Wang, Hua Wu, and Haifeng Wang. Geometry-enhanced molecular representation learning for property prediction. Nature Machine Intelligence, 4(2):127-134, 2022.

[13] Jinjia Feng, Zhen Wang, Yaliang Li, Bolin Ding, Zhewei Wei, and Hongteng Xu. Mgmae: Molecular representation learning by reconstructing heterogeneous graphs with a high mask ratio. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 509-519, 2022.

[14] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in neural information processing systems, 33:1970-1981, 2020.

[15] Kan Hatakeyama-Sato. Recent advances and challenges in experiment-oriented polymer informatics. Polymer Journal, 55(2):117-131, 2023.

[16] Tran Doan Huan and Rampi Ramprasad. Polymer structure prediction from first principles. The Journal of Physical Chemistry Letters, 11(15):5823-5829, 2020.

[17] Chiho Kim, Anand Chandrasekaran, Tran Doan Huan, Deya Das, and Rampi Ramprasad. Polymer genome: a data-powered polymer informatics platform for property predictions. The Journal of Physical Chemistry C, 122(31):17575-17585, 2018.

[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

[19] Christopher Kuenneth, Arunkumar Chitteth Rajan, Huan Tran, Lihua Chen, Chiho Kim, and Rampi Ramprasad. Polymer informatics with multi-task learning. Patterns, 2(4), 2021.

[20] Christopher Kuenneth and Rampi Ramprasad. polybert: a chemical language model to enable fully machine-driven ultrafast polymer informatics. Nature Communications, 14(1):4099, 2023.

[21] Greg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8:31, 2013.

[22] Tu Le, V Chandana Epa, Frank R Burden, and David A Winkler. Quantitative structure-property relationship modeling of diverse materials properties. Chemical reviews, 112(5):2889-2919, 2012.

[23] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.

[24] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self- supervised learning: Generative or contrastive. IEEE transactions on knowledge and data engineering, 35(1):857-876, 2021.

15

[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv: 1907.11692, 2019.

[26] Changsheng Ma, Qiang Yang, Xin Gao, and Xiangliang Zhang. Disentangled molecular graph generation via an invertible flow model. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 1420-1429, 2022.

[27] Ruimin Ma and Tengfei Luo. Pi1m: a benchmark database for polymer informatics. Journal of Chemical Information and Modeling, 60(10):4684-4690, 2020.

[28] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: A survey. ACM Computing Surveys, 56(2):1-40, 2023.

[29] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv: 1807.03748, 2018.

[30] Maylis Orio, Dimitrios A Pantazis, and Frank Neese. Density functional theory. Photosynthesis research, 102:443-453, 2009.

[31] Shingo Otsuka, Isao Kuwajima, Junko Hosoya, Yibin Xu, and Masayoshi Yamazaki. Polyinfo: Polymer database for polymeric materials design. In 2011 International Conference on Emerging Intelligent Data and Web Technologies, pages 22-29. IEEE, 2011.

[32] Noel M O'Boyle. Towards a universal smiles representation-a standard method to generate canonical smiles based on the inchi. Journal of cheminformatics, 4:1-14, 2012.

[33] G Pasut and FM Veronese. Polymer-drug conjugation, recent achievements and general strategies. Progress in polymer science, 32(8-9):933-961, 2007.

[34] Li Yan Qiu and You Han Bae. Polymer architecture and drug delivery. Pharmaceutical research, 23:1-30, 2006.

[35] Aowabin Rahman, Prathamesh Deshpande, Matthew S Radue, Gregory M Odegard, S Gowtham, Susanta Ghosh, and Ashley D Spear. A machine learning framework for predicting the shear strength of carbon nanotube-polymer interfaces based on molecular dynamics simulation data. Composites Science and Technology, 207:108627, 2021.

[36] Nilay K Roy, Walter D Potter, and David P Landau. Polymer property prediction and optimization using neural networks. IEEE Trans. Neural Networks, 17(4):1001-1014, 2006.

[37] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323-9332. PMLR, 2021.

[38] D Schaffert and E Wagner. Gene therapy progress and prospects: synthetic polymer-based systems. Gene therapy, 15(16):1131-1138, 2008.

[39] Bernhard Scholkopf, Kah-Kay Sung, Christopher JC Burges, Federico Girosi, Partha Niyogi, Tomaso Poggio, and Vladimir Vapnik. Comparing support vector machines with gaussian kernels to radial basis function classifiers. IEEE transactions on Signal Processing, 45(11):2758-2765, 1997.

[40] Lena Simine, Thomas C Allen, and Peter J Rossky. Predicting optical spectra for optoelectronic polymers using coarse-grained models and recurrent neural networks. Proceedings of the National Academy of Sciences, 117(25):13945-13948, 2020.

16

[41] Hannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, and Pietro Liò. 3d infomax improves gnns for molecular property prediction. In International Conference on Machine Learning, pages 20479-20502. PMLR, 2022.

[42] Yuancheng Sun, Yimeng Chen, Weizhi Ma, Wenhao Huang, Kang Liu, Zhiming Ma, Wei-Ying Ma, and Yanyan Lan. Pemp: Leveraging physics properties to enhance molecular property prediction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 3505-3513, 2022.

[43] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.

[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[45] Fanmeng Wang, Hongteng Xu, Xi Chen, Shuqi Lu, Yuqing Deng, and Wenbing Huang. Mperformer: An se (3) transformer-based molecular perceptron. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 2512-2522, 2023.

[46] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, 2023.

[47] Qi Wang, Yue Ma, Kun Zhao, and Yingjie Tian. A comprehensive survey of loss functions in machine learning. Annals of Data Science, pages 1-26, 2020.

[48] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, pages 429-436, 2019.

[49] Xu Wang, Huan Zhao, Wei-wei Tu, and Quanming Yao. Automated 3d pre-training for molecular property prediction. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2419-2430, 2023.

[50] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 4(3):279-287, 2022.

[51] Michael A Webb, Nicholas E Jackson, Phwey S Gil, and Juan J de Pablo. Targeted sequence design within the coarse-grained polymer genome. Science advances, 6(43):eabc6216, 2020.

[52] Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, and Xiaojie Guo. Graph neural networks: foundation, frontiers and applications. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4840-4841, 2022.[53] Changwen Xu, Yuyang Wang, and Amir Barati Farimani. Transpolymer: a transformer-based language model for polymer property predictions. npj Computational Materials, 9(1):64, 2023.

[54] Xinliang Yu, Xueye Wang, Xiaobing Li, Jinwei Gao, and Hanlu Wang. Prediction of glass transition temperatures for polystyrenes by a four-descriptors qspr model. Macromolecular theory and simulations, 15(1):94-99, 2006.

[55] Jinliang Yuan, Hualei Yu, Meng Cao, Ming Xu, Junyuan Xie, and Chongjun Wang. Semi-supervised and self-supervised classification with multi-view graph neural networks. In Proceedings of the 30th ACM international conference on information & knowledge management, pages 2466-2476, 2021.

17

[56] Pei Zhang, Logan Kearney, Debsindhu Bhowmik, Zachary Fox, Amit K Naskar, and John Gounley. Transferring a molecular foundation model for polymer property predictions. Journal of Chemical Information and Modeling, 63(24):7689-7698, 2023.

[57] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. Advances in neural information processing systems, 31, 2018.

[58] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.

[59] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In The Eleventh International Conference on Learning Representations, 2023.

[60] Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. Unified 2d and 3d pre-training of molecular representations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2626-2636, 2022.

[61] Yanqiao Zhu, Jeehyun Hwang, Keir Adams, Zhen Liu, Bozhao Nan, Brock Stenfors, Yuanqi Du, Jatin Chauhan, Olaf Wiest, Olexandr Isayev, et al. Learning over molecular conformer ensembles: Datasets and benchmarks. In The Twelfth International Conference on Learning Representations, 2023.

18