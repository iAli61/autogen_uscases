# Interpretable Machine Learning Strategies for Accurate Prediction of Thermal Conductivity in Polymeric Systems

## I. INTRODUCTION



Polymers play a crucial role in today's world, finding uses in advanced areas like implantable brain-computer interfaces, electronic chips, and wearable technologies.[1] Polymers characterized by elevated thermal conductivity (TC) are instrumental in augmenting the heat dissipa- tion capacity of devices, thereby mitigating the potential adverse impacts of overheating on device functionality or user comfort[2]. Conversely, polymers exhibiting re- duced thermal conductivity harness exceptional thermal insulation attributes, finding extensive utilization in ther- mal insulation applications, such as within construction sector walls and thermal management systems for elec- tronic devices, aiming to diminish heat loss and enhance energy efficiency. The identification of polymers with specific thermal conductivities represents a noteworthy endeavor.

arXiv:2403.20021v2 [physics.app-ph] 1 Apr 2024

However, the current dominant approaches for screen- ing polymers with specific thermal conductivities are molecular dynamics (MD) calculations[3] or experimen- tal measurements[1]. Polymeric systems, characterized by their extensive size and substantial atomic count, ren- der the application of MD methodologies for the calcu- lation of TC inefficient. Experimentally ascertaining the TC of polymers necessitates intricate procedures, such as meticulous sample preparation and rigorous regulation of environmental variables, thereby imposing stringent de- mands on the precision of experimental methodologies. [5]

* corresponding author: 202103170412@zjut.edu.cn

Such methodologies demand substantial temporal invest- ments, extending from days to weeks, to deduce the TC features of complex polymers, often yielding results with error margins that may be deemed unsatisfactory. To efficiently sift through a diverse array of materials for particular thermal conductivities, an urgent requirement emerges for a rapid and precise technique to forecast the TC of polymers.

In recent years, machine learning (ML) has wit- nessed a significant surge in its application, demonstrat- ing remarkable success in achieving high levels of ac- curacy in forecasting outcomes such as carbon dioxide emissions[6] and properties of organic solar cells[7]. Pre- vious studies [8] have leveraged a ML paradigm to es- timate the TC of materials. They compiled a dataset comprising 469 amorphous polymers, converting the polymer-Simplified Molecular Input Line Entry System (p-SMILES) into 300-dimensional continuous value vec- tors, which yielded a prediction accuracy with a coeffi- cient of determination (R2) of 0.828. However, the com- plexity of the input features and the employment of 300- dimensional vectors, derived through linguistic process- ing devoid of physical significance, resulted in predictive performance that did not meet expectations.

We decode the p-SMILES of polymers into 10 features imbued with physical significance, thereby shrinking the feature space by a factor of 30 relative to the previous methodology[7]. Our approach entails the construction of a ML model predicated on Gradient Boosting Decision Trees (GBDT) for the estimation of polymers' TC, culmi- nating in an enhanced model accuracy with a coefficient of determination (R2) of 0.93.

Furthermore, the features integrated into our model are intrinsically interpretable, laying the groundwork for interpretable analyses of the predictive framework. We have elucidated a series of characteristic polymer at- tributes, such as the number of rotatable bonds and the quantitative estimate of drug-likeness, that significantly influence the thermal conductivity of polymers. Further- more, we have delineated the physical mechanisms un- derpinning the associations between these attributes and thermal conductivity. The ML methodology delineated in this study introduces a conceptual framework for the modeling of polymers, capable of predicting not merely the TC but encompassing all pertinent properties of the polymer under investigation.

## II. METHODS



We employed the third-party libraries RadonPy[9] and RDKit[11], sourced from GitHub, as our datasets. RadonPy comprises the Simplified Molecular Input Line Entry System (SMILES)[12] representations for 1077 polymers alongside TC data computed through MD methodologies. The SMILES notation for polymers typi- cally encapsulates recurring monomeric units, contingent upon the structure and composition of monomers within the polymer. An exemplar is illustrated in Figure 1.

polymerization point 1

*

'O

Polymer monomer

polymerization point 2

*

HN

O

SMILES

polymerization point 1

polymerization point 2

:unselected:

*C(=O)Nc1ccc(Oc2ccc(N3C(=O)c4ccc(*)cc4(*)cc4C3=O)cc2)cc1

## FIG. 1. Illustration of a SMILES notation expressed as a string for depicting the molecular architecture of a polymer.



In this study, we opted to train our model using the initial 400 polymers from the dataset. The Molecu- larDescriptors module within the RDKit library facil- itated the extraction of characteristic parameters im- bued with physical significance for each polymer. These parameters were derived from the decoding of polymer SMILES into 10 eigenvectors (eg., Number of Rotatable Bonds[13]), thereby constituting a 10-dimensional fea- ture space. The determination of TC in polymers con- ventionally necessitates weeks or even months of experi- mental measurements or alternatively, days of MD sim- ulation calculations[14]. Consequently, to streamline the screening process for polymers possessing targeted TC, we opted to devise a regression model to delineate the

2

relationship between SMILES representations and TC. This approach enables the accurate prediction of poly- mer TC within a significantly shorter timeframe.

We utilized scikit-learn [15], a Python library renowned for its capabilities in ML, to conduct training on Multi- layer Perceptron (MLP)[16], Random Forest (RF)[17], and GBDT[18] models. Additionally, for training eX- treme Gradient Boosting (XGBoost) [19] model, we em- ployed the Python library xgboost [20]. Following rigorous experimentation, we identified the GBDT model as yield- ing the most favorable training results among the four models examined. Hyperparameter tuning facilitated the optimization of key parameters, with the Number of trees set to 300, Maximum depth of each tree set to 5, Mini- mum number of samples required to split a node set to 4, Minimum number of samples required at each leaf node set to 1, Learning rate set to 0.01, and Subsample ratio set to 0.9.

We operate under the assumption that the dataset ob- tained is accurate and that the TC computed through MD simulation represents the authentic TC of the poly- mer. Moreover, given that predictions of polymer TC are based solely on monomer information, it is presupposed that elements such as the degree of polymerization, tem- perature, and the spatial configuration of the polymer are considered ancillary influences on polymer TC.

## III. RESULTS AND DISCUSSION



In the present investigation, we transmute the SMILES notation into a ten-dimensional attribute sphere. The nomenclature for each feature within this multidimen- sional expanse, as delineated by the RDKit computa- tional library, is cataloged in the inaugural column of Ta- ble I. Additionally, Table I elucidates the physical signif- icances and metrications of these features. For succinct- ness, the abbreviations denoting the physical properties of these polymers, as presented in the secondary col- umn of Table I, will henceforth represent these features. The selected features encompass: the molecular weight's mean value (MWT), the Quantitative Estimate of Drug- likeness (QED), the molecule's valence electron count (NVE), the computation of Balaban's J metric (BBJ), the molecule's total surface area (TPS), the tally of Hy- drogen Bond Acceptors (NHA), the count of Rotatable Bonds (NRB), the Wildman-Crippen LogP valuation (MLP), the Wildman-Crippen MR valuation (MMR), and the enumeration of halogen elements (FHA).

To evaluate the comprehensive distribution of TC among the polymers encompassed in our dataset, Fig- ure 2a was constructed. This figure, employing a ker- nel density estimation technique, delineates TC on the x-axis, with values spanning approximately from 0.06 to 0.7 W·m-1.K-1, while the y-axis quantifies the den- sity of the numerical simulation. The prominence of the

3

TABLE I. Ten features from SMILES for training applications



|Feature (in RDKit)|Abbreviation|Physical meaning|Unit|
|---|---|---|---|
|MolWt|MWT|The average molecular weight of the molecule|amu|
|qed|QED|Quantitative Estimate of Drug-likeness|\|
|Num ValenceElectrons|NVE|The number of valence electrons the molecule has|\|
|BalabanJ|BBJ|Calculate Balaban's J value for a molecule|\|
|TPSA|TPS|The total surface area of a molecule|Å2|
|NumHAcceptors|NHA|Number of Hydrogen Bond Acceptors|\|
|NumRotatableBonds|NRB|Number of Rotatable Bonds|\|
|MolLogP|MLP|Wildman-Crippen LogP value|\|
|MolMR|MMR|Wildman-Crippen MR value|cm3/mol|
|fr_halogen|FHA|Number of halogens|\|


(a)

Kernel Density

(b)

MWT 1 -0.7

1 -0.73

0.83

0.83 0.46

0.85

0.96 0.42

8

QED -0.73

1 -0.73

0.49

-0.57

-0.59

-0.48

-0.68 -0.72

-0.3

NVE

1

-0.7:

1 -0.72

0.83

0.82

0.5

0.86 0.98

0.4

1.0

0.8

6

BB -0.73

Density

4

0.49

-0.72

-0.67

-0.66

-0.2

-0.6 -0.79

0.04

0.6

TPS

0.83

-0.5

0.83 -0.6

0.4

1

0.94 0.35

0.49

0.81

0.17

0.2

NHA 0.83

-0.5

0.82 -0.66

0.0

0.94

1 0.35

0.51

0.81 0.17

-0.2

NRB 0.46

-0.48

0.5 -0.2

0.35

0.35

1

0.6

0.5 0.02

-0.4

-0.6

MLP 0.85

2

0

0.1 0.2

0.3 0.4

0.5

0.6

Thermal Conductivity (W · m-1 . K-1)

-0.64

0.86

-0.6

0.51 0.49

0.6

0.87 0.32

MMR

0.98

-0.7

0.98 -0.79

0.81

0.8

0.5

0.87

1 0.24

FHA 0.42

-0.3

0.4 0.04

0.17

0.17

0.02

0.3

0.24

1

MWT

QED

NVE

BBJ

TPS

NHA

NRB

MLP

MMR

FHA

FIG. 2. (a) Kernel density distribution of polymer TC within the dataset; (b) Heatmap depicting pearson correlation coefficients among ten distinct features.

blue curve at any locus within Figure 2a signifies the ag- gregation of data points proximal to that specific value of TC. Manifesting a broadly symmetrical bell-shaped curve, and with the dataset affirming conformity to a nor- mal distribution as evidenced by the Shapiro-Wilk test, it is posited that our training specimens are normally distributed.

To mitigate the inclusion of superfluous data ensuing from highly correlated features and to preclude ineffi- ciencies during model training, Figure 2b was devised. This figure illustrates the Pearson correlation coefficients encapsulated within each square, quantifying the inter- relation between pairs of features. The intensity of each square's hue signifies the correlation level between the corresponding features, with darker shades indicating higher correlation (dark red for positive, dark blue for negative) and lighter shades denoting weaker correlation. A pronounced correlation is notably observed between

MWT and MMR, as well as NVE and MMR, each regis- tering a coefficient of 0.98. Despite this, a deeper exami- nation reveals that MMR, MWT, and NVE encapsulate distinct physical properties. To ensure no pertinent in- formation is overlooked, we opted to retain both MWT and NVE within our feature set. Our analysis of the ten- dimensional feature space reveals a scarcity of highly cor- related features, with the majority displaying negligible correlation. This indicates the selected features possess intrinsic value, underscoring the dataset's overall ratio- nality.

We employed pairwise relationship plots to elucidate the bivariate relationships among features, as well as the distribution of individual features within the multivari- ate dataset, as depicted in Figure 3. For illustrative pur- poses, only the first 150 data points from the dataset were selected for plotting. The ten plots residing on the diagonal of this scatterplot matrix represent kernel

4



|500 MWT 0||||||||:selected:||.|
|---|---|---|---|---|---|---|---|---|---|---|
|0.75 QED 0.50 0.25|||||||||||
|200 NVE 0|||||||:selected:|:selected:|:selected:||
|4 BBJ 2|||||||||||
|50 TPS 0||.||||:selected:|||||
|2.5 NHA 0.0||. . ..... ..|||||||||
|20 NRB C||||||||:selected:|||
|10 MLP C|:selected:||:selected:|||||||.|
|100 MMR 0|:selected:||:selected:|||||||.|
|10 FHA 0|||||||||||
||0 250 500|0.25 0.50 0.75|0 100 200|2 4|0 25 50|0 2|0 10 20|0 5 10|0 100|0 10|
||MWT|QED|NVE|BB.|TPS|NHA|NRB|MLP|MMR|FHA|


density estimation plots for single features, delineating the distribution of each feature in isolation. Each blue point within Figure 3 symbolizes a polymer sample. Off- diagonal grids showcase small plots that elucidate the relationship between features labeled on the rows and those on the columns. For instance, the second plot in the first row elucidates the relationship between the MWT and QED features. This scatterplot matrix fea- tures a scatterplot in its lower left quadrant and a con- tour plot in the upper right, with contours illustrating the data's sparsity. The dataset contains a minimal number of anomalous samples, which were substituted with other normal samples from the dataset. At this juncture, the exploratory data analysis phase preceding ML modeling has been concluded.

We first normalized the dataset, which comprises feature spaces and TC of 400 polymers, yielding a novel dataset encompassing 4400 data points. Sub- sequently, this dataset was subjected to training em- ploying four distinct ML models: MLP, RF, GBDT, and XGBoost. As delineated in the METHODS sec- tion, the training outcomes of these models are signifi- cantly influenced by their hyperparameters, which con- sequently affect the accuracy of TC predictions. We en- gaged in the selection of six hyperparameters for the GBDT model, subjecting it to training across a spec- trum of hyperparameter values. These hyperparameters include the number of weak learners (n_estimators), the maximum depth of the tree (max_depth), the min- imum number of samples required to split a node (min_samples_split), the minimum number of samples required at a leaf node (min_samples_leaf), the learning rate (learning_rate), and the subsample ratio of the

training instance (subsample).

To mitigate variability in training outcomes at- tributable to differing data partitioning approaches, thereby enhancing the stability and reliability of our es- timations, we implemented 10-fold cross-validation dur- ing the hyperparameter optimization process. Prelimi- nary training sessions revealed a propensity for overfit- ting within the model. To augment the model's general- ization capability, introduce greater stochasticity, dimin- ish its sensitivity to noise in the training data, and thus counteract overfitting, we judiciously decreased the num- ber of trees and the maximum tree depth while increasing the minimum number of samples required for both node splitting and leaf nodes. Throughout the training phase, a grid search was employed to meticulously explore the hyperparameter space for all six hyperparameters, with the objective of identifying the most efficacious hyperpa- rameter combination. For the MLP model, seven hyper- parameters were optimized, whereas the RF model's op- timization involved four hyperparameters, and the XG- Boost model was optimized across six hyperparameters. The outcomes of hyperparameter optimization for these models are systematically cataloged in Table II.

Figure 4 delineates the juxtaposition of the Predicted versus Ground Truth values across four ML models post hyperparameter optimization. The abscissa represents the true TC values of polymers, ascertained via MD sim- ulations, while the ordinate corresponds to the models' TC predictions. The dataset was partitioned into a test subset, depicted by orange dots (10%), and a training subset, illustrated by purple dots (90%). Each subplot's header enumerates the metrics R2, MAE, and RMSE,

5

TABLE II. Hyperparameter optimization results for different models



|MLP|Value|GBDT|Value|XGBoost|Value|RF|Value|
|---|---|---|---|---|---|---|---|
|hidden_layer_sizes|(50, 50)|n_estimators|300|n_estimators|100|n_estimators|150|
|alpha|0.005|max_depth|5|max_depth|5|max_depth|6|
|tol|0.0001|min_samples_split|4|min_child_weight|1|min_samples_split|2|
|max_iter|300|min_samples_leaf|1|colsample_bytree|0.7|min_samples_leaf|1|
|learning_rate_init|0.01|learning_rate|0.01|learning_rate|0.05|\|\|
|momentum|0.9|subsample|0.9|subsample|0.7|\|\|
|validation_fraction|0.1|\|\|\|\|\|\|


(a)

RF

0.40

Train: R2=0.90, MAE=0.01, RMSE=0.02 Test: R2=0.83, MAE=0.02, RMSE=0.03

0.35

(b)

XGBoost

0.40

Train: R2=0.94, MAE=0.01, RMSE=0.01 Test: R2=0.75, MAE=0.020RMSE=0.04

0.35 -

0.30 -

0.30

0.25

0.25

Predictions (W · m-1 . K-1)

0.20 -

Predictions (W · m-1 . K-1)

0.20

0.15 -

0.10

:selected: :selected:

Training set Test set

0.05

0.05

0.10

0.15

0.20 0.25

0.30

0.40

Ground Truth (W · m-1 . K-1)

(c)

GBDT

0.40 Train: R2=0.93, MAE=0.01, RMSE=0.02

Test: R2=0.88, MAE=0.02, RMSE=0.02

0.35

0.15 -

0.10 -

:unselected:

Training set Test set0.05 - 0.35

:selected:

T

0.05

0.10 0.15 0.20

0.25

0.30

0.35

0.40

Ground Truth (W · m-1 . K-1)

(d)

MLP

0.40 Train: R2=0.63, MAE=0.02, RMSE=0.03 Test: R2=0.42, MAE=0.02, RMSE=0.05

0.35

0.30

0.30

0.25

Predictions (W · m-1 . K-1)

0.20 -

:selected:

0.25

Predictions (W · m-1 . K-1)

0.20

0.15 -

0.10 -

:selected: Training set Test set

:selected:

0.05

0.05

0.10 0.15

0.20 0.25

0.30 0.35

0.40

Ground Truth (W · m-1 . K-1)

0.15

:selected:

:selected:

0.10

:unselected:

Training set Test set 0.30 :selected:

0.05

0.05

0.10

0.15

0.20

0.25

0.35

0.40

Ground Truth (W · m-1 . K-1)

FIG. 4. Comparative pairwise plots of predicted versus ground truth TC, as calculated by MD, across training and test datasets for four models: MLP, RF, GBDT, and XGBoost, with evaluation metrics including R2, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE).

utilized to evaluate model precision. It was observed that the GBDT model markedly surpassed the MLP in predictive capability and marginally exceeded both the RF and XGBoost models. Specifically, the GBDT model achieved an R2 of 0.93 on the training subset and an R2

of 0.88 on the validation subset.

Prior research endeavors have similarly employed ML techniques for polymer TC prediction, utilizing a dataset of 469 polymers and decoding SMILES to a 300-

dimensional feature space, yielding a prediction accuracy of R2=0.828. Our model demonstrates a 5.2% improve- ment in prediction accuracy over preceding research, as gauged by the R2 metric on a validation set, despite uti- lizing a dataset of equivalent scale and a feature space reduced by a factor of 30. This achievement aligns with our projected expectations. These findings underscore the viability and promise of leveraging ML methodolo- gies for predicting polymer TC, facilitating the identifi- cation of polymers with specific thermal conductivities, and even the discovery and creation of materials tailored for particular thermal applications.

Feature Importance

QED

58.79%

MLP

8.75%

MMR -

7.61%

NVE -

7.12%

NRB

Features

MWT

6.83%

6.23%

BBJ

4.51%

TPS

0.12%

NHA

0.04%

FHA - 0.004%

10-4

10-3

10-2

Importance (log scale)

10-1 100

FIG. 5. Bar chart of model feature importance contributions on a logarithmic scale. This figure displays the importance of SHAP features in predicting TC, as determined by the trained GBDT through SHAP feature importance analysis.

Each feature extracted in our study is imbued with dis- tinct physical significance, i.e., rendering our model in- herently interpretable. To delineate the impact of these features on TC and identify those of paramount impor- tance, we employed an interpretable ML framework to generate a representation of feature importance (refer to Figure 5). Employing Lundberg and Lee's SHapley Addi- tive Explanations (SHAP) [20], a methodology designed to furnish interpretations for individual predictions, al- lows us to leverage the game-theoretical foundation of Shapley values. Here, features exhibiting substantial ab- solute Shapley values are deemed crucial. Aiming for a global perspective on feature importance, the figure's abscissa represents the mean of absolute Shapley values across features, while the ordinate lists the top 10 fea- tures, arranged in descending order of their SHAP im- portance. Within the context of the GBDT model, the QED emerges as the most influential feature, altering the average predicted absolute probability of TC by 0.5879 (0.5879 on the x-axis). Subsequently, the most signifi- cant features include MLP, MMR, NVE, and NRB, which have feature importance of 0.0875, 0.0761, 0.0712 and

6

0.0683, respectively. This further elucidates the validity of our method to retain features with distinct physical interpretations, such as NVE, despite their high correla- tions, in the preliminary phase of data analysis.

High

QED

NRB

MLP

BBJ

MMR

Feature value

MWT

NVE

TPS

NHA

FHA

-0.06 -0.04 -0.02 0.00 0.02 0.04 0.06

0.08 0.10

SHAP value (impact on model output)

Low

FIG. 6. Average impact of model features on predictive out- comes: a SHAP summary visualization.

Upon determining the importance of various features, our investigation expanded to elucidate how these fea- tures-particularly the paramount ones-affect the TC. Our work aimed to decipher the physical underpin- nings distinguishing the thermal conductivities of poly- mers characterized by disparate significant features. The SHAP summary plot (Figure 6) excellently encapsulates this objective by integrating feature importance with their effects. Each dot within the summary plot cor- responds to a polymer sample associated with a spe- cific feature. The y-axis enumerates the ten distinct fea- tures, whereas the x-axis quantifies the Shapley value attributable to a feature for a given sample, with the color gradient from red to blue denoting high to low val- ues of the feature, respectively. Within the ambit of a single feature, dots sharing identical Shapley values con- verge along the x-axis, and such congruent points exhibit a vertical jitter towards the y-axis. This mechanism facil- itates an understanding of the Shapley value distribution for each feature.

Our investigation delves into the influence of key fea- tures identified within our research on the TC of poly- mers, aiming to unearth the physical rationales underpin- ning these observations. Initially, the manifestation of a positive Shapley value corresponding to a sample exhibit- ing a low QED value suggests that diminutive values of this particular feature positively impact the model's out- put. Consequently, polymers characterized by elevated QED values are inclined towards lower TC, whereas those with reduced QED values tend to demonstrate enhanced TC. To our knowledge, this constitutes the inaugural cor- relation of a polymer's TC with its QED value, unveiling a potential inverse relationship between the two param- eters.

Polymers characterized by elevated QED values fre- quently exhibit complex molecular architectures, which might encompass multiple ring structures, appendant chains, or functional groups, rendering these polymers molecularly akin to pharmaceutical entities.[21] It is pos- tulated that such intricacy and the extent of branching could attenuate intermolecular forces, thereby diminish- ing the material's thermal energy conduction efficacy. Furthermore, an observed trend indicates that polymers with higher NRB values manifest enhanced TC, sug- gesting a positive correlation between NRB values and TC. This phenomenon is attributed to the premise that thermal energy transmission in polymers is contingent not solely on intermolecular interactions but also on the molecules' translational, vibrational, and rotational de- grees of freedom. A substantial NRB may denote in- creased intramolecular degrees of freedom for absorb- ing and redistributing thermal energy, alongside aug- mented molecular flexibility to foster a more ordered structure. Under certain conditions, these molecules pos- sessing greater specific heat capacity and inherent energy have the potential to amplify thermal transport at the macroscopic level.## IV. CONCLUSIONS



In summary, we have devised a model employing ML techniques that adeptly forecasts the TC of polymers

[1] Wolf P D, Reichert W M. Indwelling neural implants: Strategies for contending with the in vivo environ- ment [M]//Thermal Considerations for the Design of an Implanted Cortical Brain-Machine Interface (BMI). CRC Press, 2008.

[2] Candadai A A, Nadler E J, Burke J S, et al. Thermal and mechanical characterization of high performance polymer fabrics for applications in wearable devices[J]. Scientific Reports, 2021, 11(1): 8705.

[3] Müller-Plathe F. A simple nonequilibrium molecular dy- namics method for calculating the thermal conductiv- ity[J]. The Journal of chemical physics, 1997, 106(14): 6082-6085.

[4] Zhao D, Qian X, Gu X, et al. Measurement techniques for thermal conductivity and interfacial thermal conduc- tance of bulk and thin film materials[J]. Journal of Elec- tronic Packaging, 2016, 138(4): 040802.

[5] Henry A. Thermal transport in polymers[J]. Annual re- view of heat transfer, 2014, 17.

[6] Mardani A, Liao H, Nilashi M, et al. A multi-stage method to predict carbon dioxide emissions using di- mensionality reduction, clustering, and machine learning techniques[J]. Journal of Cleaner Production, 2020, 275: 122942.

[7] Padula D, Simpson J D, Troisi A. Combining electronic and structural features in machine learning models to predict organic solar cells properties[J]. Materials Hori-

7

characterized by known SMILES notations. This model leverages data pertaining to the physical features and TCs of 400 polymers[1, 2]. For the first time, our method- ology eschews traditional text-processing tactics in favor of interpreting the SMILES notations of polymers into ten physically significant features, thereby circumventing the generation of a high-dimensional, sparse vector. The model predicts the TC of polymers on the test set with an accuracy of R2=0.88. Furthermore, through the lens of interpretable analysis, we have unearthed potential in- verse relationships between the TC of polymers and their QED, alongside direct correlations with NRB. These cor- relations are elucidated from a physical standpoint, ex- amining factors such as intermolecular forces and molec- ular freedom degrees. Our model excels in identifying the traits that predicate a polymer's TC by analyzing its monomeric units, thereby serving as a good pre-screening method in the quest for polymers of specified TCs on a grand scale. In addition, our work may provide some ideas for the design of polymers with specific TC in terms of physical properties. This ML method we designed to study the TC of polymers can also be applied in the study of other properties of polymers, which is highly general- izable and applicable.

zons, 2019, 6(2): 343-349.

[8] Ma R, Zhang H, Luo T. Exploring high thermal con- ductivity amorphous polymers using reinforcement learn- ing[J]. ACS Applied Materials & Interfaces, 2022, 14(13): 15587-15598.

[9] Hayashi Y, Shiomi J, Morikawa J, et al. RadonPy: auto- mated physical property calculation using all-atom clas- sical molecular dynamics simulations for polymer infor- matics[J]. npj Computational Materials, 2022, 8(1): 222.

[10] Kusaba M, Hayashi Y, Liu C, et al. Representation of materials by kernel mean embedding[J]. Physical Review B, 2023, 108(13): 134107.

[11] Landrum G. Rdkit documentation[J]. Release, 2013, 1(1- 79): 4.

[12] eininger D. SMILES, a chemical language and informa- tion system. 1. Introduction to methodology and encod- ing rules[J]. Journal of chemical information and com- puter sciences, 1988, 28(1): 31-36.

[13] Lipinski C A, Lombardo F, Dominy B W, et al. Exper- imental and computational approaches to estimate sol- ubility and permeability in drug discovery and develop- ment settings [J]. Advanced drug delivery reviews, 1997, 23(1-3): 3-25.

[14] Ma R, Zhang H, Luo T. Exploring high thermal con- ductivity amorphous polymers using reinforcement learn- ing[J]. ACS Applied Materials & Interfaces, 2022, 14(13): 15587-15598.

[15] Pedregosa F, Varoquaux G, Gramfort A, et al. Scikit- learn: Machine learning in Python[J]. the Journal of ma- chine Learning research, 2011, 12: 2825-2830.

[16] Cybenko G. Approximation by superpositions of a sig- moidal function[J]. Mathematics of control, signals and systems, 1989, 2(4): 303-314.

[17] Breiman L. Random forests[J]. Machine learning, 2001, 45: 5-32.

[18] Hastie T, Tibshirani R, Friedman J, et al. Boosting and additive trees[J]. The elements of statistical learning: data mining, inference, and prediction, 2009: 337-387.

8

[19] Chen T, Guestrin C. Xgboost: A scalable tree boosting system[C]//Proceedings of the 22nd acm sigkdd interna- tional conference on knowledge discovery and data min- ing. 2016: 785-794.

[20] Lundberg S M, Lee S I. A unified approach to interpret- ing model predictions[J]. Advances in neural information processing systems, 2017, 30.

[21] Tian S, Wang J, Li Y, et al. The application of in silico drug-likeness predictions in pharmaceutical research[J]. Advanced drug delivery reviews, 2015, 86: 2-10.

