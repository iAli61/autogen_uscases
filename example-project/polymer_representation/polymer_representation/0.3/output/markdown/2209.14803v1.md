# polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics

## b Training polyBERT



c Generate 100 million PSMILES



|ID|Polymer 1|9|Polymer 2|C2|
|---|---|---|---|---|
|1|[*]CC[*]|40|[*]CC([*])C|60|
|2|[*]CCO[*]|100|-|0|
|3|[*]CC([*])C1CC1|30|[*]CC([*])C1CCC1|70|
|4||...||...|


Pretraining

PSMILES

O=C(C(C[*])(C)[*])OC

100 million

hypothetical polymers

Chem-

informatics

polyBERT (Transformer)

Handcrafted

fingerprints

-

Multitask property predictors

Multitask property predictors

polyBERT

Preprocessing

[*] CC ([*]) (C) C(=O) OC

Canonicalization

Tokenization polyBERT fingerprints

[*] CC ([*]) (C) C(=O) OC

Masking

[*] CC (C) C(=O) OC

Inference

Polymer Properties



|||||DeBERTa|||
|---|---|---|---|---|---|---|
|||0.2|0.3|0.3|0.2|0.8|
|Embedding||...||...|...||
|||0.9|0.6|0.4|0.5|0.2|
||||||||


Transformer encoder

O

1

New

polymers

BRICS Composition

Fragments

BRICS Decomposition



|ID|Tg|Tm|Ta|E|ε|σ|...|
|---|---|---|---|---|---|---|---|
|1|230|361|683|190|460|16|...|
|2|220|326|604|170|267|11|...|
|3|320|520|620|600|113|23||
|4|...|...|...|...|...|||


Transformer encoder

Feed forward network

PSMILES

[*] CC ([*]) (C) C(=O) OC

Polymer

~13 000

synthesized polymers

Figure 1: Polymer informatics with polyBERT. a Prediction pipelines. The left pipeline shows the prediction using handcrafted fingerprints from cheminformatics tools, while the right pipeline (present work) portrays a fully end-to-end machine-driven predictor using polyBERT. Property symbols are defined in Table 1. ID1 and ID3 are copolymers, and ID2 is a homopolymer. b polyBERT is a polymer chemical language linguist. polyBERT canon- icalizes, tokenizes, and masks polymer SMILES18 (PSMILES) strings strings before passing them to the DeBERTa model. A last dense layer with a softmax activation function finds the masked tokens. polyBERT fingerprints (dashed arrow) are the averages over the token dimension (sentence average) of the last Transformer encoder. c 100 million hypothetical PSMILES strings. First, 13 766 known (i.e., previously synthesized) polymers are decom- posed to 4424 fragments using the BRICS19 method. Second, re-assembling the BRICS fragments in many different ways generates 100 million hypothetical polymers by randomly and enumeratively combining the fragments.

3

eling. In this work, we envision simplified molecular-input line-entry system (SMILES) 18 strings that have been used to represent polymers as the "chemical language" of polymers. We use millions of polymer SMILES (PSMILES) strings for training a language model called polyBERT to become an expert - a linguist - of the polymer chemical language. In combina- tion with multitask deep neural networks, 6,7 polyBERT enables a fully end-to-end machine- driven polymer informatics pipeline that uses and unleashes the true power of artificial intelligence methods. Multitask deep neural networks harness inherent correlations in multi- fidelity and multi-property data sets, scale effortlessly in cloud computing environments, and generalize to multiple prediction tasks.

Recent studies 26-28 demonstrated the benefits of using Transformers in the molecule chemical space. For example, Wang et al. 27 have trained a BERT model29 (the most common general language model) with a data set of molecule SMILES strings. Using BERT's latent space representations of molecules as fingerprints, the authors show that their approach out- performs other fingerprinting methods (including fingerprints of an unsupervised recurrent neural network and a graph-based neural network). Similarly, Schwaller et al. 30,31 have de- veloped a Transformer model to predict retrosynthesis pathways of molecules from reactants and reagents that outperforms known algorithms in the reaction prediction literature. No past study has applied Transformers to polymers.

This study has several critical and novel ingredients. First, we generate a data set of 100 million hypothetical polymers by enumeratively combining chemical fragments ex- tracted from a list of more than 13 000 synthesized polymers. Next, we train polyBERT, a DeBERTa 32-based encoder-only Transformer, using this hypothetical polymer data set to become a polymer chemical linguist. During training, polyBERT learns to translate input PSMILES strings to numerical representations that we use as polymer fingerprints. Finally, we map the polyBERT fingerprints to about 3 dozen polymer properties using our mul- titask ML framework to yield fully machine-driven ultrafast polymer property predictors. For benchmarking, the performance (both accuracy and speed) of this new end-to-end prop-

4

erty prediction pipeline is compared with the state-of-the-art handcrafted Polymer Genome8 (PG) fingerprint based pipeline pioneered previously. Using the ultrafast polyBRET polymer informatics pipeline, we are in a position to predict the properties of the 100 million hypo- thetical polymers intending to find property boundaries of the polymer universe. This work contributes to expediting the discovery, design, development, and deployment of polymers by harnessing the true power of language, data, and artificial intelligence models.

## Results



Data Sets Figure 1c sketches the two-step process for fabricating 100 million hypothetical PSMILES strings. We use the Breaking Retrosynthetically Interesting Chemical Substruc- tures (BRICS) 19 method (as implemented in RDKit33) to decompose previously synthesized 13 766 polymers into 4424 unique chemical fragments. Random and enumerative compo- sitions of these fragments yield 100 million hypothetical PSMILES strings that we first canonicalize (see Methods section) and then use for training polyBERT. The hypothetical PSMILES strings are chemically valid polymers but, mostly, have never been synthesized before.

Once polyBERT has completed its unsupervised learning task using the 100 million hypothetical PSMILES strings, multitask supervised learning maps polyBERT polymer fin- gerprints to multiple properties to produce property predictors. We use the property data set in Table 1 for training the property predictors. The data set contains 28061 (~80 %) homopolymer and 7456 (~20%) copolymer (total of 35 517) data points of 29 experimen- tal and computational polymer properties that pertain to 11 145 different monomers and 1 338 distinct copolymer chemistries, respectively. Each of the 7 456 copolymer data points involves two distinct comonomers at various compositions. All data points in the data set have been used in past studies 6,7,11,34-41 and were produced using computational methods or obtained from literature and other public sources. Supplementary Figures S3-S8 show

5

Table 1: Training data set for the property predictors. The properties are sorted into categories, showed at the top of each block. The data set contains 29 properties (dielectric constants kf are available at 9 different frequencies f). HP and CP stand for homopolymer and copolymer, respectively.



|Property|Symbol|Unit|Sourceª|Data range|Data points HP CP All|||
|---|---|---|---|---|---|---|---|
|Thermal||||||||
|Glass transition temp.|Tg|K|Exp.|[8e+01, 9e+02]|5 183|3 312|8 495|
|Melting temp.|Tm|K|Exp.|2e+02, 9e+02]|2132|1 523|3 655|
|Degradation temp.|Ta|K|Exp.|[3e+02, 1e+03]|3 584|1 064|4 648|
|Thermodynamic & physical||||||||
|Heat capacity|Cp|Jg-1K-1|Exp.|[8e-01, 2e+00]|79||79|
|Atomization energy|Eat|eV atom-1|DFT|[-7e+00, -5e+00]|390||390|
|Limiting oxygen index|Oi|%|Exp.|[1e+01, 7e+01]|101||101|
|Crystall. tendency (DFT)|Xc|%|DFT|[1e-01, 1e+02]|432||432|
|Crystall. tendency (exp.)|Xe|%|Exp.|[1e+00, 1e+02]|111||111|
|Density|ρ|g cm-3|Exp.|[8e-01, 2e+00]|910||910|
|Electronic||||||||
|Band gap (chain)|Egc|eV|DFT|[2e-02, 1e+01]|4 224||4 224|
|Band gap (bulk)|Egb|eV|DFT|[4e-01, 1e+01]|597||597|
|Electron affinity|Eea|eV|DFT|[4e-01, 5e+00]|368||368|
|Ionization energy|E;|eV|DFT|[4e+00, 1e+01]|370||370|
|Electronic injection barrier|E;b|eV|DFT|[2e+00, 7e+00]|2 610||2610|
|Cohesive energy density|δ|eV|Exp.|[2e+01, 3e+02]|294||294|
|Optical & dielectric||||||||
|Refractive index (DFT)|nc||DFT|[1e+00, 3e+00]|382||382|
|Refractive index (exp.)|ne||Exp.|[1e+00, 2e+00]|516||516|
|Dielec. constant (DFT)|ko||DFT|[3e+00, 9e+00]|382||382|
|Dielec. constant at freq. fb|kŕ||Exp.|2e+00, 1e+01]|1 187||1 187|
|Mechanical||||||||
|Young's modulus|E|MPa|Exp.|2e-02, 4e+03]|592|322|914|
|Tensile strength at yield||MPa|Exp.|[3e-05, 1e+02]|216|78|294|
|Tensile strength at break|6|MPa|Exp.|[5e-03, 2e+02]|663|318|981|
|Elongation at break|Eb||Exp.|[3e-01, 1e+03]|868|260|1128|
|Permeability||||||||
|O2 gas permeability|HO2|barrer|Exp.|[5e-06, 1e+03]|390|210|600|
|CO2 gas permeability|PCO2|barrer|Exp.|[1e-06, 5e+03]|286|119|405|
|N2 gas permeability|UN2|barrer|Exp.|[3e-05, 5e+02]|384|99|483|
|H2 gas permeability|PH2|barrer|Exp.|[2e-02, 5e+03]|240|46|286|
|He gas permeability|PHe|barrer|Exp.|[5e-02, 2e+03]|239|58|297|
|CH4 gas permeability|PCH4|barrer|Exp.|[4e-04, 2e+03]|331|47|378|
||||||28 061|7 456|35 517|


a Experiments (Exp.); density functional theory (DFT)

b f E {1.78, 2, 3, 4, 5, 6, 7, 9, 15} is the log10 (frequency in Hz); e.g., k3 is the dielectric constant at a frequency of 1 kHz.

6

histograms for each property.

polyBERT polyBERT iteratively ingests 100 million hypothetical PSMILES strings to learn the polymer chemical language, as sketched in Figure 1b. polyBERT is a DeBERTa 32 model (as implemented in Huggingface's Transformer Python library42) with a supplemen- tary three-stage preprocessing unit for PSMILES strings. First, polyBERT transforms a input PSMILES string into its canonical form (e.g., [*] CCOCCO [*] to [*] COC [*]) using the canonicalize_psmiles Python package developed in this work. Details can be found in the Methods section. Second, polyBERT tokenizes canonical PSMILES strings using the Senten- cePiece43 tokenizer. The tokens are frequent patterns in PSMILES strings and determined in a pretraining process of SentencePiece with the 100 million hypothetical PSMILES strings. Third, polyBERT masks 15 % (default parameter for masked language models) of the tokens to create a self-supervised training task. In this training task, polyBERT is taught to predict the masked tokens using the non-masked surrounding tokens by adjusting the weights of the Transformer encoders (fill-in-the-blanks task). We use 80 million PSMILES strings for train- ing and 20 million PSMILES strings for validation. The validation F1-score is > 99. This exceptionally good F1-score indicates that polyBERT finds the masked tokens in almost all cases. The total CO2 emissions for training polyBERT on our hardware are estimated to be 12.6 kgCO2eq (see CO2 Emission section).

The training with 80 million PSMILES strings renders polyBERT an expert polymer chemical linguist who knows grammatical and syntactical rules of the polymer chemical language. polyBERT learns patterns and relations of tokens via the multi-head self-attention mechanism and fully connected feed-forward network of the Transformer encoders. 25 The attention mechanism instructs polyBERT to devote more focus to a small but essential part of a PSMILES string. polyBERT's learned latent spaces after each encoder block are numerical representations of the input PSMILES strings. The polyBERT fingerprint is the average over the token dimension (sentence average) of the last latent space (dotted

7

line in Figure 1b). We use the Python package SentenceTransformers44 for extracting and computing polyBERT fingerprints.

a polyBERT

## b Polymer Genome



HP

CP

[*]cc([*])cc

[*]CC([*]) CCC

[*]CC([*])c1ccncc1

200

400

600

800

500 Tg (K)

750

Td (K)

1000

2.5

T

5.0 7.5

Egc (eV)

Figure 2: Two-dimensional UMAP 45 plots for polyBERT and Polymer Genome fingerprints and all homo- and copolymer chemistries in Table 1. The triangles (blue, orange, and green) in the first column indicate fingerprint positions in the UMAP spaces of three selected polymers. The colored dots in columns two, three, and four indicate property values of Tg, Ta, and Egc, which stand for the glass transition temperature, degradation temperature, and band gap (chain), respectively. Light gray dots show polymers with unknown property values. The PSMILES strings [*]CC( [*])CC, [*] CC ( [*]) CCC, and [*]CC ( [*]) c1ccncc1 denote poly(but-1-ene), poly(pent-1-ene), and poly(4-vinylpyridine), respectively.

Fingerprints s For acquiring analogies and juxtaposing chemical relevancy, we compare polyBERT fingerprints with the handcrafted Polymer Genome8 (PG) fingerprints that nu- merically encode polymers at three different length scales. A description of PG fingerprints can be found in the Methods section. The PG fingerprint vector for the data set in this work has 945 components and is sparsely populated (93.9% zeros). The reason for this ultra sparsity is that many PG fingerprint components count chemical groups in polymers. 8 A fingerprint component of zero indicates that a chemical group is not present. In contrast, polyBERT fingerprint vectors have 600 components and are fully dense (0% zeros). Fully

8

dense and lower-dimensional fingerprints are often advantageous for ML models whose com- putation time scales superlinear (O(ns), s > 1) with the data set size (n) such as Gaussian process or kernel ridge techniques. Moreover, in the case of neural networks, sparse and high-dimensional input vectors can cause unnecessary high memory load that reduces train- ing and inference speed. We note that the dimensionality of polyBERT fingerprints is a parameter that can be chosen arbitrarily to yield the best training result. A summary of the key figures can be found in Supplementary Table S2.

Figure 2 shows uniform manifold approximation and projection (UMAP) 45 plots for all homo- and copolymer chemistries in Table 1. The colored triangles in the first column indicate the coordinates of three selected polymers for polyBERT and PG fingerprints. We observe for both fingerprint types that the orange and blue triangles are very close, while the green triangle is separate. We also note that polymers corresponding to the orange and blue triangles, namely poly(but-1-ene) and poly(pent-1-ene), have similar chemistry (different by only one carbon atom), but poly(4-vinylpyridine) represented by a green triangle, is different. This chemically intuitive positioning of fingerprints suggests the chemical relevancy of fingerprint distances. The cosine fingerprint distances reported in Supplementary Figure S1 allow for the same conclusion.

The second, third, and fourth columns of Figure 2 display the same UMAP plots as in the first column. Colored dots indicate the property values of Tg, Ta, and Egc, while light gray dots show polymer fingerprints with unknown property values. We observe localized clusters of similar color in each plot pertaining to polymers of similar properties. Although this finding is not surprising for the PG fingerprint because it relies on handcrafted chem- ical features that purposely position similar polymers next to each other, it is remarkable for polyBERT. With no chemical information and purely based on training on a massive amount of PSMILES strings, polyBERT has learned polymer fingerprints that match chem- ical intuition. This again shows that polyBERT fingerprints have chemical pertinence and their distances measure polymer similarity (e.g., using the cosine distance metric).

9

103

polyBERT (CPU)

polyBERT (GPU)

-

Polymer Genome

102

101

Computation time (s)

X

100

10-1

100

101

102

103

Number of PSMILES

TTTT

104

Figure 3: Computation time of polymer fingerprints. The fingerprints are computed on one CPU core (Intel(R) Xeon(R) CPU E5-2667), except for polyBERT (GPU) fingerprints that are computed on one GPU (Quadro GP100). Computation times per PSMILES string, in the order of the legend, are 33.39, 0.76, and 163.59 ms/PSMILES (computed for 104 PSMILES), respectively.

The computation of polyBERT and PG fingerprints scales nearly linear with the number of PSMILES strings although their performance can be quite different, as shown in the log- log scaled Figure 3. The computation of polyBERT (GPU) is over two orders of magnitude (215 times) faster than computing PG fingerprints. polyBERT fingerprints may be computed on CPUs and GPUs. Because of the presently large efforts in industry to develop faster and better GPUs, we expect the computation of polyBERT fingerprint to become even faster in the future. Time is extremely important for high-throughput polymer informatics pipelines that identify polymers from large candidate sets. 11 With an estimate of 0.30 ms/PSMILES for the multitask deep neural networks (see Property Prediction section), the total time using the polyBERT-based pipeline to predict 29 polymer properties sums to 1.06 ms/polymer/GPU.

Property Prediction For benchmarking the property prediction accuracy of polyBERT and PG fingerprints, we train multitask deep neural networks for each property category defined in Table 1. Multitask deep neural networks have demonstrated best-in-class results

10



|a|polyBERT|Polymer Genome|b|PB|PG I|c|min 1|mean 1|max 1|
|---|---|---|---|---|---|---|---|---|---|
|To
-|0.92 0.01|0.92 ± 0.00||0.99|0.98||162.21|498.71|836.81|
|Tm|0.84 ± 0.02|0.84 ± 0.02||0.97|0.97||275.11|592.19|873.36|
|-|0.70 0.03|0.72 0.02||0.96|0.96||394.11|713.01|1185.81|
|Thermal
-|0.82 ± 0.02|0.83 0.01||0.97|0.97|||||
|Cp|0.61 £ 0.11|0.76 0.10||0.93|0.97||0.87|1.28|2.33|
|Eat -|0.85 ± 0.02|0.94 0.02||0.97|0.98||-6.69|-6.03|-4.93|
|0; -|0.57 0.16|0.61 ± 0.16||0.92|0.96||15.11|32.47|72.87|
|Xc -|0.50 £ 0.11|0.47 £ 0.06||0.90|0.90||3.90|41.35|100.50|
|Xe|0.44 ± 0.28|0.41 ₺ 0.32||0.87|0.90||5.19|38.25|92.05|
|p|0.75 ₺ 0.03|0.81 ₺ 0.05||0.95|0.96||0.88|1.30|2.07|
|Thermo. & phys. -|0.62 ± 0.12|0.67 ± 0.12||0.92|0.94|||||
|Egc -|0.89 0.02|0.90 0.02||0.98|0.98||0.42|3.42|10.39|
|Egb|0.93 0.01|0.94 0.01||0.98|0.98||0.78|3.06|10.59|
|Eea|0.93 0.03|0.93 0.03||0.99|0.99||0.59|2.26|5.12|
|E|0.82 0.07|0.85 £ 0.03||0.96|0.97||3.81|5.82|10.63|
|Eib -|0.71 0.05|0.72 ± 0.06||0.94|0.93||2.00|3.34|5.42|
|δ|0.57 £ 0.36|0.65 ± 0.28||0.96|0.95||32.15|132.43|293.57|
|Electronic -|0.81 0.09|0.83 £ 0.07||0.97|0.97|||||
|nc -|0.86 £ 0.06|0.89 0.05||0.98|0.99||1.55|1.95|3.22|
|ne -|0.76 ₺ 0.06|0.78 ₺ 0.02||0.98|0.98||1.33|1.63|2.21|
|kc -|0.86 0.06|0.84 ± 0.04||0.97|0.98||2.92|4.45|10.46|
|Kf|0.91 0.03|0.92 ± 0.02||0.98|0.98||2.10|3.45|9.86|
|Optical & diele. -|0.85 0.05|0.86 0.03||0.97|0.98|||||
|E -|0.75 ₺ 0.07|0.75 ₺ 0.04||0.94|0.94||177.56|2016.50|3884.81|
|Øy -|0.80 £ 0.08|0.81 £ 0.08||0.97|0.96||5.94|67.52|123.94|
|0b -|0.76 0.05|0.79 0.04||0.94|0.94||9.05|83.44|184.27|
|Eb -|0.62 ± 0.06|0.61 ₺ 0.07||0.92|0.92||0.85|14.79|565.19|
|Mechanical -|0.73 £ 0.07|0.74 0.06||0.94|0.94|||||
|HO2|0.96 0.01|0.96 0.01||0.99|0.99||-0.36|5.97|3047.37|
|UCO2|0.94 ± 0.02|0.94 ± 0.03||0.98|0.98||-0.40|25.41|21933.14|
|MN2|0.96 0.03|0.96 0.03||0.99|0.99||-0.39|1.57|892.52|
|MH2|0.97 0.01|0.97 0.01||0.99|1.00||-0.41|41.68|14659.80|
|MHe|0.95 0.02|0.96 0.02||0.99|0.99||-0.34|34.99|6484.88|
|HOCH4|0.95 0.03|0.96 0.03||0.99|0.99||-0.47|1.74|1683.41|
|Gas permeability -|0.95 0.02|0.96 ₺ 0.02||0.99|0.99|||||
|Overall|0.80 ± 0.06|0.81 ₺ 0.06||0.96|0.97|||||


0.99

0.99

Overall

0.80 ± 0.06

0.81 ₺ 0.06

0.96

0.97

0.5

0.6

-

-

0.7

0.8

0.9

R2

Figure 4: R2 performance values for polyBERT (PB) and Polymer Genome (PG) fingerprints. a R2 averages of the five cross-validation validation data sets along with standard deviations (10). b R2 values of the meta learner's test data set. The category-averaged R2 values are stated in the last rows of each block, while overall R2 values are given in the very last block. The properties gas permeabilities (Ar) and elongation at break (Eb) are trained on log base 10 scale (x +> log10(x+1)). The R2 values are reported on this scale. c Minimum, mean, and maximum of polyBERT-based property predictions for 100 million hypothetical polymers. Symbols are defined in Table 1.

11for polymer property predictions, 6,7,11 while being fast, scalable, and readily amenable if more data points become available. Unlike single-task models, multitask models simultaneously predict numerous properties (tasks) and harness inherent but hidden correlations in data to improve their performance. Such correlation exists, for instance, between Tg and Tm, but the exact correlation varies across specific polymer chemistries. Multitask models learn and improve from these varying correlations in data. The training protocol of the multitask deep neural networks follows state-of-the-art methods involving five-fold cross-validation and a consolidating meta learner that forecasts the final property values based upon the ensemble of cross-validation predictors. More details about multitask deep neural networks are provided in the Methods section. Their training process is outlined in Supplementary Figure S2.

Figure 4a shows the color-encoded five-fold cross-validation coefficient of determination (R2) averages across the five validation data sets for 29 polymer properties. Root-mean- square errors (RMSEs) can be found in Supplementary Table S1. Overall, PG performs best (R2 = 0.81) but is very closely followed by polyBERT (R2 = 0.80). This overall performance order of the fingerprint types is persistent with the category averages and properties, except for Xc, Xe, and Eb, where polyBERT slightly outperforms PG fingerprints. We note that polyBERT and PG fingerprints are both practical routes for polymer featurization because their R2 values lie close together and are generally high. polyBERT fingerprints have the accuracy of the handcrafted PG fingerprints but are over two orders of magnitude faster (see Figure 3).

Figure 4b shows high R2 values for each meta learner (one for each category), suggesting an exceptional prediction performance across all properties. We train the meta learners on unseen 20% of the data set and validate using 80% of the data set (also used for cross- validation). The reported validation R2 values thus only partly measure the generalization performance with respect to the full data set. Meta learners can be conceived as taking decisive roles in selecting the best values from the predictions of the five cross-validation

12

models. We use the meta learners for all property predictions in this work. Supplementary Figures S9-S14 show the meta learners' parity plots.

The ultrafast and accurate polyBERT-based polymer informatics pipeline allows us to predict all 29 properties of the 100 million hypothetical polymers that were originally created to train polyBERT. Figure 4c shows the minimum, mean, and maximum for each property. Histograms are given in Supplementary Figures S17-S22. Given the vast size of our data set and consequent chemical space of the 100 million hypothetical polymers, the minimum and maximum values can be interpreted as potential boundaries of the total polymer property space. The data set with 100 million hypothetical polymers including the predictions of 29 properties is available for academic use. The total CO2 emissions for predicting 29 properties of 100 million hypothetical polymers are estimated to be 5.5 kgCO2eq (see CO2 Emission section).## Other Advantages of polyBERT: Beyond Speed and Accuracy



The feed-forward network (last layer in Figure 1b), which predicts masked tokens during the self-supervised training of polyBERT, enables the mapping of numerical latent spaces (i.e., fingerprints) to PSMILES strings. However, because we average over the token dimension of the last latent space to compute fingerprints, we cannot unambiguously map the current polyBERT fingerprints back to PSMILES strings. A modified future version of polyBERT that provides PSMILES strings encoding and fingerprint decoding could involve inserting a dimensionality-reducing layer after the last Transformer encoder. Fingerprint decoders are important elements of design informatics pipelines that invert the prediction pipeline to meet property specifications. We note that the current choice of computing polyBERT fingerprints as pooling averages stems from basic dimensionality reduction considerations that require no modification of the Transformer architecture.

A second advantage of the polyBERT approach is interpretability. Analysing the chemical relevancy of polyBERT fingerprints (as discussed in the Fingerprints section) in greater

13

detail can reveal chemical functions and interactions of structural parts of the polymers. As shown for trained NLP Transformers, 46 deciphering and visualizing the attention layers of the Transformer encoders can reveal such information. Saliency methods47 may explain the relationships between structural parts of the PSMILES strings (inputs) and polymer properties (outputs).

## Discussion



Here, we show a generalizable, ultrafast, and accurate polymer informatics pipeline that is seamlessly scalable on cloud hardware and suitable for high-throughput screening of huge polymer spaces. polyBERT, which is a Transformer-based NLP model modified for the poly- mer chemical language, is the critical element of our pipeline. After training on 100 million hypothetical polymers, the polyBERT-based informatics pipeline arrives at a representation of polymers and predicts polymer properties over two orders of magnitude faster but at the same accuracy as the best pipeline based on handcrafted PG fingerprints.

The accurate prediction of 29 properties for 100 million hypothetical polymers in a reason- able time demonstrates that polyBERT is an enabler to extensive explorations of the polymer universe at scale. polyBERT paves the pathway for the discovery of novel polymers 100 times faster (and potentially even faster with newer GPU generations) than state-of-the-art infor- matics approaches - but at the same accuracy as slower handcrafted fingerprinting methods - by leveraging Transformer-based ML models originallydro developed for NLP. polyBERT fingerprints are dense and chemically pertinent numerical representations of polymers that adequately measure polymer similarity. They can be used for any polymer informatics task that requires numerical representations of polymers such as property predictions (demon- strated here), polymer structure predictions, ML-based synthesis assistants, etc. polyBERT fingerprints have a huge potential to accelerate past polymer informatics pipelines by re- placing the handcrafted fingerprints with polyBERT fingerprints. polyBERT may also be

14

used to directly design polymers based on fingerprints (that can be related to properties) using polyBERT's decoder that has been trained during the self-supervised learning. This, however, requires retraining and structural updates to polyBERT and is thus part of a future work.

## Methods



PSMILES Canonicalization The string representations of homopolymer repeat units in this work are polymer SMILES (PSMILES) strings. PSMILES strings follow the SMILES18 syntax definition but use two stars to indicate the two endpoints of the polymer repeat unit (e.g., [*] CC [*] for polyethylene). The raw PSMILES syntax is non-unique; i.e., the same polymer may be represented using many PSMILES strings; canonicalization is a scheme to reduce the different PSMILES strings of the same polymer to a singel unique canonicalized PSMILES string. polyBERT requires canonicalized PSMILES strings because polyBERT fingerprints change with different writings of PSMILES strings. In contrast, PG fingerprints are invariant to the way of writing PSMILES strings and, thus, do not require canonicaliza- tion. Figure 5 shows three variances of PSMILES strings that leave the polymer unchanged. The translational variance of PSMILES strings allows to move the repeat unit window of polymers (cf., white and red box). The multiplicative variance permits to write polymers as multiples of the repeat unit (e.g., two-fold repeat unit of Nylon 6), while the permutational variance stems from the SMILES syntax definition 18 and allows syntactical permutations of PSMILES strings that leave the polymer unchanged.

For this work, we developed the canonicalize_psmiles Python package that finds the canonical form of PSMILES strings in four steps; (i) it finds the shortest PSMILES string by searching and removing repetition patterns, (ii) it connects the polymer endpoints to create a periodic PSMILES string, (iii) it canonicalizes the periodic PSMILES string using RDKit 33's canonicalization routines, (iv) it breaks the periodic PSMILES string to create

15

Nylon 6 IZ

Translation

O

ZI

O

[*]NCCCCCC(=O)[*]

[*]CCC(=O)NCCC[*]

Multiplication:

[*]NCCCCCC(=O)NCCCCCC(=O)[*] Multiplication and permutation: [*]NCCCCCC(NCCCCCC([*])=O)=O

Figure 5: Translational, multiplicative, and permutational variances of PSMILES strings. The gray and red boxes represent the smallest repeat unit of poly(hexano-6-lactam) (Nylon 6). The red box can be translated to match the black box. The dashed boxes show the second smallest repeat unit (two-fold repeat unit) of Nylon 6.

the canonical PSMILES string. The canonicalize_psmiles package is available at https : //github. com/Ramprasad-Group/canonicalize_psmiles.

Polymer Fingerprinting Fingerprinting converts geometric and chemical information of polymers (based upon the PSMILES string) to machine-readable numerical representations in the form of vectors. These vectors are the polymer fingerprints and can be used for prop- erty predictions, similarity searches, or other tasks that require numerical representations of polymers.

We compare the polyBERT fingerprints, developed in this work, with the handcrafted Polymer Genome (PG) polymer fingerprints. PG fingerprints capture key features of poly- mers at three hierarchical length scales. 8,22 At the atomic scale (1st level), PG fingerprints track the occurrence of a fixed set of atomic fragments (or motifs).23 The block scale (2nd level) uses the quantitative structure-property relationship (QSPR) fingerprints 20,36 for capturing features on larger length-scales as implemented in the cheminformatics toolkit RDKit. 33 The chain scale (3rd level) fingerprint components deal with "morphological de- scriptors" such as the ring distance or length of the largest side-chain. 36 The PG fin- gerprints are developed within the Ramprasad research group and used, for example, at

16

## https://PolymerGenome. org. More details can be found in References 8,36.



As discussed recently, 6,11 we sum the composition-weighted polymer fingerprints to com- pute copolymer fingerprints F = >{ Fici, where N is the number of comonomers in the copolymer, Fi the ith comonomer fingerprint, and ci the fraction of the ith comonomer. This approach renders copolymer fingerprints invariant to the order in which one may sort the comonomers and satisfies the two main demands of uniqueness and invariance to dif- ferent (but equivalent) periodic unit specifications. Contrary to homopolymer fingerprints, copolymer fingerprints may not be interpretable (e.g., the composition-weighted sum of the fingerprint component "length of largest side-chain" of two homopolymers has no physical meaning).

Multitask Neural Networks Multitask deep neural networks simultaneously learn mul- tiple polymer properties to utilize inherent correlations of properties in data sets. The training protocol of the concatenation-conditioned multitask predictors follows state-of-the- art techniques involving five-fold cross-validation and a meta learner that forecasts the final property values based upon the ensemble of cross-validation predictors. 6,7,11 Supplementary Figure S2 details this process. After shuffling, we split the data set into two parts and use 80 % for the five cross-validation models and for validating the meta learners. 20% of the data set is used for training the meta learners. We perform data set stratification of all splits based on the polymer properties. All parameters of the neural networks, such as the number of layers, number of nodes, dropout rates, and activation functions, are optimized using the Hyperband method48 of the Python package KerasTuner. 49 The multitask deep neural networks are implemented using the Python API of TensorFlow. 50

## CO2 Emission



Experiments were conducted using a private infrastructure, which has an estimated carbon efficiency of 0.432 kgCO2eq kWh-1. A cumulative of 31 hours of computation was performed

17

on four Quadro-GP100-16GB (thermal design power of 235 W) for training polyBERT. Total emissions are estimated to be 12.6 kgCO2eq. The total emissions for predicting 29 properties for 100 million hypothetical polymers are estimated to be 5.5 kgCO2eq. Estimations were conducted using the Machine Learning Impact calculator presented in Reference 51.

## Data and Code Availability



The polyBERT code and data set of 100 million hypothetical polymers with the predictions of 29 properties are available for academic use at https: //github. com/Ramprasad-Group/ polyBERT. The trained polyBERT model is available at https : //huggingface. co/kuelumbus/ polyBERT. The Python package for canonicalizing PSMILES strings is available at https : //github. com/Ramprasad-Group/canonicalize_psmiles. polyBERT-based property pre- dictions will be made accessible through the polymer informatics platform Polymer Genome at https://PolymerGenome. org.

## Declaration of Interests



R.R. is the founder of the company Matmerize, Inc., that intends to provide polymer infor- matics services. A provisional patent on polyBERT has been filed by R.R. and C.K.

## Acknowledgement



C.K. thanks the Alexander von Humboldt Foundation for financial support. We acknowledge funding from the Office of Naval Research through a Multidisciplinary University Research Initiative grant (N00014-17-1-2656) and the National Science Foundation (#1941029).

18

## Author Contributions



C. K. designed, trained and evaluated the machine learning models and drafted this paper. The work was conceived and guided by R. R. All authors discussed results and commented on the manuscript.

## Supporting Information Available



Supplementary Figures and Tables are available.

## References



(1) Plastics Europe. https://plasticseurope. org/knowledge-hub/

plastics-the-facts-2021/.

(2) Batra, R .; Song, L .; Ramprasad, R. Emerging materials intelligence ecosystems propelled by machine learning. Nature Reviews Materials 2021, 6, 655-678, DOI: 10.1038/s41578-020-00255-y.

(3) Chen, L .; Pilania, G .; Batra, R .; Huan, T. D .; Kim, C .; Kuenneth, C .; Ramprasad, R. Polymer informatics: Current status and critical next steps. Materials Science and Engineering: R: Reports 2021, 144, 100595, DOI: 10. 1016/j . mser. 2020. 100595.

(4) Audus, D. J .; de Pablo, J. J. Polymer Informatics: Opportunities and Challenges. ACS Macro Letters 2017, 6, 1078-1082, DOI: 10. 1021/acsmacrolett. 7b00228.

(5) Adams, N .; Murray-Rust, P. Engineering Polymer Informatics: Towards the Computer- Aided Design of Polymers. Macromolecular Rapid Communications 2008, 29, 615-632, DOI: 10.1002/marc.200700832.

19

(6) Kuenneth, C .; Schertzer, W .; Ramprasad, R. Copolymer Informatics with Multitask Deep Neural Networks. Macromolecules 2021, 54, 5957-5961, DOI: 10.1021/acs.macromol.1c00728.

(7) Kuenneth, C .; Rajan, A. C .; Tran, H .; Chen, L .; Kim, C .; Ramprasad, R. Polymer informatics with multi-task learning. Patterns 2021, 2, 100238, DOI: 10.1016/j.patter.2021.100238.

(8) Doan Tran, H .; Kim, C .; Chen, L .; Chandrasekaran, A .; Batra, R .; Venkatram, S .; Kamal, D .; Lightstone, J. P .; Gurnani, R .; Shetty, P .; Ramprasad, M .; Laws, J .; Shel- ton, M .; Ramprasad, R. Machine-learning predictions of polymer properties with Poly- mer Genome. Journal of Applied Physics 2020, 128, 171104, DOI: 10. 1063/5. 0023759.

(9) Chen, G .; Tao, L .; Li, Y. Predicting Polymers' Glass Transition Temperature by a Chemical Language Processing Model. Polymers 2021, 13, 1898, DOI: 10.3390/polym13111898.

(10) Pilania, G .; Iverson, C. N .; Lookman, T .; Marrone, B. L. Machine-Learning-Based Pre- dictive Modeling of Glass Transition Temperatures: A Case of Polyhydroxyalkanoate Homopolymers and Copolymers. Journal of Chemical Information and Modeling 2019, 59, 5013-5025, DOI: 10.1021/acs. jcim. 9b00807.

(11) Kuenneth, C .; Lalonde, J .; Marrone, B. L .; Iverson, C. N .; Ramprasad, R .; Pi- lania, G. Bioplastic Design using Multitask Deep Neural Networks. 2022, DOI: 10.48550/arXiv.2203.12033.

(12) Barnett, J. W .; Bilchak, C. R .; Wang, Y .; Benicewicz, B. C .; Murdock, L. A .; Bereau, T .; Kumar, S. K. Designing exceptional gas-separation polymer membranes using machine learning. Science Advances 2020, 6, DOI: 10. 1126/sciadv. aaz4301.

(13) Kim, C .; Batra, R .; Chen, L .; Tran, H .; Ramprasad, R. Polymer design using genetic

20

algorithm and machine learning. Computational Materials Science 2021, 186, 110067, DOI: 10.1016/j. commatsci.2020. 110067.

(14) Kern, J .; Chen, L .; Kim, C .; Ramprasad, R. Design of polymers for energy storage capacitors using machine learning and evolutionary algorithms. Journal of Materials Science 2021, 56, 19623-19635, DOI: 10. 1007/s10853-021-06520-x.

(15) Gurnani, R .; Kamal, D .; Tran, H .; Sahu, H .; Scharm, K .; Ashraf, U .; Ram- prasad, R. polyG2G: A Novel Machine Learning Algorithm Applied to the Genera- tive Design of Polymer Dielectrics. Chemistry of Materials 2021, 33, 7008-7016, DOI: 10.1021/acs. chemmater. 1c02061.

(16) Batra, R .; Dai, H .; Huan, T. D .; Chen, L .; Kim, C .; Gutekunst, W. R .; Song, L .; Ramprasad, R. Polymers for Extreme Conditions Designed Using Syntax-Directed Variational Autoencoders. Chemistry of Materials 2020, 32, 10489-10500, DOI: 10.1021/acs. chemmater. 0c03332.

(17) Wu, S .; Kondo, Y .; Kakimoto, M .- a .; Yang, B .; Yamada, H .; Kuwajima, I .; Lam- bard, G .; Hongo, K .; Xu, Y .; Shiomi, J .; Schick, C .; Morikawa, J .; Yoshida, R. Machine-learning-assisted discovery of polymers with high thermal conductivity us- ing a molecular design algorithm. npj Computational Materials 2019, 5, 66, DOI: 10.1038/s41524-019-0203-2.

(18) Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. Journal of Chemical Information and Modeling 1988, 28, 31-36, DOI: 10.1021/ci00057a005.

(19) Degen, J .; Wegscheid-Gerlach, C .; Zaliani, A .; Rarey, M. On the Art of Compiling and Using 'Drug-Like' Chemical Fragment Spaces. ChemMedChem 2008, 3, 1503-1507, DOI: 10.1002/cmdc.200800178.

21

(20) Le, T .; Epa, V. C .; Burden, F. R .; Winkler, D. A. Quantitative Structure-Property Relationship Modeling of Diverse Materials Properties. Chemical Reviews 2012, 112, 2889-2919, DOI: 10.1021/cr200066h.

(21) Rogers, D .; Hahn, M. Extended-Connectivity Fingerprints. Journal of Chemical Infor- mation and Modeling 2010, 50, 742-754, DOI: 10.1021/ci100050t.

(22) Mannodi-Kanakkithodi, A .; Pilania, G .; Huan, T. D .; Lookman, T .; Ramprasad, R. Machine Learning Strategy for Accelerated Design of Polymer Dielectrics. Scientific Reports 2016, 6, 20952, DOI: 10. 1038/srep20952.

(23) Huan, T. D .; Mannodi-Kanakkithodi, A .; Ramprasad, R. Accelerated materials prop- erty predictions and design using motif-based fingerprints. Physical Review B 2015, 92, 014106, DOI: 10. 1103/PhysRevB. 92. 014106.

(24) Moriwaki, H .; Tian, Y .- S .; Kawashita, N .; Takagi, T. Mordred: a molec- ular descriptor calculator. Journal of Cheminformatics 2018, 10, 4, DOI: 10.1186/s13321-018-0258-y.

(25) Vaswani, A .; Shazeer, N .; Parmar, N .; Uszkoreit, J .; Jones, L .; Gomez, A. N .; Kaiser, L .; Polosukhin, I. Attention Is All You Need. 2017, DOI: 10.48550/arXiv. 1706. 03762.

(26) Chithrananda, S .; Grand, G .;

Ramsundar, B. ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property 10.48550/arXiv.2010.09885.

Prediction.

2020, DOI:

(27) Wang, S .; Guo, Y .; Wang, Y .; Sun, H .; Huang, J. SMILES-BERT. Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Bi- ology and Health Informatics. New York, NY, USA, 2019; pp 429-436, DOI: 10.1145/3307339.3342186.

22

(28) Li, J .; Jiang, X. Mol-BERT: An Effective Molecular Representation with BERT for Molecular Property Prediction. Wireless Communications and Mobile Computing 2021, 2021, 1-7, DOI: 10. 1155/2021/7181815.

(29) Devlin, J .; Chang, M .- W .; Lee, K .; Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2018, DOI: 10.48550/arXiv. 1810.04805.

(30) Schwaller, P .; Laino, T .; Gaudin, T .; Bolgar, P .; Hunter, C. A .; Bekas, C .; Lee, A. A. Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Predic- tion. ACS Central Science 2019, 5, 1572-1583, DOI: 10.1021/acscentsci. 9b00576.

(31) Schwaller, P .; Petraglia, R .; Zullo, V .; Nair, V. H .; Haeuselmann, R. A .; Pisoni, R .; Bekas, C .; Iuliano, A .; Laino, T. Predicting retrosynthetic pathways using transformer- based models and a hyper-graph exploration strategy. Chemical Science 2020, 11, 3316-3325, DOI: 10.1039/C9SC05704H.

(32) He, P .; Liu, X .; Gao, J .; Chen, W. DeBERTa: Decoding-enhanced BERT with Disen- tangled Attention. 2020, DOI: 10. 48550/arXiv. 2006. 03654.

(33) Landrum, G .; others, RDKit: Open-source cheminformatics. 2006,

(34) Jha, A .; Chandrasekaran, A .; Kim, C .; Ramprasad, R. Impact of dataset uncertainties on machine learning model predictions: the example of polymer glass transition tem- peratures. Modelling and Simulation in Materials Science and Engineering 2019, 27, 024002, DOI: 10.1088/1361-651X/aaf8ca.

(35) Kim, C .; Chandrasekaran, A .; Jha, A .; Ramprasad, R. Active-learning and materials design: the example of high glass transition temperature polymers. MRS Communica- tions 2019, 9, 860-866, DOI: 10. 1557/mrc. 2019.78.

23

(36) Kim, C .; Chandrasekaran, A .; Huan, T. D .; Das, D .; Ramprasad, R. Polymer Genome: A Data-Powered Polymer Informatics Platform for Property Predictions. The Journal of Physical Chemistry C 2018, 122, 17575-17585, DOI: 10.1021/acs. jpcc. 8b02913.

(37) Patra, A .; Batra, R .; Chandrasekaran, A .; Kim, C .; Huan, T. D .; Ram- prasad, R. A multi-fidelity information-fusion approach to machine learn and pre- dict polymer bandgap. Computational Materials Science 2020, 172, 109286, DOI: 10.1016/j.commatsci.2019.109286.(38) Chen, L .; Kim, C .; Batra, R .; Lightstone, J. P .; Wu, C .; Li, Z .; Deshmukh, A. A .; Wang, Y .; Tran, H. D .; Vashishta, P .; Sotzing, G. A .; Cao, Y .; Ramprasad, R. Frequency-dependent dielectric constant prediction of polymers using machine learning. npj Computational Materials 2020, 6, 61, DOI: 10. 1038/s41524-020-0333-6.

(39) Venkatram, S .; Kim, C .; Chandrasekaran, A .; Ramprasad, R. Critical Assessment of the Hildebrand and Hansen Solubility Parameters for Polymers. Journal of Chemical Information and Modeling 2019, 59, 4188-4194, DOI: 10.1021/acs. jcim. 9b00656.

(40) Zhu, G .; Kim, C .; Chandrasekarn, A .; Everett, J. D .; Ramprasad, R .; Lively, R. P. Polymer genome-based prediction of gas permeabilities in polymers. Journal of Polymer Engineering 2020, 40, 451-457, DOI: 10. 1515/polyeng-2019-0329.

(41) PolyInfo. https://polymer.nims.go. jp/en/.

(42) Wolf, T. et al. Transformers: State-of-the-Art Natural Language Processing. Pro- ceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing: System Demonstrations. Stroudsburg, PA, USA, 2020; pp 38-45, DOI: 10.18653/v1/2020.emnlp-demos.6.

(43) Kudo, T .; Richardson, J. SentencePiece: A simple and language indepen- dent subword tokenizer and detokenizer for Neural Text Processing. 2018, DOI: 10.48550/arXiv. 1808.06226.

24

(44) Reimers, N .; Gurevych, I. Sentence-BERT: Sentence Embeddings using Siamese BERT- Networks. 2019, DOI: 10.48550/arXiv. 1908. 10084.

(45) McInnes, L .; Healy, J .; Melville, J. UMAP: Uniform Manifold Approximation and Pro- jection for Dimension Reduction. 2018, DOI: 10.48550/arXiv. 1802. 03426.

(46) Vig, J. A Multiscale Visualization of Attention in the Transformer Model. Pro- ceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics: System Demonstrations. Stroudsburg, PA, USA, 2019; pp 37-42, DOI: 10.18653/v1/P19-3007.

(47) Bastings, J .; Filippova, K. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? 2020, DOI: 10.48550/arXiv.2010.05607.

(48) Li, L .; Jamieson, K .; DeSalvo, G .; Rostamizadeh, A .; Talwalkar, A. Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization. Journal of Machine Learning Research 2016, 18, 1-52, DOI: 10.48550/arXiv. 1603. 06560.

(49) O'Malley, T .; Bursztein, E .; Long, J .; Chollet, F .; Jin, H .; Invernizzi, L. Keras Tuner. 2019; https://github. com/keras-team/keras-tuner.

(50) Martin, A. et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. 2015; https://www.tensorflow.org/.

(51) Lacoste, A .; Luccioni, A .; Schmidt, V .; Dandres, T. Quantifying the Carbon Emissions of Machine Learning. 2019, DOI: 10.48550/arXiv. 1910. 09700.

25