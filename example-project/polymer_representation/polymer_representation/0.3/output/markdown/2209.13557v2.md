# Polymer informatics at-scale with multitask graph neural networks

## 1 Introduction



Polymers have emerged as a powerful class of materials for a wide range of applications be- cause of their low-cost processing, chemical stability, tunable chemistries, and low densities. These attributes have led to vigorous, widespread, and sustained research, and to the devel- opment of new polymeric materials. 1-3 The result is a constant flux of materials data. Over the past decade, the polymer informatics community has translated this data stream into machine-learned property predictors that efficiently screen libraries of candidate polymers for subsequent experimental inquiry. 4,5

Currently, most approaches for polymer screening rely on handcrafted features-extracted from the chemical structure of a polymer repeat unit-as input for property predictors. 6,7 These approaches are highly accurate, but feature extraction becomes a bottleneck (as dis- cussed in Section 3.1) when used to screen vast swathes of the polymer chemical space. This bottleneck is increasingly exposed by the proliferation of enumeration methods 8,9 and long-sought 10,11 inverse predictors, 12-16 which directly locate optimal pockets of the chemical space from a user-defined wish list of material properties. By leveraging these tools, the day that we routinely generate billions of polymer candidates is fast approaching. Advances in polymer screening and feature engineering are needed to keep up with this pace.

An alternative to handcrafting features is "machine learn" them. One approach is to represent the material as raw text, such as a simplified molecular-input line-entry system (SMILES) 17 or BigSMILES18 string, and then learn features with a neural network specifi- cally designed for natural language processing. 19 Another promising approach is to represent the material as a graph, and then train a Graph Neural Network (GNN) 20 to learn features. To date, GNNs have outperformed approaches based on handcrafted features 20-24 on the massive QM9 database25 for small molecules. Similarly, feature learning approaches have supplanted traditional methods in other domains (e.g., convolutional neural networks26 in computer vision and transformers27 in natural language processing) where the extraction of handcrafted representations from the input data is non-trivial or impractical. 26

2

Another important emerging trend in machine learning (ML) for materials science is multitask learning. 5,28 The core idea behind multitask learning is that, by training a model to simultaneously learn multiple correlated target properties, the model is less likely to produce overfitted predictions to the training set of any one target property.28 As a result, the predictive performance for each property is improved. This idea is seen in nature as well. For example, there is evidence that training in one sport can improve a young athlete in another related sport. 29

A handful of polymer GNNs have been explored in the past. 30-36 The majority of these approaches are single task. The GNN proposed by Mohapatra et al. 34 is suitable for biopoly- mers, in which the monomer sequence is known. Other approaches, 32,33,35,36 geared toward synthetic polymers (the subject of interest in this work), represent a polymer using the graph of a predominant repeat unit. This introduces the need for invariance to certain transforma- tions of the repeat unit graph: translation, addition, and subtraction (as defined in Section 2.2). A subset of the GNNs for synthetic polymers 35,36 are invariant to translation, but not to addition and subtraction. In other words, a GNN that preserves the invariant properties of polymer repeat units has not been developed until now. Our work, a powerful multitask GNN architecture (see Fig. 1) for polymers, fills this gap. We call this development the Polymer Graph Neural Network (polyGNN).

In the small molecule domain, the adoption of GNNs is motivated by systematic work 20 comparing GNNs and handcrafted approaches on even footing across a diverse set of molecules and predictive tasks. Analogous studies are absent from the synthetic polymer domain. Pre- vious works have compared feature learning and handcrafted approaches for up to two 31,35 polymer properties, or for several properties in the same class 30 (e.g., electronic properties). In what follows, we compare polyGNN with the handcrafted fingerprint originally hosted under the Polymer Genome (PG) project4 on a large and diverse data set consisting of more than 13,000 polymers and 30+ predictive tasks-spanning thermal, thermodynamic, physical, electronic, optical, dielectric, and mechanical properties, the Hildebrand solubility

3

:unselected:

## Learned atomic fingerprints



C :selected:

:unselected:

:unselected:

Message Passing

:unselected:

Learned polymer fingerprint

M

Encoder

C

1

C

:unselected:

n

O

C

:unselected:

Block

:unselected:

:unselected:

e

a

:unselected:

Estimator

n



|Property|Prediction|
|---|---|
|Td|470|
||330|
|Tg|230|
|. . .|. . .|


:unselected:

Td:[1, 0, 0, ... ]

Tm :[0, 1, 0, ... ]

Tg: [0, 0, 1, ... ] ...

Repeat unit (e.g., polyethylene glycol)

Featurized periodic graph

Property selectors

Figure 1: The polyGNN architecture. The Encoder converts the repeat unit SMILES string to a periodic graph and then computes initial atomic and bond fingerprint vectors (green and purple squares, respectively). A subsequent set of atomic fingerprints (yellow squares) are learned by the Message Passing Block and then averaged, yielding the learned polymer fingerprint (light blue square). This fingerprint and a series of selector vectors are passed to the Estimator, producing a series of property predictions. Ta, Tm, Tg refer to the critical temperatures for thermal decomposition, melting, and glass transition, respectively.

parameter, as well as permeability of six gases.

Our benchmark, the PG fingerprint, contains descriptors that correspond to one of three length scales. The finest-level components are atomic triples (e.g., C¿O;Nk) where the sub- scripts denote the atomic coordination. The next (block) level contains pre-defined atomic fragments (e.g., the common cycloalkenes). These two levels contain strictly one-hot fea- tures. At the highest (chain) level are numerical features that describe the atomic or block topology (e.g., the number of atoms in the largest side chain). The handcrafted PG fin- gerprint is the current state-of-the-art in polymer representation, and has shown success in the numerical representation of materials over a wide chemical and property space. 4,8,37 The handcrafted PG fingerprint-based property predictors thus serve as veritable performance baselines. We find that polyGNN, relative to these baselines, leads to a one to two orders of magnitude faster fingerprinting and better or comparable model accuracy in most polymer property prediction tasks. polyGNN thus offers a powerful new polymer informatics option for screening the polymer chemical space at scale.

4

## 2 Methods



## 2.1 Data set and preparation



Our corpus contains measurements for up to 36 properties of 13,388 polymers, yielding over 21,000 data points in total. The unit and symbol for each property is listed in Fig. 2A. The distribution of data points per property is plotted in Fig. 2B. These data points come from in-house density functional theory (DFT) computations, 38-40 experimental data col- lected from the literature, 41-46 printed handbooks, 47-49 and online databases. 50,51 Band gaps were calculated for both individual polymer chains Egc and polymer crystal (bulk) structures Egb using DFT. DFT data contain uncertainties due to the choice of exchange correlation functional, pseudopotentials, optimization procedure, etc. while data from physical experi- ment comes with uncertainty due to sample and measurement conditions. Thus, data for the same property but from different sources (e.g., DFT-computed and experimentally measured refractive index) are treated separately and then co-learned with multitask learning.

Our multitask learning approach requires data preprocessing steps. First, the training data for each property was MinMax scaled between zero and one. This ensures that the op- timizer of a multitask ML model equally weights the loss for each property during training. Second, to better exploit correlations between properties,5 we divided our entire 36 property data set into six "property groups": thermal properties, thermodynamic & physical proper- ties, electronic properties, optical & dielectric properties, solubility & gas permeability, and mechanical properties. The stratification of properties by group is shown in Fig. 2B. Finally, we designate each property within one group a unique one hot "selector" vector (see Fig. 1 for example selector vectors of thermal properties). These vectors are used by our ML models to distinguish between multiple tasks.

5

A



|Symbol|Name|Unit|
|---|---|---|
|1|Thermal conductivity|W mK-1|
|Tm|Melting temperature|K
:selected:|
|Ta|Decomposition temperature|K
:selected:|
||Glass transition temperature|K
:selected:|
|Eat|Atomization energy|eV atom-1|
|Cp|Heat capacity|JgK-|
|O|Limiting oxygen index|%|
|Xe|Crystallization tendency (exp.)|%|
||Fractional free volume|1|
|Xc
:selected:|Crystallization tendency (DFT)|%|
|p|Density|g cm-3|
|Ea|Electron affinity|eV
:selected:|


57



|Symbol|Name|Unit|
|---|---|---|
|E|Ionization energy|eV|
|Egb|Band gap (bulk)|eV|
|Egc|Band gap (chain)|eV|
|€0|Dielectric constant (DFT)|1|
|Ef|Dielectric constant (exp.)|1|
|no|Refractive index (DFT)|1|
|ne|Refractive index (exp.)|1|
|Y
:selected:|Young's modulus|MPa|
|Ots
:selected:|Tensile strength|MPa|
|:selected:|Hildebrand solubility parameter|V MPa
:selected:|
|:selected:|Gas permeability|Barrer|
|-
:unselected:||-|


910

Mechanical

432 Solubility & perm.

111

128

101

79

3021

2158

1187

629 672

516

242

111

52 53

55

55

À Tm Ta To Eat Cp Oi Xe Vif Xc P Ea Ei Egb Egc 60

Ef nc ne

Y Ots Os Mg

Figure 2: Breakdown of our data set. (A) The symbol, name, and unit of each property in our data set. For properties with data from both experiment and DFT calculations, the two sources are distinguished by the abbreviations "expt." and "DFT". Our data set includes the permeability ug of six gases g E {He, H2, CO2, 02, N2, CH4}. Each permeability data point is scaled by x -> log10(x + 1). Our experimental dielectric constant Ef data contains measurements at nine frequencies f € {1.78, 2, 3, 4, 5, 6, 7, 9, 15} in log10Hz. The distribution of ug and Ef are given in Section S1. (B) The data set size per property, shown on both the y-axis and above each bar. Bars of the same color belong to the same property class. "perm." stands for gas permeability.

## 2.2 polyGNN



All GNNs rely on a well-defined graph representation of their input. If the input is a small molecule, then building a corresponding graph is straight-forward-each heavy (i.e., non- hydrogen) atom is a graph node and each bond between heavy atoms is a graph edge.

6

However, polymers are macromolecules with numerous atoms and bonds. Creating a node and edge for each atom or bond will generate a massive graph. Machine learning based on thousands of such graphs would be computationally inefficient. Instead, we construct a polymer graph from its repeat unit alone and propose that additional information (e.g., molecular weight, end groups, etc.)-if available be concatenated to each computed atom or bond fingerprint and/or to the learned polymer fingerprint.

Ideally, our learned polymer fingerprint must respect the invariances present in a poly- mer repeat unit. We identify three key transformations-translation, addition, and subtrac- tion-that repeat units of infinite 2D polymer chains are invariant to. We define translation as the movement of the periodicity window, which can produce periodic repeat units that are all equivalent. For example, (-OCC-), (-COC-), and (-CCO-) are equivalent repeat units of polyethylene glycol, related to one another by translation. We define addition (sub- traction) as the extension (reduction) of a repeat unit by one or more minimal repeat units. For example, (-COCO-) and (-COCOCO-) are equivalent repeat units, related to one another by the addition (or subtraction) of their minimal repeat unit, (-CO-). We have constructed polyGNN to be invariant under such transformations, as discussed below.

The polyGNN architecture is composed of three main modules: an Encoder for processing the repeat unit, a Message Passing Block for fingerprinting, and an Estimator to co-learn multiple properties. In the polyGNN Encoder, bonds are added between heavy atoms at the boundary of any input repeat unit, forming a periodic polymer graph (as shown in Fig. 1). This ensures that the graph of the repeat unit, and hence its learned fingerprint, is invariant to translation. Then, each atom and bond in the graph are given initial feature vectors (described later in Section 2.3) that are computed using RDKit.52 The featurized graph is passed to the Message Passing Block and then to the aggregation function. In the Message Passing Block, the initial feature vectors are passed between neighboring atoms. This information flow is the mechanism by which rich polymer features are learned (described later in Section 2.4).

7

After message passing, the sequence of learned atomic fingerprints is aggregated into a single polymer fingerprint by taking the mean. Taking the mean rather than the sum ensures that, for example, (-COCO-) and (-COCOCO-) are mapped to the same fingerprint. However, there are polymers (see Fig. 3A) where the desired invariance is not preserved. These conflicts arise because RDKit treats periodic polymer graphs as cyclic molecules. To address these conflicts, we propose two approaches. In the first approach, which we will continue to refer to as polyGNN, the original training data set is augmented with trans- formed repeat units (see Fig. 3B). Thus, although polyGNN is not invariant to addition or subtraction in these complicated cases, it achieves approximate invariance after learning from augmented data. This choice was inspired by state-of-the art image classification mod- els, which are trained using cropped and flipped images. 53 In this work, we find that data augmentation is also effective for training polyGNNs but does increase training time-a one- time cost. As an alternative, we created a variant, polyGNN2, with guaranteed invariance to addition and subtraction (and thus no need for augmentation). Invariance is achieved by modifying the Encoder to compute features on an extended polymer graph instead of on the periodic graph (see Section S2). However, operating on the extended graph notably slows fingerprinting in polyGNN2, and so we instead focus on polyGNN in what follows.

## 2.3 Fingerprinting graphs



The node features used in this work are element type, node degree, implicit valence, formal charge, number of radical electrons, hybridization, aromaticity (i.e., whether or not a given node is part of an aromatic ring), and number of hydrogen atoms. The edge features are bond type, conjugation (i.e., whether or not a given edge is part of a conjugated system), and ring member (i.e., whether or not a given edge is part of a ring).

8

## A



Repeat unit

C

f

C

C

:selected: C

C

Periodic graph

B

Original data point

Augmented points

(0)

(0)

C :selected:

:selected: C

C (0)

(0)

(1)

C

(I) C :selected:

C

(1)

C

C (I)

C

(1) (1)



|Repeat unit|Property value (arb. units)|
|---|---|
||3.14159|
|C C|Copied from above|
|C C n|Copied from above|


Figure 3: Overview of data set augmentation. (A) Two equivalent repeat units of infinite polyacetylene and their corresponding periodic graphs. Each atom in the graph is labeled with a zero if the atom is aliphatic or labeled with a one if the atom is aromatic. Other atomic features and all bond features are not shown for visual clarity. (B) Data augmentation strategy for polyGNN. Rows of the original training data are transformed by repeat unit addition.

## 2.4 Neural message passing



In GNNs, "messages" between neighboring atoms in a graph are iteratively passed along chemical bonds. After each iteration, every atom fingerprint is updated using the messages. In this way, atoms learn about their local neighborhood over time. By fitting parameters (e.g., weights and biases) in the model, the information contained in each message is opti- mized for the task at hand. This process is captured by three general but abstract equations presented in Section S3. In this section, for concreteness, we will demonstrate message passing using a highly simplified example.

First, consider the graph of infinite polyethylene glycol (PEG), shown in Fig. 1. We restrict our initial atom features to the element type and our initial bond features to the bond type. Thus, all edge fingerprints on the PEG graph are set to [1, 0, 0, 0] (indicating the

9

C

:selected:

presence of single bonds and no double, triple, or aromatic bonds). The two carbon atoms in PEG are initialized with a fingerprint of [1, 0] (indicating the presence of C atoms and not O atoms). We index these two nodes 0 and 1. The oxygen atom, with index 2, in PEG is initialized with a fingerprint of [0, 1]. Now, we compute messages mi,j between all pairs of chemically bonded atoms using the functional form :unselected:

mi,j = ReLU (Wo x x(0), c), e.]

where i, j are atom indices, xi is an initial atom fingerprint, and ei,j is a bond fin- gerprint. Note that, for simplicity, we ignore bias terms and use the Rectified Linear Unit (ReLU) activation in this example. Wø is a matrix of parameters. Before training, the pa- rameters are randomly initialized. During training, the parameters are iteratively updated (i.e., learned) using some flavor of stochastic gradient descent. In this example, our choice of initial parameters will be guided by mathematical convenience, and we do not consider subsequent weight updates. Choosing

Wø =



||10101000|||||||
|---|---|---|---|---|---|---|---|
||10101000|||||||
||10101000|||||||
||10101000|||||||
||10101000|||||||
||10101000|||||||
||10101000|||||||
||10101000||||||L|


,

gives us

10

m0,1 = m1,0 = [3, 3, 3, 3, 3, 3, 3, 3] m0,2=m1,2 = [2,2, 2, 2, 2, 2, 2, 2]

m2,0 = m2,1 = [2, 2, 2, 2, 2, 2, 2, 2].

Now, these messages can be used to update the fingerprint of each atom using the func- tional form

(1) = ReLU Wx X x; , 2mij]')

where Wy is a matrix of parameters, and j takes on values corresponding to atoms that share a chemical bond with atom i. After we conveniently initialize Wx to a 2 x 10 all-ones matrix, we have

x( )= [41,41]

x11) = [41, 41]

.(1) x2 = [33, 33].

So, by exchanging messages with neighbors, the fingerprint of each carbon atom in PEG was updated from [1, 0] to [41, 41] and the fingerprint of each oxygen atom was updated from [0, 1] to [33, 33]. The effect of message passing is clear. Initially, the oxygen atom was not aware of neighboring carbon atoms (that is, x2,2 = 0, where xi,1 is the lth dimension of xi). However, after passing one round of messages, the oxygen atom becomes aware of its carbonaceous neighbors (i.e., x2.2 0). Likewise, the carbon atoms become aware of their neighboring oxygen atom over time.

11

## 3 Results and Discussion



## 3.1 Benchmarking speed



polyGNN was developed with a primary objective in mind: to increase the rate at which large libraries of polymers may be screened. We quantified this rate by measuring the time needed to fingerprint a data set of 13,338 known polymers on a variety of different capacities and hardware. Capacity, as used in this work, is a hyperparameter that specifies both the number of message passing steps and the depth of each multilayer perceptron (MLP) in the network.

The timings to compute 13,388 polymer fingerprints with a randomly initialized polyGNN model are given in Fig. 4. A shallow polyGNN (with a capacity of two) fingerprints the set of polymers in 32 seconds (2.4 ms per polymer) on one CPU or 30 seconds (2.2 ms per polymer) on one GPU. Meanwhile, a deep polyGNN (with a capacity of 12) takes 57 seconds (4.3 ms per polymer) to compute the fingerprint set on one CPU or 32 seconds on one GPU. For each of the above, the time spent on the Encoder was fixed at 26 seconds. The remaining time was spent on the Message Passing Block which, unlike the Encoder, can run on CPUs or graphics processing units (GPUs).

By extrapolation, this means that fingerprinting a library of 1 billion polymers would take 26 days in the best case (shallow polyGNN run on a GPU) and 47 days in the worst case (deep polyGNN run on one CPU). Meanwhile, at a rate of 125.4 ms per polymer, fingerprinting a library of 1 billion polymers would take nearly 4 years on one CPU using the handcrafted PG approach. Of course, the rates for either approach can be further sped up with parallelization and/or increased random access memory.

## 3.2 Benchmarking accuracy



Here we evaluate the predictive accuracy of polyGNN models on 34 of the 36 properties in our data set; dielectric constant at 107 and 109 Hz (67 and 69) were excluded because

12

125 -

100

1 10 7

ms

polymer

8

1679s

Capacity

2

12

6

7

4 -

2

32s (52x)

57s (29x)

30s (56x)

32s (52x)

0

Handcrafted PG

polyGNN, CPU

polyGNN, GPU

Figure 4: Fingerprint time as a function of method, capacity, and hardware. Fingerprint time t, measured in milliseconds per polymer, is plotted on the y-axis. t was computed using a diverse set of 13,388 polymers. Above each bar is the total time (in plain text) in seconds taken to compute fingerprints for the entire set as well as the speed up (in parentheses) relative to the handcrafted PG method. t varies depending on the fingerprint method, the hardware, and the model capacity. Method and hardware are labelled on the x-axis; CPU and GPU refer to one Intel® Xeon® Gold 6140 CPU core and to one 32 GB Nvidia® Tesla® V100-PCIE GPU, respectively. Capacity is denoted by bar color.

our corpus contains fewer than 50 data points for these properties. Data for the remaining properties was randomly cut into a training and a test set in a 4 : 1 ratio. Three such random cuts were performed per property, so that statistics (e.g., standard deviation) of model performance could be computed.

Kuenneth et al.5 showed that multitask learning significantly improves the accuracy of polymer property prediction, relative to single task learning. Thus, we train single task (ST) and multitask (MT) polyGNNs and compare both on the same data. As a benchmark, we also train both ST and MT "PG-MLPs" (i.e., MLPs that use the handcrafted PG fingerprint as input; see Section S4 for details on this architecture). A detailed discussion of our training procedure can be found in Section S5. The root-mean-squared-error (RMSE) and R2 values of polyGNN and PG-MLP are compared in Tables 1 and S1.

We note several observations from these results. First, our data augmentation strategy plays a critical role in teaching polyGNN models invariance to addition and subtraction (see Table S2). Second, we find that MT learning is an important component of our approach, especially in low data situations. As shown in Table S1, polyGNNs that do not use MT

13

learning exhibit erroneous predictions (i.e., negative R2 value) for five properties-Ei, €1.78, €2, 65, 66 each with 158 or fewer data points. In contrast, with MT learning, polyGNNs exhibit positive R2 for each of the 34 properties studied.

Third, we find that polyGNNs tend to exbihit better or comparable accuracy than PG- MLPs, especially when the number of training data points is greater than 300. For the 14 properties containing more than 300 data points, each MT polyGNN model is either more accurate than or comparably accurate to its corresponding MT PG-MLP model (we define two models as having comparable accuracy for a property if the difference in average RMSE of their predictions is within 5% of that property's standard deviation o, see Table S3 for a complete list of o values). However, for the 20 properties containing 300 data points or less, the situation becomes more complex. MT polyGNN models still perform well relative to the MT PG-MLP benchmark, but not for every property. MT polyGNN models are more or comparably accurate for 16 properties, but are notably less accurate on four properties (experimental crystallization tendency Xe, €1.78, €2, 66).

The relatively low performance on these four properties could be explained by the fact that the polyGNN models trained here struggle to learn the block- or chain-level features (which typically consist of 4+ atoms) present in the handcrafted PG fingerprint. In prin- ciple, increasing the number of message passing steps-so as to capture larger length scale features-should mitigate this challenge. In practice, however, we observe a threshold num- ber of message passing steps. Above three message passing steps, model generalization only worsens-regardless of the property of interest. This empirical observation has been re- ported by others and is due to a collapse in which the learned fingerprints of all polymers, even chemically distinct ones, converge. 54,55 However, as evidenced by the impressive perfor- mance of the MT polyGNN models on a vast majority of properties, the inability to learn block- or chain-level features features is often ameliorated by the ability to learn lower-level features that go beyond those currently present in the handcrafted PG fingerprint. Still, the development of techniques that encourage GNNs to surpass the message passing threshold

14

Table 1: Average RMSE plus/minus one standard deviation on unseen test data.y



|Property|MT polyGNN|MT PG-MLP|ST polyGNN|ST PG-MLP|
|---|---|---|---|---|
|入*|0.0547 ± 0.0103|0.0630 ± 0.0082|0.0580 ± 0.0096|0.0663 ± 0.0201|
|Tm|45.0 ± 1.8|47.2 ± 2.2|55.3 ± 2.8|53.1 ± 1.3|
|Ta|58.7 ±3.3|59.3 ± 2.0|67.7 ± 3.2|71.9 ± 6.9|
|To|31.7 ± 1.5|34.0 ± 0.9|36.6 ± 1.0|35.5 ± 1.6|
|Eat *|0.114 ± 0.071|0.284 ± 0.089|0.0913 ± 0.0224|0.155 ± 0.040|
|Co *|0.172 ± 0.033|0.223 ± 0.085|0.171 ± 0.019|0.161 ± 0.030|
|O ;*|8.99 ± 1.01|9.77 ± 1.57|8.79 ± 0.46|8.63 ± 0.47|
|X *|15.0 ± 3.7|13.1 ± 4.6|15.8 ± 3.9|17.1 ± 5.1|
|Vf*|0.0380 ± 0.0191|0.0423 ± 0.0216|0.0330 ± 0.0182|0.0373 ± 0.0215|
|Xc|16.6 ± 1.3|17.4± 2.5|18.6 ± 1.9|19.1 ± 2.2|
|ρ|0.0640 ± 0.0053|0.0937 ± 0.0025|0.0627 ± 0.0015|0.385 ± 0.264|
|Ea *|0.380 ± 0.034|0.483 ± 0.148|0.341 ± 0.055|0.357 ± 0.107|
|E ;*|0.540 ± 0.170|0.678 ± 0.231|59.9 ± 102.5|0.676 ± 0.139|
|Egb *|0.468 ± 0.066|0.535 ± 0.123|0.716 ± 0.164|0.737 ± 0.058|
|Egc|0.445 ± 0.018|0.491 ± 0.033|0.442 ± 0.020|0.494 ± 0.026|
|En *|0.285 ± 0.101|0.284 ± 0.061|0.362 ± 0.086|0.252 ± 0.014|
|* €1.78|0.427 ± 0.042|0.328 ± 0.067|1.34 ± 0.30|0.988 ± 0.517|
|E*|0.478 ± 0.228|0.376 ± 0.257|2.67 ± 2.78|0.937 ± 0.201|
|60 *|0.621 ± 0.250|0.806 ± 0.338|1.39 ± 0.21|1.42 ± 0.22|
|Eg*|0.284 ± 0.018|0.252 ± 0.030|0.650 ± 0.108|0.602 ± 0.175|
|Er *|0.212 ± 0.023|0.243 ± 0.011|0.479 ± 0.266|0.658 ± 0.358|
|Er *|0.323 ± 0.075|0.274 ± 0.034|0.676 ± 0.315|0.487 ± 0.214|
|€15|0.125 ± 0.015|0.145 ± 0.019|0.144 ± 0.021|0.171 ± 0.027|
|* nc|0.0507 ± 0.0186|0.0733 ± 0.0191|0.0933 ± 0.0304|0.0957 ± 0.0251|
|ne|0.0413 ± 0.0023|0.0437 ± 0.0090|0.0540 ± 0.0087|0.0760 ± 0.0262|
|Y|0.827 ± 0.099|0.760 ± 0.169|0.877 ± 0.074|0.860 ± 0.196|
|Ots|23.3 ± 5.5|22.2 ± 3.9|28.1 ± 4.6|25.8 ± 3.9|
|ds*|1.15 ± 0.11|2.11 ± 0.10|1.65 ± 0.33|1.36 ± 0.09|
|WHO *|0.133 ± 0.017|0.111 ± 0.014|0.265 ± 0.065|0.246 ± 0.011|
|* μH2|0.127 ± 0.006|0.104 ± 0.011|0.287 ± 0.013|0.367 ± 0.034|
|μCO2|0.166 ± 0.015|0.161 ± 0.019|0.430 ± 0.025|0.525 ± 0.212|
|μCH4|0.132 ± 0.024|0.113 ± 0.023|0.366 ± 0.030|0.397 ± 0.006|
|μN2|0.124 ± 0.011|0.109 ± 0.018|0.410 ± 0.104|0.397 ± 0.038|
|μO2|0.139 ± 0.014|0.114 ± 0.004|0.399 ± 0.062|1.83 ± 2.46|


+ Starred properties contain 300 or fewer data points. Models with the best, or comparable with the best, average RMSE are bolded. The unit of each RMSE value matches those listed in Fig. 2a; for example, the RMSE of the MT polyGNN approach on Tg is 31.7 + 1.5 K.

15

is a critical next step. We leave this task for future work.

## 4 Summary and Outlook



In summary, we have produced polyGNN-the first-ever protocol that integrates polymer feature learning from SMILES strings and other relevant features, invariant transformations, data augmentation and multitask learning. Through careful comparison, we show that our protocol culminates in ultrafast polymer fingerprinting and accurate property prediction over the most comprehensive array of chemistries and properties studied to date. The gains in speed are essential when screening large candidate sets (e.g., millions or billions of polymers) and/or when computational resources are limited. Our approach is especially accurate when the data set size is moderate to large. Even with data sets containing less than 300 points, our approach is at least competitive with presently adopted methods in a majority of cases.

Looking ahead, though polyGNNs perform remarkably well in the experiments tried here, handcrafted polymer fingerprints have advantages. In tasks where chain- or block- level features are essential, handcrafted fingerprinting approaches may yield the best model accuracy. Advances in the optimization of graph neural networks are needed to make the accuracy of polyGNNs competitive in these tasks. Finally, a handcrafted feature is, by definition, interpretable. In contrast, the features learned by the polyGNNs presented here are not interpretable. Following the work of others, 56 future polyGNN architectures may incorporate attention mechanisms for partial interpretability. However, the interpretability of polyGNN features at the level of handcrafted features will require further innovation. Despite these shortcomings, we anticipate that the adoption of polyGNNs and related approaches will increase as they unlock the ability to screen truly massive polymer libraries at scale.

16

## 5 Public Use



The sources of data used in this work and the availability of each source is reported in the pa- per. The code used to train our polyGNN models is available at github. com/Ramprasad-Group/polygnn for academic use.

## Acknowledgement



This work was financially supported by the Office of Naval Research through a Multi- University Research Initiative (MURI) grant (N00014-17-1-2656), the Center for Understand- ing and Control of Acid Gas Induced Evolution of Materials for Energy (UNCAGE ME, an Energy Frontier Research Center) funded by the U.S. Department of Energy (DOE) un- der Award # DE-SC0012577, and by the National Science Foundation under grant 1941029. C.K. thanks the Alexander von Humboldt Foundation for financial support. R.G. is the main architect of the machine learning models and wrote this paper. C.K. and A.T. supported in the development and debugging of the machine learning models. The work was conceived and guided by R.R. All authors discussed results and commented on the manuscript.

## References



(1) Baldwin, A. F. et al. Poly(dimethyltin glutarate) as a Prospective Material for High Dielectric Applications. Advanced Materials 2015, 27, 346-351.

(2) Mannodi-Kanakkithodi, A .; Chandrasekaran, A .; Kim, C .; Huan, T. D .; Pilania, G .; Botu, V .; Ramprasad, R. Scoping the polymer genome: A roadmap for rational polymer dielectrics design and beyond. Materials Today 2018, 21, 785-796.

(3) Hu, Y .; Zhao, W .; Wang, L .; Lin, J .; Du, L. Machine-Learning-Assisted Design of

17

Highly Tough Thermosetting Polymers. ACS Applied Materials and Interfaces 2022, 14, 55016.

(4) Doan Tran, H .; Kim, C .; Chen, L .; Chandrasekaran, A .; Batra, R .; Venkatram, S .; Kamal, D .; Lightstone, J. P .; Gurnani, R .; Shetty, P .; Ramprasad, M .; Laws, J .; Shel- ton, M .; Ramprasad, R. Machine-learning predictions of polymer properties with Poly- mer Genome. Journal of Applied Physics 2020, 128, 171104.

(5) Kuenneth, C .; Rajan, A. C .; Tran, H .; Chen, L .; Kim, C .; Ramprasad, R. Polymer informatics with multi-task learning. Patterns 2021, 2.

(6) Barnett, J. W .; Bilchak, C. R .; Wang, Y .; Benicewicz, B. C .; Murdock, L. A .; Bereau, T .; Kumar, S. K. Designing exceptional gas-separation polymer membranes using machine learning. Science Advances 2020, 6, eaaz4301.

(7) Patel, R. A .; Borca, C. H .; Webb, M. A. Featurization strategies for polymer sequence or composition design by machine learning. Molecular Systems Design & Engineering 2022, 7, 661-676.

(8) Ma, R .; Luo, T. PI1M: A benchmark database for polymer informatics. Journal of Chemical Information and Modeling 2020, 60, 4684-4690.

(9) Ruddigkeit, L .; Van Deursen, R .; Blum, L. C .; Reymond, J. L. Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17. Journal of Chemical Information and Modeling 2012, 52, 2864-2875.

(10) Franceschetti, A .; Zunger, A. The inverse band-structure problem of finding an atomic configuration with given electronic properties. Nature 1999 402:6757 1999, 402, 60-63.

(11) Batra, R .; Song, L .; Ramprasad, R. Emerging materials intelligence ecosystems pro- pelled by machine learning. Nature Reviews Materials 2020, 1-24.

18

(12) Gurnani, R .; Kamal, D .; Tran, H .; Sahu, H .; Scharm, K .; Ashraf, U .; Ramprasad, R. polyG2G: A Novel Machine Learning Algorithm Applied to the Generative Design of Polymer Dielectrics. Chemistry of Materials 2021, 33, 7008-7016.

(13) Batra, R .; Dai, H .; Huan, T. D .; Chen, L .; Kim, C .; Gutekunst, W. R .; Song, L .; Ram- prasad, R. Polymers for Extreme Conditions Designed Using Syntax-Directed Varia- tional Autoencoders. Chemistry of Materials 2020, 32, 10489-10500.

(14) Kim, C .; Batra, R .; Chen, L .; Tran, H .; Ramprasad, R. Polymer design using genetic algorithm and machine learning. Computational Materials Science 2021, 186, 110067.

(15) Yao, Z .; Sánchez-Lengeling, B .; Bobbitt, N. S .; Bucior, B. J .; Kumar, S. G. H .; Collins, S. P .; Burns, T .; Woo, T. K .; Farha, O. K .; Snurr, R. Q .; Aspuru-Guzik, A. In- verse design of nanoporous crystalline reticular materials with deep generative models. Nature Machine Intelligence 2021, 3, 76-86.

(16) Zunger, A. Inverse design in search of materials with target functionalities. Nature Reviews Chemistry 2018 2:4 2018, 2, 1-16.

(17) Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. Journal of Chemical Information and Computer Sciences 2002, 28, 31-36.

(18) Lin, T. S .; Coley, C. W .; Mochigase, H .; Beech, H. K .; Wang, W .; Wang, Z .; Woods, E .; Craig, S. L .; Johnson, J. A .; Kalow, J. A .; Jensen, K. F .; Olsen, B. D. BigSMILES: A Structurally-Based Line Notation for Describing Macromolecules. ACS Central Science 2019, 5, 1523-1531.

(19) Chen, G .; Tao, L .; Li, Y. Predicting polymers glass transition temperature by a chemical language processing model. Polymers 2021, 13, 1-14.

19

(20) Gilmer, J .; Schoenholz, S. S .; Riley, P. F .; Vinyals, O .; Dahl, G. E. Neural Message Passing for Quantum Chemistry. 34th International Conference on Machine Learning, ICML 2017 2017, 3, 2053-2070.

(21) Schütt, K. T .; Kindermans, P. J .; Sauceda, H. E .; Chmiela, S .; Tkatchenko, A .; Müller, K. R. SchNet: A continuous-filter convolutional neural network for model- ing quantum interactions. Advances in Neural Information Processing Systems 2017, 2017-December, 992-1002.

(22) Jørgensen, P. B .; Jacobsen, K. W .; Schmidt, M. N. Neural Message Passing with Edge Updates for Predicting Properties of Molecules and Materials. arXiv:1806.03146 2018,

(23) Hy, T. S .; Trivedi, S .; Pan, H .; Anderson, B. M .; Kondor, R. Predicting molecular properties with covariant compositional networks. The Journal of Chemical Physics 2018, 148, 241745.

(24) Zhang, S .; Liu, Y .; Xie, L. Molecular Mechanics-Driven Graph Neural Network with Multiplex Graph for Molecular Structures. arXiv:2011.07457 2020,

(25) Ramakrishnan, R .; Dral, P. O .; Rupp, M .; Von Lilienfeld, O. A. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data 2014 1:1 2014, 1, 1-7.

(26) Lecun, Y .; Bengio, Y .; Hinton, G. Deep learning. Nature 2015 521:7553 2015, 521, 436-444.

(27) Vaswani, A .; Shazeer, N .; Parmar, N .; Uszkoreit, J .; Jones, L .; Gomez, A. N .; Kaiser, .; Polosukhin, I. Attention Is All You Need. Advances in Neural Information Processing Systems 2017, 2017-December, 5999-6009.

(28) Caruana, R .; Pratt, L .; Thrun, S. Multitask Learning. Machine Learning 1997 28:1 1997, 28, 41-75.

20

(29) Brenner, J. S. et al. Sports specialization and intensive training in young athletes. Pediatrics 2016, 138.

(30) Jørgensen, P. B .; Mesta, M .; Shil, S .; García Lastra, J. M .; Jacobsen, K. W .; Thyge- sen, K. S .; Schmidt, M. N. Machine learning-based screening of complex molecules for polymer solar cells. Journal of Chemical Physics 2018, 148.

(31) Zeng, M .; Kumar, J. N .; Zeng, Z .; Savitha, R .; Chandrasekhar, V. R .; Hippal- gaonkar, K. Graph Convolutional Neural Networks for Polymers Property Prediction. arXiv:1811.06231 2018,

(32) St John, P. C .; Phillips, C .; Kemper, T. W .; Wilson, A. N .; Guan, Y .; Crowley, M. F .; Nimlos, M. R .; Larsen, R. E. Message-passing neural networks for high-throughput polymer screening. Journal of Chemical Physics 2019, 150.

(33) Hatakeyama-Sato, K .; Tezuka, T .; Umeki, M .; Oyaizu, K. AI-Assisted Exploration of Superionic Glass-Type Li+ Conductors with Aromatic Structures. Journal of the American Chemical Society 2020, 142, 3301-3305.

(34) Mohapatra, S .; An, J .; Gómez-Bombarelli, R. Chemistry-informed macromolecule graph representation for similarity computation, unsupervised and supervised learn- ing. Machine Learning: Science and Technology 2022, 3, 015028.

(35) Aldeghi, M .; Coley, C. W. A graph representation of molecular ensembles for polymer property prediction. Chemical Science 2022, 13, 10486-10498.

(36) Antoniuk, E. R .; Li, P .; Kailkhura, B .; Hiszpanski, A. M. Representing Polymers as Periodic Graphs with Learned Descriptors for Accurate Polymer Property Predictions. Journal of Chemical Information and Modeling 2022,

(37) Gurnani, R .; Yu, Z .; Kim, C .; Sholl, D. S .; Ramprasad, R. Interpretable Machine

21

Learning-Based Predictions of Methane Uptake Isotherms in MetalOrganic Frame- works. Chemistry of Materials 2021, 33, 3543-3552.

(38) Huan, T. D .; Mannodi-Kanakkithodi, A .; Ramprasad, R. Accelerated materials prop- erty predictions and design using motif-based fingerprints. Physical Review B - Con- densed Matter and Materials Physics 2015, 92, 014106.

(39) Huan, T. D .; Mannodi-Kanakkithodi, A .; Kim, C .; Sharma, V .; Pilania, G .; Ram- prasad, R. A polymer dataset for accelerated property prediction and design. Scientific Data 2016 3:1 2016, 3, 1-10.

(40) Sharma, V .; Wang, C .; Lorenzini, R. G .; Ma, R .; Zhu, Q .; Sinkovits, D. W .; Pilania, G .; Oganov, A. R .; Kumar, S .; Sotzing, G. A .; Boggs, S. A .; Ramprasad, R. Rational design of all organic polymer dielectrics. Nature Communications 2014 5:1 2014, 5, 1-8.

(41) Park, J. Y .; Paul, D. R. Correlation and prediction of gas permeability in glassy poly- mer membrane materials via a modified free volume based group contribution method. Journal of Membrane Science 1997, 125, 23-39.

(42) Kim, C .; Chandrasekaran, A .; Huan, T. D .; Das, D .; Ramprasad, R. Polymer Genome: A Data-Powered Polymer Informatics Platform for Property Predictions. The Journal of Physical Chemistry C 2018, 122, 17575-17585.

(43) Zhu, G .; Kim, C .; Chandrasekarn, A .; Everett, J. D .; Ramprasad, R .; Lively, R. P. Polymer genome-based prediction of gas permeabilities in polymers. Journal of Polymer Engineering 2020, 40, 451-457.(44) Chen, L .; Kim, C .; Batra, R .; Lightstone, J. P .; Wu, C .; Li, Z .; Deshmukh, A. A .; Wang, Y .; Tran, H. D .; Vashishta, P .; Sotzing, G. A .; Cao, Y .; Ramprasad, R. Frequency-dependent dielectric constant prediction of polymers using machine learning. npj Computational Materials 2020 6:1 2020, 6, 1-9.

22

(45) Lightstone, J. P .; Chen, L .; Kim, C .; Batra, R .; Ramprasad, R. Refractive index pre- diction models for polymers using machine learning. Journal of Applied Physics 2020, 127.

(46) Venkatram, S .; Batra, R .; Chen, L .; Kim, C .; Shelton, M .; Ramprasad, R. Predict- ing Crystallization Tendency of Polymers Using Multifidelity Information Fusion and Machine Learning. Journal of Physical Chemistry B 2020, 124, 6046-6054.

(47) Brandrup, J .; Immergut, E. H .; Grulke, E. A. Polymer Handbook, 4th ed .; John Wiley & Sons: New York, 1999.

(48) Barton, A. F. M. CRC Handbook of Solubility Parameters and Other Cohesion Param- eters, 2nd ed .; Routledge, 2013; p 768.

(49) Bicerano, J. Prediction of Polymer Properties; Marcel Dekker, Inc .: New York, 2002.

(50) Otsuka, S .; Kuwajima, I .; Hosoya, J .; Xu, Y .; Yamazaki, M. PoLyInfo: Polymer Database for Polymeric Materials Design. 2011 International Conference on Emerg- ing Intelligent Data and Web Technologies (EIDWT). Tirana, 2011; pp 22-29.

(51) Crow Polymer Properties Database. https : //polymerdatabase. com/, accessed March 13, 2022.

(52) RDKit, Open Source Toolkit for Cheminformatics. https: //www.rdkit.org/, accessed March 13, 2022.

(53) He, K .; Zhang, X .; Ren, S .; Sun, J. Deep residual learning for image recognition. Pro- ceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2016, 2016-December, 770-778.

(54) Godwin, J .; Schaarschmidt, M .; Gaunt, A .; Sanchez-Gonzalez, A .; Rubanova, Y .; Veličković, P .; Kirkpatrick, J .; Battaglia, P. Simple GNN Regularisation for 3D Molec- ular Property Prediction & Beyond. arXiv:2106.07971 2021,

23

(55) Chen, D .; Lin, Y .; Li, W .; Li, P .; Zhou, J .; Sun, X. Measuring and Relieving the Over- smoothing Problem for Graph Neural Networks from the Topological View. AAAI 2020 - 34th AAAI Conference on Artificial Intelligence 2019, 3438-3445.

(56) Veličković, P .; Casanova, A .; Liò, P .; Cucurull, G .; Romero, A .; Bengio, Y. Graph Attention Networks. 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings 2017,

(57) Gurnani, R. Debugging Neural Networks. 2021; https://nanohub.org/resources/ netdebugger.

(58) Glorot, X .; Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelli- gence and Statistics. Chia Laguna Resort, Sardinia, Italy, 2010; pp 249-256.## Supporting Information



## S1 Data breakdown



Our data set includes the permeability u of six gases g E {He, H2, CO2, 02, N2, CH4}. The number of data points per gas is: 281 for He, 288 for H2, 342 for CO2, 380 for CH4, 431 for N2, and 436 for O2. Our experimental dielectric constant Er data contains measurements at nine frequencies f € {1.78, 2, 3, 4, 5, 6, 7, 9, 15} in log10Hz. The number of data points per frequency is: 51 for 101.78 Hz, 77 for 102 Hz, 172 for 103 Hz, 124 for 104 Hz, 66 for 105 Hz, 158 for 106 Hz, 20 for 107 Hz, 12 for 109 Hz, 507 for 1015 Hz.

24

## S2 polyGNN2 Encoder



The input to the polyGNN2 Encoder is a repeat unit. From this repeating unit, the trimer graph (shown in Figure S1) is created. Then, the trimer graph is featurized. The atoms, bonds, and features corresponding to the central repeat unit of the trimer graph are used to create the periodic graph. Thus, the initial fingerprint of the periodic graph is always invariant to addition and subtraction, as shown for the example of polyacetylene in Figure S1. However, the polyGNN2 Encoder is slower than the polyGNN Encoder. While the polyGNN Encoder takes 26 seconds to featurize the graphs of 13 338 polymers, the polyGNN2 Encoder takes 37 seconds to featurize this set of polymer graphs.

Repeat unit

Trimer graph

Periodic graph

A

C

C

O

-

B

C

C C

C

of

n

(0)

(0)

C :selected:

C

*

C

C

(0)

(0)

(0)

(0)

C

C

C C (0)

(0)

(0)

A

C

C C (0)

*

C (0)

(0)

:selected: C

*

C

(0)

(0)

(0)

C

C

C

C

(0)

(0)

B

(0)

(0)

(0)

(0) (O)

(0)

C

C

C

C

C

*

C :selected:

C

C

C

C

C

C

(0)

(0)

(0) (0)

(0)

(0)

(0)

:selected:

(0)

A

C :selected:

(0)

(0)

B

C :selected:

C :unselected:

(0) C

C (0)

C :selected:

C :selected:

(0)

(0)

C

(0)

C :selected:

(0)

Figure S1: Two equivalent repeat units ("A" and "B", where "A" and "B" refer to four and six atom repeat units, respectively) of polyacetylene and their corresponding trimer and periodic graphs. Each repeat unit is converted to a trimer graph. Each node (i.e., heavy atom) in the trimer graph is featurized. Each atom is labeled with a zero if the atom is aliphatic or is labeled with a one if the atom is aromatic. Other atomic features and all bond features are not shown for visual clarity. The atoms, bonds, and features at the center of the trimer graph (shaded in yellow) are used to form the periodic graph.

25

## S3 polyGNN architecture



polyGNNs contain three modules: the Encoder, Message Passing Block, and the Estimator. The inputs to polyGNN are a polymer repeat unit and a property of interest (or, equivalently, the property's associated selector vector). The two outputs of a polyGNN model are the repeat unit's fingerprint and the value of the property of interest.

In the Encoder, the repeat unit is first converted to a periodic graph, with each atom as a node and each bond as an edge. Then, each node and edge in the graph are given an initial fingerprint. After the graph elements have been assigned their initial features, the graph is passed to the Message Passing Block. Here, "messages" between neighboring atoms are iteratively passed along chemical bonds. After each iteration, every node fingerprint is updated using the messages, while each bond fingerprint remains the same. The message passed from atom j to atom i at time step k is calculated according to Eq. 1.

mì. (k) = (k) (2(k), a(k), e;) (1)

where each ø(k) is a parameterized function, æ k) and æ k) are the encodings of neighboring atoms after time step k, and ei,j is the fingerprint of the bond that joins atoms i, j. mij (k) = 0 if i, j do not share a chemical bond. After initialization, each node receives messages from all of its neighbors. These messages are aggregated by some permutation-invariant function I (e.g., sum, mean, max). We use the sum in this work. The aggregated message, along with the current node encoding, is used to update the node encoding. The node update process is defined in Equation 2.

ac(k) = x(k) (xk-1), I({m;Vj € [1, Np]}) + x(k-2)

(2)

where each x(k) is a parameterized function, p is a polymer, [1, Np] is the set of integers between 1 and Np, Np is the number of atoms in the repeat unit of p, and x;) (k) = 0, Vk < 0. Messages are passed for T time steps, where T is also the capacity in this work. The fingerprint

26

of the entire polymer, xp, is calculated by the graph aggregation function Ag, as shown in Eq. 3.

Xp = Ag(x), x ) = Np 1 i=1 Np

x E.

i Finally, xp and the selector s can be passed to the Estimator. Here, these inputs are mapped to some polymer property prediction, yp, via a parameterized function y. We implement v as a multilayer perceptron.

(3)

## yp= 4(xp, s)



(4)

During training, the parameters of all ((k), (k), v are learned simultaneously. As shown in Eq. 2, our update step leverages skip connections, which have been shown to improve the optimization of shallow layers in deep neural networks. 53

## S4 Handcrafted PG models



The handcrafted PG models are made up of five MLP submodels (see Section S4), trained using five-fold cross-validation. The input to each MLP is the handcrafted PG fingerprint of a given polymer repeat unit and a property selector and the output is the predicted property value.

## S5 Training procedure



Each of the models discussed in the main text are ensemble models, composed of several submodels. The output of the ensemble is computed by a simple average of each submodel's output. For multitask ensembles, data for all properties within a group were combined, target values were scaled, and selectors were assigned as described in Section 2.1 of the main text. For single-task ensembles, properties were not combined into groups, MinMax scaling

27

was not performed, and selectors were set to empty vectors. Next, to train and evaluate the model, the data was split according to the schematic in Figure S2 described below.

Full data set

O

Tune capacity Training set

## 2



HP training set

Tune remaining HPs

## Repeat 3x



3

CV

val |

Fold I: Train model I of ensemble

CV val 2

Fold 2: Train model 2 of ensemble

Folds 3-5: Train models 3-5 of ensemble

Test set

HP val set

4

Evaluate ensemble on the Test set generated in Step I.

Figure S2: The various steps in our training and evaluation protocol. This protocol involved three runs, with three rounds of data splitting per run. All splits are random. "HP" stands for hyperparameter, "val" stands for validation, and "CV" stands for cross-validation.

First, the entire data set for each property was randomly cut into 80% training, 20% test splits three times. All subsequent steps were performed for each training-test set pair. Using the NNDebugger package, 57 the optimal capacity was found by attempting to overfit the entire training data set. The data set was considered overfit if the R2 value was greater than 0.97. If the data was not overfit, then the capacity corresponding to the highest R2 value was used. The capacity range considered was between two and fourteen. The training data set was then divided into an 80% hyperparameter (HP) training set and a 20% HP validation

28

set. The remaining HPs (batch size, learning rate, dropout percentage) were optimized using the package scikit-optimize. The set of HPs corresponding to the lowest RMSE on the HP validation set was considered optimal.

Finally, the training data set was split into five folds using cross-validation (CV), pro- ducing one CV train data set and one CV validation data set per fold. For each fold, the model's HPs were fixed as the optimal HPs and the model's learnable parameters were fit to the CV train data set for 1000 epochs. At the end of 1000 epochs, the model parameters corresponding to the epoch with the lowest RMSE in the CV validation data set were chosen. After all five models were trained on their respective CV splits, the models were placed in an ensemble. The ensemble was used to make predictions of the test set, so far completely unseen by the ensemble during HP optimization or model training with CV.

All neural network architectures used dropout layers, fully connected layers, and Leaky ReLU activations (with a negative slope equal to 0.01). All architectures were created using PyTorch and/or PyTorch Geometric. The weights of all models were optimized using the Adam optimizer and the mean squared error loss function. All weights were initialized according to a Xavier uniform distribution58 with a gain of one. All biases were initialized using the default PyTorch setting.

## S6 Extended Results



We computed both the root-mean-squared-error (RMSE) and Pearson correlation coefficient (R2) of each trained ensemble on unseen data for each property. The RMSE values are tabulated in the main body and the R2 values are tabulated in Table S1.

As discussed in the main body, we desire that the final output of each polyGNN model is approximately invariant to addition and subtraction. In other words, the variance in predictions between a set of equivalent repeat units should be low. As shown in Table S2, we find that our proposed data set augmentation does lead to a significant decrease in prediction

29

variance a majority of the time.

Table S2 contains the average variance of models trained with and without augmentation, var augment and var no augment, respectively, on each of the 36 properties studied in this work. We define

var

1

P

PEP var(f(x), Vx E Ep)

where P is a set of non-equivalent repeat units, var is the variance function, f is a machine learning model, and Ep is a set of repeat units related to p (a repeat unit in P) by addition or subtraction. In this work, P is a set of 9 repeat units not seen by any model during training, and each Ep is composed of p, 2p, 3p, 4p and 5p. For example, if p is (-C-) then Ep = {(-C-), (-CC-), (-CCC-), (-CCCC-), (-CCCCC-)}.

Table S3 lists standard deviations of the data in our corpus, grouped by property.

30

Table S1: Average R2 plus/minus one standard deviation on unseen test data. Starred properties contain 300 or fewer data points.



|Property|MT polyGNN|MT handcrafted PG|ST polyGNN|ST handcrafted PG|
|---|---|---|---|---|
|入*|0.106 ± 0.245|-0.177 ± 0.205|0.002 ± 0.209|-0.367 ± 0.678|
|Tm|0.840 ± 0.019|0.824 ± 0.023|0.757 ± 0.032|0.777 ± 0.018|
|Ta|0.746 ± 0.019|0.741 ± 0.021|0.662 ± 0.019|0.618 ± 0.059|
|Tg|0.913 ± 0.004|0.901 ± 0.004|0.885 ± 0.011|0.892 ± 0.004|
|Eat *|0.933 ± 0.057|0.614 ± 0.107|0.959 ± 0.011|0.879 ± 0.058|
|C0 *|0.729 ± 0.135|0.541 ± 0.280|0.734 ± 0.125|0.757 ± 0.142|
|O ;*|0.561 ± 0.092|0.475 ± 0.166|0.581 ± 0.057|0.596 ± 0.058|
|X *|0.227 ± 0.112|0.432 ± 0.137|0.143 ± 0.133|0.018 ± 0.210|
|Vf*|0.380 ± 0.319|0.259 ± 0.355|0.534 ± 0.265|0.417 ± 0.351|
|Xc|0.519 ± 0.025|0.479 ± 0.078|0.397 ± 0.073|0.368 ± 0.072|
|ρ|0.894 ± 0.022|0.777 ± 0.034|0.900 ± 0.014|-4.126 ± 4.386|
|E *|0.738 ± 0.114|0.613 ± 0.105|0.781 ± 0.108|0.782 ± 0.079|
|E ;*|0.786 ± 0.156|0.659 ± 0.263|-6553.467 ± 11352.055|0.681 ± 0.162|
|Egb|0.928 ± 0.035|0.908 ± 0.040|0.839 ± 0.067|0.828 ± 0.046|
|Egc|0.915 ± 0.002|0.896 ± 0.014|0.916 ± 0.005|0.895 ± 0.006|
|E0*|0.557 ± 0.358|0.652 ± 0.088|0.354 ± 0.404|0.683 ± 0.197|
|*。 €1.78|0.842 ± 0.102|0.896 ± 0.088|-0.494 ± 0.851|0.224 ± 0.503|
|Eg *|0.750 ± 0.252|0.807 ± 0.255|-12.141 ± 21.000|0.166 ± 0.302|
|Ex *|0.819 ± 0.144|0.704 ± 0.199|0.203 ± 0.125|0.168 ± 0.116|
|* £4|0.861 ± 0.071|0.888 ± 0.062|0.346 ± 0.178|0.441 ± 0.232|
|Er *|0.797 ± 0.088|0.720 ± 0.149|-0.553 ± 2.096|-1.899 ± 3.879|
|66 *|0.519 ± 0.219|0.670 ± 0.078|-0.913 ± 1.112|0.013 ± 0.531|
|€15|0.860 ± 0.042|0.813 ± 0.060|0.815 ± 0.066|0.742 ± 0.084|
|nc *|0.874 ± 0.061|0.737 ± 0.083|0.543 ± 0.307|0.532 ± 0.217|
|ne|0.850 ± 0.020|0.826 ± 0.085|0.738 ± 0.083|0.437 ± 0.417|
|Y|0.501 ± 0.107|0.587 ± 0.094|0.428 ± 0.176|0.480 ± 0.073|
|Its|0.638 ± 0.212|0.675 ± 0.155|0.499 ± 0.173|0.576 ± 0.152|
|10 * S|0.770 ± 0.074|0.235 ± 0.180|0.536 ± 0.156|0.686 ± 0.045|
|* μHe|0.969 ± 0.002|0.978 ± 0.007|0.877 ± 0.037|0.891 ± 0.023|
|* μH2|0.983 ± 0.002|0.988 ± 0.002|0.914 ± 0.018|0.857 ± 0.041|
|μCo2|0.980 ± 0.006|0.981 ± 0.006|0.866 ± 0.027|0.779 ± 0.174|
|μCH4|0.986 ± 0.005|0.990 ± 0.004|0.897 ± 0.029|0.881 ± 0.009|
|μN2|0.985 ± 0.003|0.988 ± 0.003|0.833 ± 0.070|0.844 ± 0.034|
|μO2|0.981 ± 0.003|0.987 ± 0.002|0.845 ± 0.028|-6.798 ± 13.235|


31

Table S2: The average variance of models trained with and without augmentation. The unit of each property is given in Table S3.



|Property|var no augment|var augment|var no augment var augment|
|---|---|---|---|
|Ea|0.0237|0.0086|2.763|
|Eat|0.0055|0.0014|3.840|
|E;|0.0144|0.0123|1.171|
|Egb|0.0332|0.0177|1.874|
|Eg|0.0414|0.0310|1.338|
|€0|0.0016|0.0021|0.7769|
|nc|0.0009|0.0020|0.4282|
|Cp|0.0040|0.0062|0.6490|
|Ots|0.0046|0.0011|4.318|
|Tg|0.0088|0.0009|9.969|
|Tm|0.0088|0.0018|4.807|
|Y|0.0049|0.0009|5.313|
|Xe|0.0214|0.0145|1.480|
|Xc|0.0186|0.0083|2.225|
|€1.78|0.0010|0.0010|0.9407|
|€15|0.0031|0.0018|1.765|
|€2|0.0014|0.0009|1.631|
|€3|0.0010|0.0005|2.156|
|€4|0.0010|0.0012|0.8613|
|€5|0.0019|0.0014|1.432|
|€6|0.0010|0.0026|0.3841|
|€7|0.0011|0.0012|0.8877|
|Eg|0.0007|0.0011|0.6105|
|Vff|0.0070|0.0079|0.8951|
|O;|0.0029|0.0027|1.089|
|PCH4|0.0167|0.0093|1.788|
|MCO2|0.0155|0.0088|1.764|
|MH2|0.0165|0.0092|1.795|
|PHe|0.0162|0.0066|2.454|
|UN2|0.0178|0.0113|1.581|
|HO2|0.0190|0.0117|1.630|
|ne|0.0036|0.0021|1.699|
|ρ|0.0037|0.0022|1.690|
||0.0096|0.0073|1.330|
|λ|0.0079|0.0014|5.774|
|Ta|0.0123|0.0005|26.71|


32

Table S3: The standard deviation (o) of data in our corpus, grouped by property.



|Property|σ|
|---|---|
|λ|0.0653 W/mK|
|Tm|109.3 K|
|Ta|114.7 K|
|Tg|109.0 K|
|Eat|0.470 eV/atom|
|Cp|0.374 J/gK|
|O;|13.10 %|
|Xe|18.3 %|
|Vff|0.0477|
|Xc|23.7 %|
|ρ|0.1991 g/cc|
|Ea|0.777 eV|
|E;|1.101 eV|
|Egb|1.760 eV|
|Egc|1.561 eV|
|EO|0.726|
|€1.78|1.388|
|€2|1.331|
|€3|1.276|
|€4|0.991|
|€5|1.039|
|€6|0.854|
|€15|0.359|
|nc|0.1713|
|ne|0.1142|
|Y|1.401 MPa|
|Ots|40.9 MPa|
||2.64 vMPa
:selected:|
|PHe|0.806 Barrer|
|PH2|0.993 Barrer|
|PCO2|1.207 Barrer|
|PCH4|1.050 Barrer|
|UN2|0.950 Barrer|
|PO2|1.018 Barrer|


