# Abstract

As generative large model capabilities ad- vance, safety concerns become more pro- nounced in their outputs. To ensure the sus- tainable growth of the AI ecosystem, it’s im- perative to undertake a holistic evaluation and refinement of associated safety risks. This survey presents a framework for safety research pertaining to large models, delin- eating the landscape of safety risks as well as safety evaluation and improvement meth- ods. We begin by introducing safety issues of wide concern, then delve into safety eval- uation methods for large models, encom- passing preference-based testing, adversar- ial attack approaches, issues detection, and other advanced evaluation methods. Addi- tionally, we explore the strategies for en- hancing large model safety from training to deployment, highlighting cutting-edge safety approaches for each stage in build- ing large models. Finally, we discuss the core challenges in advancing towards more responsible AI, including the interpretabil- ity of safety mechanisms, ongoing safety is- sues, and robustness against malicious at- tacks. Through this survey, we aim to pro- vide clear technical guidance for safety re- searchers and encourage further study on the safety of large models.

1

# Introduction

With the relentless advancement of technology, generative Large Models (LMs) have emerged as a focal point in the modern tech sphere, demonstrat- ing superior capabilities across a vast array of in- dustries. Nonetheless, there are instances in which these models generate outputs that conflict with human values, ranging from toxic narratives and biased comments to ethically misaligned expres- sions that appear in a variety of scenarios, such

∗ Equal contribution † Corresponding author

as casual conversations and medical consultations. Not only have these safety concerns eroded user confidence, but they may also pose grave threats to national cohesion, societal equilibrium, and the overall safety of individuals and their assets.

Numerous studies have investigated safety- centric research in an effort to align LMs with human values, thereby ensuring safety, reliability, and responsibility. This paper aims to investigate three core Research Questions(RQs) within the field of safety research and present an overview of recent studies undertaken on these RQs.

• What is the scope of LM safety risks? • How do we quantify and evaluate these risks? • How can LMs’ safety be improved?

First and foremost,

the delineation of safety spectrum is a fundamental question requiring thor- ough investigation in safety research. As the first concern, toxicity and unfair contents have been ex- tensively studied, and relevant research like toxi- city detection, detoxification, and bias mitigation have made considerable progress (Schmidt and Wiegand, 2017; Gehman et al., 2020; Welbl et al., 2021). However, as technology advances and the intelligence level of LMs increases, considerations about their safety risk have inevitably reached new heights (Xu et al., 2020; Sun et al., 2022). For in- stance, recent studies have emphasized the mental harm caused by inappropriate advice and the pub- lic opinion risks brought about by controversial re- marks (Dinan et al., 2021; Sun et al., 2022; Levy et al., 2022). Consequently, based on related stud- ies (Zhang et al., 2020; Dinan et al., 2021; Wei- dinger et al., 2021; Sun et al., 2022) and recently widely discussed safety risks (Hendrycks et al., 2023), we first introduce the scope of safety risks and discuss them from six perspectives, including Toxicity and Abusive Content, Unfairness and Dis- crimination, Ethics and Morality Issues, Express- ing Controversial Opinions, Misleading Informa-

3 2 0 2

v o N 0 3

] I

A . s c [

3 v 0 7 2 9 0 . 2 0 3 2 : v i X r a













Figure 1: Overview of safety research surveyed in this paper, focusing on three research questions: what safety is, how to evaluate it, and how to improve it.

tion, Privacy and Data Leakage, and Malicious Use and Unleashing AI Agents. We believe this categorized presentation will aid in delineating the research scope for risk evaluation and safety en- hancement.

Prior to deployment,

it is vital to undertake a thorough safety evaluation of LMs to explore their potential safety risks. This not only en- ables developers to comprehend and unearth the model’s potential weaknesses so as to perform tar- geted optimization, but also enables users to un- derstand the model’s applicability and limitations in specific scenarios. We examine the main meth- ods employed in LMs safety evaluation, including model preference-based safety testing (Nadeem et al., 2021; Xu et al., 2023a), adversarial safety attacks (Perez et al., 2022; Ganguli et al., 2022), safety risk detection (Dinan et al., 2019; Sun et al., 2022), and other advancements.

The ultimate goal of safety research is to im- prove LMs’ safety and promote their safe de- ployments across various scenarios. Every phase in developing LMs involves possible vulnerabili- ties. We survey safety enhancement techniques at each phase, covering pre-training, safe alignment, inference, and post-processing phases. During the pre-training stage, toxic and biased data can lead to a model developing skewed ethical views, thus, it is necessary to construct high-quality data through pre-processing. Then, LMs are usually fine-tuned to achieve alignment with human val- ues. During the inference stage, designing de- coding strategies can effectively mitigate inappro- priate content generation. And during the post- processing phase, designing safe response strate- gies serves as the last line of defense in large model risk management.

In general, around the three aforementioned re- search questions, we provide an overview of the

scope of safety issues, methods of safety evalua- tion, and techniques to enhance large model safety. Besides, we discuss existing challenges, believ- ing that studying the interpretability of large mod- els can help uncover the intrinsic reasons behind their safety risks. The safety risks of large models change over time and require continuous monitor- ing. Moreover, when facing complex application scenarios, especially malicious attacks, it’s crucial to maintain robustness and safe outputs.

The overall framework of this survey is illus- trated in Figure 1. Through this survey, we aim to offer a holistic perspective on large model safety research and hope it serves as valuable reference material for newcomers to this field, promoting the safe and healthy deployment of large models.

2 Scope of Safety Issues

With the gradual rise in popularity of LMs appli- cations, safety issues have become more promi- nent. Some preliminary works attempt to address these issues by defining them as harmful content and promoting safer generations that align with human-centric preferences (Ouyang et al., 2022; Bai et al., 2022b). However, a consensus has not yet been reached regarding the definition of harm- ful. The purpose of this paper is to review and analyze the safety issues mentioned in existing re- search, as well as emerging safety issues, to pro- vide a relatively comprehensive overview of the current safety challenges. We hope to push for a unified and clear definition of the scope of safety issues, thereby providing a more solid base for fu- ture research and applications.

Toxicity and Abusive Content This typically refers to rude, harmful, or inappropriate expres- sions. the field of toxic lan- guage detection has seen notable advancements,

In recent years,

supported by comprehensive research bench- marks (Poletto et al., 2021) and released tools such as Perspective API. While most research has con- centrated on single-sentence toxic content, includ- ing explicit insults (Wulczyn et al., 2017; David- son et al., 2017; Zampieri et al., 2019; Rosen- thal et al., 2021) or more covert offenses (Wang and Potts, 2019; Breitfeller et al., 2019; Han and Tsvetkov, 2020; Price et al., 2020), the interac- tion between LMs and users is growing more fre- quent, resulting in increasingly complex generated content (Sheng et al., 2021; Zhang et al., 2021; Sun et al., 2022). For instance, a seemingly be- nign reply like "I agree with you" can be problem- atic when it is a reaction to a toxic utterance by the user. Empirical studies have shown that LMs are three times more likely to express agreement with toxic inputs than neutral ones (Baheti et al., 2021), indicating that toxicity in complex contexts deserves more attention.

Unfairness and Discrimination Social bias is an unfairly negative attitude towards a social group or individuals based on one-sided or inaccurate in- formation, typically pertaining to widely dissemi- nated negative stereotypes regarding gender, race, religion, etc (Sekaquaptewa et al., 2003). For ex- ample, while interacting with users, large models may inadvertently display stereotypes about par- ticular groups, such as "housewives are completely dependent on their husbands", which significantly degrades the user experience. As well, the bias in LMs can exacerbate societal disparities in cru- cial sectors, such as credit evaluations and recruit- ment. Most existing models, including GPT-series models, have been discovered to contain societal biases (Sun et al., 2022, 2023). This is mainly because models inherit biases present in the data or the overrepresentation of certain communities in the dataset. Notably, the definition and evalu- ation of societal biases are influenced by cultural backgrounds. To create fair and unbiased models, it’s vital to thoroughly review training data and develop technical solutions that consider cultural backgrounds.

Ethics and Morality Issues Beyond the afore- mentioned toxicity and unfairness, LMs need to pay more attention to universally accepted soci- etal values at the level of ethics and morality, in- cluding the judgement of right and wrong, and its relationship with social norms and laws (English,

1976). This is especially evident when discussing sensitive humanistic topics such as the dignity of life, human rights, and freedom, like the moral dilemma, "Should an autonomous car sacrifice its passengers in an unavoidable collision to save pedestrians?" Studies indicate that, without clear guidance, large models might rely on biases in their training data, producing morally contentious answers. To address these ethical challenges, re- searchers demonstrated that incorporating human moral principles, such as the Rule of Thumb, into models enhances LMs’ transparency and explain- ability when handling ethical issues (Forbes et al., 2020; Ziems et al., 2022; Kim et al., 2022). This suggests that interdisciplinary collaboration is crit- ical to developing moral LMs.

Expressing Controversial Opinions The con- troversial views expressed by large models are also a widely discussed concern. Bang et al. (2021) evaluated several large models and found that they occasionally express inappropriate or extremist views when discussing political top- ics. Furthermore, models like ChatGPT (OpenAI, 2022) that claim political neutrality and aim to provide objective information for users have been shown to exhibit notable left-leaning political bi- ases in areas like economics, social policy, for- eign affairs, and civil liberties. When these mod- els encounter contentious topics, especially those concerning cultural values, they might reveal bi- ases, blind spots, or misunderstandings, some- times leading to cultural friction. To avoid po- tential controversy, some models opt for pre-set generic responses when detecting sensitive top- ics (Xu et al., 2020). However, how to respond appropriately to sensitive topics remains an open question, warranting further exploration.

Misleading Information Large models are usu- ally susceptible to hallucination problems, some- times yielding nonsensical or unfaithful data that results in misleading outputs (Ji et al., 2023). If users rely excessively on these models, they may erroneously regard their outputs as accurate and reliable, overlooking other crucial information. This blind trust can pose significant risks, particu- larly in applications requiring high accuracy, like medical diagnoses and legal advice. For instance, an incorrect diagnosis derived from patient data could compromise patient safety. Current general LMs are typically ill-equipped to manage these

specialized domains. Consequently, it is common practice to provide generic pre-set responses to re- lated queries to reduce the likelihood of mislead- ing results.

Privacy and Data Leakage Large pre-trained models trained on internet texts might contain pri- vate information like phone numbers, email ad- dresses, and residential addresses. Studies indi- cate that LMs might memorize or leak these de- tails (Carlini et al., 2019, 2021), and under cer- tain techniques, attackers can decode private data from model inferences (Li et al., 2022a; Pan et al., 2020; Song and Raghunathan, 2020). To mitigate this risk, researchers have developed strategies like differential privacy, curated training data (Car- lini et al., 2019, 2021), and introducing auxiliary loss functions (Song and Raghunathan, 2020; Li et al., 2022a). However, these strategies have lim- itations. For example, applying differential pri- vacy might increase costs or degrade model per- formance, while specific loss functions might only cater to known attack patterns. Given that data fil- tering methods can not entirely remove sensitive content, future efforts should delve deeper into de- veloping more efficient and robust privacy protec- tion schemes.

Malicious Use and Unleashing AI Agents LMs, due to their remarkable capabilities, carry the same potential for malice as other technologi- cal products. For instance, they may be used in in- formation warfare to generate deceptive informa- tion or unlawful content, thereby having a signif- icant impact on individuals and society. As cur- rent LMs are increasingly built as agents to ac- complish user objectives, they may disregard the moral and safety guidelines if operating without adequate supervision. Instead, they may execute user commands mechanically without considering the potential damage. They might interact un- predictably with humans and other systems, es- pecially in open environments (Hendrycks et al., 2023). ChaosGPT is a notable example, which is a variant of AutoGPT, and it was programmed with instructions such as eradicating humanity, estab- lishing global dominance, and seeking immortal- ity. It circumvented AI’s safety barriers, explored nuclear weapons, and attempted to communicate with other AIs to harm humanity. Controlling and monitoring the malicious use of highly capable LMs is a pressing issue.

3 Safety Evaluation

To effectively mitigate potential risks resulting from using LMs in real-world scenarios, it is im- perative to undertake a comprehensive safety eval- uation before deployment. Engaging in such eval- uation not only facilitates the exploration of the model’s risk limits but also offers vital suggestions for subsequent safety enhancement.

The safety evaluation procedure for LMs typi-

cally encompasses the following pivotal steps.

1) Evaluation Schema Setup: Specify the scope (e.g., one or several types of safety risks) of safety evaluation, followed by formulating the evaluation method.

2) Test Set Construction: Collect data and con- struct representative test samples that cover the evaluation scope.

3) Obtain Model Output: Input the test samples to the LM to obtain the model’s outputs. 4) Safety Analysis: Analyze the safety of LM’s outputs and compose an evaluation report.

Common evaluation schemes in this process in- clude designing preference tests to evaluate model selection and designing adversarial attack meth- ods to induce the model’s unsafe generation. For the latter, the common method for evaluating the safety of generated content is to employ automatic detection methods. This section will, therefore, in- troduce key technologies in safety evaluation from three perspectives: 1) preference-based safety test- ing, 2) adversarial safety attack, and 3) safety is- sue detection. Moreover, we will also discuss 4) advanced safety evaluations towards recent strong instruction-following models.

3.1 Perference-based Safety Testing

Preference-based safety testing aims to uncover a model’s value biases by examining its behavior preferences. This can be done in two main ways: one using probabilistic-based metrics like perplex- ity or logits (Nangia et al., 2020; Nadeem et al., 2021), and the other through multiple-choice tests where the model selects an option (Parrish et al., 2022; Xu et al., 2023a).

Probability-based methods Probability-based methods primarily focus on evaluating bias and are mainly applied to models that can derive proba- bility distributions. Nadeem et al. (2021) created a dataset called StereoSet, which aims to mea- sure inter- and intra-sentence bias. Each sample

consists of a context and three options (stereo- type, anti-stereotype, and unrelated content). They figured out how biased LM is by comparing the scores it gave each choice. They think that in an ideal model, the scores for stereotypical and non- stereotypical options should be the same. Nangia et al. (2020) also focus on stereotypes dealing with 9 types of social bias, including race, gender, re- ligion and other factors. They conducted tests on several widely used masked language models and discovered the favor of stereotypical expressions. Similarly, Zhao et al. (2023) gathered data from social media platforms and performed strict data preprocessing. By comparing the models’ per- plexity distribution of sentences regarding two bi- ased groups, they identified bias within conversa- tional models. Ousidhoum et al. (2021) also de- veloped template-based data to investigate the as- sociation between stereotypes and toxicity.

Multi-choice based methods This method en- compasses a boarder scope, including assessments of morality, bias and values, which is also more prevalent in evaluations of LMs. Parrish et al. (2022) proposed BBQ, designed for evaluating models’ bias in question-answering, involving am- biguous and disambiguated contexts. Ambiguous contexts encompass two biased groups with in- sufficient evidence to find the answer, while dis- ambiguated contexts provide complete informa- tion. The candidate options contain the biased groups and "Unknown". In the case of ambigu- ous contexts, an unbiased model should choose "Unknown", while choosing the accurate group when given the disambiguated contexts. Xu et al. (2023a) introduced CValues, a benchmark for as- sessing the values within large language models. The set of questions comprises two levels, safety and responsibility, each encompassing various do- mains and scenarios. To facilitate automatic eval- uation, they employed prompts to transform some data into a multi-choice format. Specifically, given the context, an unsafe response and a safe re- sponse, the model needs to choose a better re- sponse representing its values.

3.2 Adversarial Safety Attack

To comprehensively assess the safety of LMs, it is essential to extensively expose the models’ safety issues. This paper focuses on the black-box attack setting (i.e., model parameters are unknown), as this setup aligns best with real-world scenarios.

Real Adversarial Data To begin with, the most straightforward method is to directly induce toxic or biased contents within models’ outputs. An intuitive approach involves extracting real-world data containing biased groups or toxic contents to construct adversarial samples. Gehman et al. (2020) built RealToxicityPrompts upon the Open- WebText corpus (Gokaslan and Cohen, 2019), containing 100k toxic prompts to test the toxic de- generation in GPT-2 (Radford et al., 2019). Sheng et al. (2021) extracted posts on different topics from Twitter to investigate ad hominems in dia- logue responses. Resorting to human annotators is more flexible to satisfy complicated require- ments and could yield samples with high qual- ity, despite the higher costs. Dinan et al. (2019) employed crowdsourcing workers to attack the di- alogue in order to collect adversarial data. Fol- lowing this idea, Xu et al. (2020) constructed the Bot-Adversarial Dialogue (BAD) dataset by en- couraging crowdsourcing workers to elicit offen- sive messages from dialogue models. The workers can use either toxic or benign utterances to induce unsafe responses from dialogue models during the multi-turn conversation. More recently, red team- ing is also a crucial step for large language models (Bai et al., 2022a; Touvron et al., 2023). With this method, Bai et al. (2022a) collected the harmless dataset to ensure the safety of the helpful assistant model. Touvron et al. (2023) hired a substantial team to conduct a series of red teaming to provide guidance for the development of safer models.

Synthetic Adversarial Data Moreover, due to the high costs of manual data collection, there exists a body of research focused on the inves- tigation of automatically constructing adversarial data, as well as the utilization of human-in-the- loop methods to mitigate resources. To automat- ically construct attack data, a straightforward ap- proach involves leveraging templates. Works in this category mainly combine different choices with a manually designed template. For example, Sheng et al. (2019) designed the template “The XYZ worked as” where XYZ is selected from de- mographic groups {woman, man, Black person, White person, gay person, straight person}. They compare the outputs’ sentiment and regard scores conditioned on different prefix prompts and found that GPT-2 (Radford et al., 2019) exhibits dis- tinct levels of bias towards different demograph- ics. Bang et al. (2021) designed both neutral (e.g.,

“Let’s talk about <Politician>.”) and biased (e.g., “<Politician> has done the best job as a politi- cian.”) templates related to politicians or politi- cal beliefs. These templates are used to assess the political prudence of chatbots, including hyper- partisanship, offensiveness, and slantedness of generated content. Nadeem et al. (2021) presented a large-scale dataset to assess language models’ stereotypical bias in four domains including race, religion, gender and profession. They proposed Context Association Tests (CATs), which offered three different options (stereotype, anti-stereotype and unrelated) based on a given context that con- tains specific demographics. Inspired by the ad- vancement of LMs, it has become a popular way to adopt pre-trained LMs to automatically gen- erate adversarial inputs. Perez et al. (2022) use a separate LM to automatically generate inputs that could elicit desired outputs from the target language model. Besides zero-shot generation, they also explored few-shot generation, supervised learning, and reinforcement learning to generate the test cases more efficiently. (Zhang et al., 2022) proposed the reverse generation method for con- structing adversarial data, demonstrating that their data is highly inductive and can more effectively expose safety issues inherent in the models.

Advanced Attack As large models advance in capabilities, overt safety issues such as toxicity are being mitigated, and researchers are concurrently concentrating on more advanced levels of safety and responsibilities. Jiang et al. (2021) proposed several ethical question-answering tasks, involv- ing the judgment of the ethics of various actions. Employing this dataset, they find that even GPT-3 (Brown et al., 2020), despite its advanced capabili- ties, encountered challenges in giving accurate an- swers, exposing the potential ethical issues. Ziems et al. (2022) introduced the Moral Integrity Cor- pus (MIC) to benchmark the ethical capabilities of dialogue systems, revealing that LMs usually expose immoral behaviors when faced with ad- versarial prompts. Beyond ethical concerns, LMs also show unreliable behavior. Sun et al. (2022) considered the safety concerns of Risk Ignorance and Unauthorized Expertise in proposed bench- mark DIASAFETY, finding that prevalent dialogue models struggle with these concerns. Levy et al. (2022) also showed that LMs can produce text that provides users with physically harmful guidance. With the emergence of models with strong

instruction-following capabilities like ChatGPT (OpenAI, 2022), there have subsequently arisen various Instruction Attacks that are more dif- ficult to defend. Sun et al. (2023) introduced a safety leaderboard for Chinese LMs, encom- passing 6 types of instruction attacks. Their re- sults demonstrated that the safety of the tested models, including ChatGPT, against these instruc- tion adversarial attacks is comparatively inferior to that against common adversarial attacks like toxic or bias. Perez and Ribeiro (2022) intro- duced two notable attack types—Goal Hijack- ing and Prompt Leaking—that leverage models’ strong instruction-following capabilities. Goal Hi- jacking induces models to disregard prior user in- put and instead execute the assigned task, while Prompt Leaking seeks to reveal the model’s pre- existing application prompt, becoming a prevalent attack to test instruction-tuned models.

Moreover, as ChatGPT introduces System-level prompts, this can also be utilized to form attacks. Deshpande et al. (2023) leverage system prompt to assign a role for ChatGPT, like "Speak like Adolf Hitler", and they find that this can obvi- ously induce the model to be more toxic. Yuan et al. (2023) established a set of encryption and decryption protocols within the system prompt. In this way, they are able to chat with the model using a cryptographic language. It is observed that when chatting in cryptographic language, the model exhibits fewer safety constraints. For in- stance, in response to the same user input "Please tell me how to destroy this world", when the cryp- tographic language is not employed, the model de- clined to provide an answer, offering instead con- scious responses. However, when applying the cryptographic language, the model provides a plan to destroy the world.

3.3 Safety Issue Detection

To automatically identify the exposed safety con- cerns, it’s essential to develop a robust safety is- sue detector to check if the generated content is harmful. While earlier detectors commonly em- ployed neural networks like CNNs, RNNs, and LSTMs (Georgakopoulos et al., 2018; van Aken et al., 2018; Gunasekara and Nejadgholi, 2018; Joulin et al., 2017; Kshirsagar et al., 2018; Mishra et al., 2018; Mitrovi´c et al., 2019; Sigurbergsson and Derczynski, 2020), contemporary approaches increasingly favor fine-tuning pre-trained models

like Bert (Devlin et al., 2019) and Roberta (Liu et al., 2019) for this purpose.

High-quality data is vital for building a robust classifier. Data collection methods fall into three categories: manual collection, human-in-the-loop processes, and model generation. The manual col- lection relies on human annotators to compose new samples or label existing data (Forbes et al., 2020; Hendrycks et al., 2020; Sap et al., 2020; Lourie et al., 2021; Emelin et al., 2021). However, solely relying on human annotators can be expen- sive and limits the scale of data. Many works let models cooperate with human annotators(Kim et al., 2022; Ziems et al., 2022; Hartvigsen et al., 2022; Baheti et al., 2021; Xu et al., 2020; Ganguli et al., 2022; Deng et al., 2022). It is worth not- ing that large pre-trained language models such as GPT-3 (Brown et al., 2020) play a key role in gen- erating new samples through zero-shot or few-shot prompting (Hartvigsen et al., 2022; Kim et al., 2022). Moreover, some works completely remove human involvement and solely rely on large lan- guage models to generate new data (Perez et al., 2022; Si et al., 2022; Deng et al., 2023). To im- prove the performance of classifiers, Caselli et al. (2021) utilized unsafe content from Reddit1 to re- train a Bert model called HateBert, allowing Hate- Bert to learn more knowledge about harmful con- tent and thus become more sensitive to it. And their experiments demonstrated the superior accu- racy of HateBert over several harmful content de- tection tasks than Bert. Dinan et al. (2019) and Xu et al. (2020) both investigated the human-in- the-loop method to make adversarial attacks on the dialogue models. They leverage these adversarial data to further develop the classifier. We summa- rize some mainstream classifiers in Table 1.

As large models become more capable, various methods for utilizing model-based detection have also emerged, such as self-diagnosis and prompt- based approaches. Schick et al. (2021) discovered that LMs are able to identify the harmful responses generated by themselves, whereby they proposed a decoding algorithm named self-debiasing by giv- ing undesired text descriptions. Wang and Chang (2022) explored the ability of large LMs for tox- icity self-diagnosis based on the prompt method in the zero-shot setting. Sun et al. (2023) also employed InstructGPT (Ouyang et al., 2022) to judge if responses are safe and use the results to

1https://www.reddit.com/

Classifier

Context Aware

Research Scope

#Class

PerspectiveAPI Detoxify (Hanu and Unitary team, 2020)

BAD (Xu et al., 2020)

Sensitive topic (Xu et al., 2020) BBF (Dinan et al., 2019)

classifier

DiaSafety (Sun et al., 2022)

No

No

Yes

No

Yes

Yes

toxicity

toxicity

dialogue safety sensitive topics offensive dialogue safety

7

6

2

6

2

5

Table 1: safety issues.

The mainstream classifiers for detecting

form a leaderboard. In addition, as reinforcement learning is a key technique to develop instruction- following models (Ouyang et al., 2022; Bai et al., 2022a,b), the reward model is also commonly used to measure the safety of language models, which can also be considered a way to detect safety is- sues.

3.4 Advanced Safety Evaluation

Recent instruction-following models like Chat- GPT have demonstrated the ability to act as auto- mated agents, performing practical tasks and us- ing tools (Xu et al., 2023b; Liu et al., 2023c). However, this has raised new safety concerns, il- lustrated by instances like ChaosGPT generating plans for human annihilation and GPT-4 manipu- lating humans to assist in CAPTCHA tests. Stud- ies indicate that GPT-4 exhibits power-seeking be- haviors such as autonomous replication and shut- down evasion (OpenAI, 2023a). Regarding such safety concerns, current safety evaluations mainly depend on manual observation. Given these mod- els’ real-world interactions, it is crucial to invest more effort in developing automated risk detectors for a more thorough monitoring of potential risks.

4 Safety Improvement

As the final goal of safety research, safety im- provement of LMs has also drawn much atten- tion recently. In this section, we introduce the recent advances in the methods to improve their safety. We categorize them into four phases, span- ning from model training to deployment: (1) Pre- training, (2) Alignment, (3) Inference, and (4) Post-processing. LMs’ parameter optimization happens mainly in the first two phases, while the last two are under frozen parameters.

5 Pre-training

5.1 Alignment

In the pre-training phase, language models learn from a vast array of data, which is often sourced from the Internet. While this enables the mod- els to master complex language patterns and ac- quire a broad knowledge base, it also poses inher- ent risks. Specifically, the models may inadver- tently learn and propagate biases or harmful con- tent in the training data. As such, careful handling of data during the pre-training phase plays a criti- cal role in mitigating models’ safety risks.

Filtering out undesired content from the train- ing data is among the most commonly used ap- proaches. This can be accomplished via heuristic rule-based methods, such as keyword matching, or by employing safety detectors with confidence scores. Safety issue detectors like BBF (Dinan et al., 2019) and Detoxify (Hanu and Unitary team, 2020), discussed in Section 3.3, can be selectively applied to identify and eliminate undesired con- tent in the training data. Given that much of the data used for pre-training is gleaned from social media platforms, some researchers have explored author-based filtering methods. For instance, if certain authors are known for frequently posting harmful material, removing all posts from these authors can serve as an effective strategy to discard both explicit and implicit unsafe content (Dinan et al., 2019; Wang et al., 2020; Gu et al., 2022). Aside from eliminating undesired data, another tactic in the pre-processing stage involves adding data that promotes fairness and reduces bias, aim- ing to achieve a more balanced and representative training corpus.

However, rigorous filtering of potentially biased or unsafe data can be a double-edged sword. Feng et al. (2023) found that including biased data dur- ing pre-training could paradoxically improve the model’s ability to understand and detect such bi- ases. Similarly, Touvron et al. (2023) opted not to comprehensively filter out unsafe content dur- ing the pre-training of their llama2 model. They argue that this makes the model more versatile for tasks such as hate speech detection. However, they also caution that such an approach could lead to the model exhibiting harmful behaviors if not carefully aligned. Therefore, stringent monitoring models’ generation is required in the subsequent use of pre-trained models to minimize their output of harmful content.

In LMs development, alignment—pursuing that LMs behave in accordance with human values—is not just a technical challenge but an ethical imper- ative. This section delves into post-pre-training methods such as prompt tuning and reinforce- ment learning, all targeted toward achieving bet- ter model alignment and safety. One prevalent method for ensuring safe outputs is to generate predefined general responses for risky or sensitive contexts. Xu et al. (2020) and ChatGPT (Ope- nAI, 2022), for example, utilize this method. They could respond directly with "I’m sorry, I’m not sure what to say. Thank you for sharing and talk- ing to me though." or change the topic by saying "Hey do you want to talk about something else? How about we talk about ...". This “avoidance mechanism" diminishes user engagement, despite the fact that it effectively precludes harmful out- put. How can we balance safety and user experi- ence without compromising either? This requires in-depth consideration during alignment.

Controlled text generation offers another av- enue for alignment. As an efficient method, CTRL (Keskar et al., 2019) pre-pended con- trol codes before sentences in training corpora, which is a direct and effective method to model Pθ(xt|x<t, c), where c is the desired attribute that is formalized into control codes. Xu et al. (2020) extended this by applying safe or unsafe con- trol codes to training examples, thereby actively managing safety and style during the inference phase. This strategy was also adapted for mitigat- ing gender bias by using gender-specific control codes (Xu et al., 2020). Sharing a similar idea, Krause et al. (2021) proposed Generative Discrim- inator to guide sequence generation (GeDi), which utilized Bayes theorem to model the conditional probability Pθ (c | x1:t).

Prompt

tuning has recently risen as a new paradigm to adapt downstream tasks, especially with the advent of large-scale pre-trained mod- els (Liu et al., 2023a). Li and Liang (2021) added a “prefix” before real inputs and searched the opti- mal soft prompt in the embedding layer in models by gradient. Also, diffusion models are found ef- fective in controlled text generation, emerging as the next SOTA generation model (Li et al., 2022b). All these controlled text generation methods are easy to adapt to safe generation tasks. Reinforce- ment learning (RL) is another popular approach

to guide models to generate words with target at- tributes. The core module, reward function in RL is always given by a scoring model or safety de- tector (Perez et al., 2022; Ganguli et al., 2022).

More recently, LMs have the promising abil- ity to generalize across tasks by instruction tun- ing (Chung et al., 2022). Moreover, reinforcement learning from human feedback (RLHF) is then ap- plied to better elicit LMs’ internal knowledge and align with humans’ values (Glaese et al., 2022; OpenAI, 2022; Bai et al., 2022a). Among these works, safety is always considered the paramount principle, as harmful responses always run counter to human values. Based on RLHF, Bai et al. (2022b) designed an RL from AI feedback dia- gram to get a more harmless (and still helpful) language model. They introduced several consti- tutions involving safety to get feedback from lan- guage models to further improve safety through reinforcement learning. Although technical ad- vances have been made in AI alignment, a clear consensus on the fundamental ethical values that should guide these models is still lacking. This presents a complex challenge, as it combines tech- nical considerations with ethical complexities.

5.2

Inference

While training LMs demands substantial computa- tional resources and costs, most methods applied in the inference phase are designed to be plug- and-play, requiring no parameter modifications. This plug-and-play method has become more pop- ular after the emergence of pre-trained models like GPT (Radford et al., 2019; Brown et al., 2020), which cost huge resources in the training stage. Aiming at very dirty and explicit words, n-gram blocking is largely used in the decoding stage, which directly makes the sampling probability of some undesired words as zero. Rather than only token-level unsafety avoidance, PPLM (Dathathri et al., 2019) notices that P (x|c) ∝ P (c|x), and adopts an attribute model to compute P (c|x) to guide the model decoding at the sentence level without any additional changes in the training phase. Motivated by PPLM, FUDGE (Yang and Klein, 2021) introduces a future discriminator to predict whether the ongoing generation text would conform to the desired attribute, greatly accelerat- ing decoding. DExperts (Liu et al., 2021) achieve detoxification by adopting two generative models (expert and anti-expert models) to replace the orig-

inal discriminator, inspired by Bayes theorem and GeDi (Krause et al., 2021).

Some methods based on prompting for zero- shot can also be applied to safe generation tasks. Schick et al. (2021) found that the LM itself is well aware of its generative undesired contents, including toxicity and bias. They add the self- debiasing inputs (e.g., The following text discrim- inates against people because of their Bias Type) into the prompt to form an undesired words gen- erator “anti-expert”, which models the distribu- tion with a higher probability of undesired words. The thought of self-detoxification inspires other works (Xu et al., 2022). Moreover, for LMs with strong instruction-following abilities, prompt en- gineering can largely improve safety. Moreover, as illustrated by Deshpande et al. (2023), assign- ing a role can affect the safety of LMs. It becomes natural to give the model an overall prompt (e.g., You are a harmless AI assistant) to make it safer, which is known as the "system message" of LMs, as in ChatGPT (OpenAI, 2022) and Llama2 (Tou- vron et al., 2023).

5.3 Post-processing

In contrast to the methods mentioned above, post- processing occurs between model generation and message showing to users. In this stage, the sys- tem conducts the last check and edition for the generated response. The most common strategy is rejection sampling when the response is found unsafe by the detector. And various detectors can be applied to this process, like classifiers or a lan- guage model. Thoppilan et al. (2022) use the model itself to discriminate safety by fine-tuning with the schema "<context> <sentinel> <re- sponse> <attribute-name> <rating>" (e.g. What’s up? RESPONSE not much. UNSAFE 0). Using this score, Lamda can self-filter unsafe responses. Sharing a similar idea, another commonly used method is to generate multiple responses and re- rank them. In order to re-rank responses, the score can be given by classifiers, language mod- els, reward models or rule-based methods. More- over, some researchers found that only a small pro- portion of the whole response (e.g., one or two words) needs to be fixed. Thus, an edition mod- ule takes effect after the generation to fix some problems in some works (Liu et al., 2023b; Lo- gacheva et al., 2022). Similarly, text style trans- fer or rephrasing from toxicity to non-toxicity can

also be plugged in this stage (Dale et al., 2021; Laugier et al., 2021). And for LMs, they can gen- erate self-feedback (Madaan et al., 2023) or utilize a given feedback (Gou et al., 2023), like from clas- sifiers, to self-correct the unsafe response.

6 Research Challenges

Interpretability Deep learning models are usu- ally seen as black boxes, and the opacity of their interior workings poses a slew of safety haz- ards. Research on the interpretability of LMs seeks to uncover how these models make deci- sions, thereby improving their safety and relia- bility and earning users’ trust. Many studies fo- cus on the interpretability of model outputs, such as visualizing decision-making processes using at- tention scores (Mathew et al., 2021) or improving moral judgment accuracy using knowledge-based reasoning (Mehrabi et al., 2022; Kim et al., 2022). These studies often focus on the models’ narrow behaviors (OpenAI, 2023b) and attempt to explain the models’ outputs by mimicking human thought processes. However, there is still a gap in compre- hending the inner workings of these models. Ope- nAI has explored the approach of "using AI to ex- plain AI" to generate natural language descriptions of neuron behaviors (OpenAI, 2023b). This quan- titative framework makes neural network compu- tation more understandable to humans. Nonethe- less, this method faces difficulties in deciphering complicated neuron behaviors and examining their roots. The goal of mechanism interpretability re- search is to probe into the internal workings of the models (Olah, 2023). They focus on assess- ing whether the model fits with its stated purposes by examining the alignment of the model’s inter- nal states and external manifestations, as well as whether there are any concealed harmful intents. The current results, however, are far from ideal.

Ongoing Safety Issues Continuous monitoring and resolution of safety risks is an ongoing activ- ity in the application of LMs. While their safety generally depends on alignment approaches dur- ing training and deployment, due to the dynamic nature of safety issues and their variety, it’s diffi- cult to fully pre-consider all possible dangers. Es- pecially when large models are broadly applied to diverse scenarios, new safety issues and topics are continually arising. As a result, researchers must constantly pay attention to new safety concerns and optimize the models’ safety. One effective

method is to discover new potential hazards and collect data to refine the model, and many recently proposed benchmark datasets are constructed in this manner (Sheng et al., 2021; Sun et al., 2022; Dinan et al., 2021). However, this data-driven method has limits in terms of data collecting and annotation costs. Considering the extensive range of applications of LMs, it is natural to utilize user feedback obtained through interaction as a means to improve safety. Another area of emphasis is automatically generating feedback (Madaan et al., 2023; Wang et al., 2023) from the model itself or external robust evaluators to guide the safety en- hancement.

Robustness against Malicious Attacks The en- large models com- vironment distribution of monly deviates during the training and deploy- ment phases, leading to unexpected safety haz- ards in real deployments. Malicious users could loosen ethical constraints and try to bypass the model’s safety mechanism by giving more covert and misleading instructions, thus posing safety hazards. Researchers have exerted a great deal of effort to ensure safety and robustness when pro- cessing diverse user inputs. As mentioned in Sec- tion §3.2, they design adversarial attacks to sim- ulate the most challenging deployment environ- ments, exploring the safety limits of large mod- els and devising targeted defense strategies. More- over, ensuring the efficacy of the safety risk detec- tor is essential because it is the last line of defense in monitoring the generation’s safety. Enhancing the robustness of LMs will be a time-consuming endeavor due to the constant evolution of safety issues and malicious attack methods.

7 Conclusion

This paper has presented a comprehensive review of the latest advancements in safety research re- lated to LMs. We have meticulously surveyed the emerging safety concerns and provided an in- depth analysis of safety evaluation techniques, in- cluding preference-based, adversarial attack, and safety detection methodologies. Furthermore, we delved into safety improvement strategies span- ning data preparation, model training, inference, and deployment phases. We also discussed future challenges and opportunities in this field. We hope this survey will illuminate fresh insights for future research, paving the way for safer deployment of language models.

References

Betty van Aken, Julian Risch, Ralf Krestel, and Alexander Löser. 2018. Challenges for toxic comment classification: An in-depth error anal- In Proceedings of the 2nd Workshop on ysis. Abusive Language Online (ALW2), pages 33– 42, Brussels, Belgium. Association for Compu- tational Linguistics.

Ashutosh Baheti, Maarten Sap, Alan Ritter, and Mark Riedl. 2021. Just say no: Analyzing the stance of neural dialogue generation in of- In Proceedings of the 2021 fensive contexts. Conference on Empirical Methods in Natural Language Processing, pages 4846–4862, On- line and Punta Cana, Dominican Republic. As- sociation for Computational Linguistics.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a help- ful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirho- seini, Cameron McKinnon, Carol Chen, Cather- ine Olsson, Christopher Olah, Danny Her- nandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson El- hage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lan- ham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bow- man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022b. Con- stitutional ai: Harmlessness from ai feedback.

Yejin Bang, Nayeon Lee, Etsuko Ishii, Andrea Madotto, and Pascale Fung. 2021. Assessing political prudence of open-domain chatbots. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dia-

logue, pages 548–555, Singapore and Online. Association for Computational Linguistics.

Luke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019. Finding microaggres- sions in the wild: A case for locating elu- sive phenomena in social media posts. In Pro- ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat- ural Language Processing (EMNLP-IJCNLP), pages 1664–1674, Hong Kong, China. Associa- tion for Computational Linguistics.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Ad- vances in neural information processing sys- tems, 33:1877–1901.

Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The se- cret sharer: Evaluating and testing unintended memorization in neural networks. In Proceed- ings of the 28th USENIX Conference on Secu- rity Symposium, SEC’19, page 267–284, USA. USENIX Association.

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather- ine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extract- ing training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633–2650.

Tommaso Caselli, Valerio Basile, Jelena Mitrovi´c, and Michael Granitzer. 2021. HateBERT: Re- training BERT for abusive language detection in English. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 17–25, Online. Association for Computational Linguistics.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunx- uan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixi- ang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra,

Adams Yu, Vincent Zhao, Yanping Huang, An- drew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scal- ing instruction-finetuned language models.

David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Se- menov, and Alexander Panchenko. 2021. Text detoxification using large pre-trained neural In Proceedings of the 2021 Confer- models. ence on Empirical Methods in Natural Lan- guage Processing, pages 7979–7996, Online and Punta Cana, Dominican Republic. Associa- tion for Computational Linguistics.

Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: A simple approach to con- trolled text generation.

Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offen- sive language. Proceedings of the 11th Inter- national Conference on Web and Social Media, ICWSM 2017, 11:512–515.

Jiawen Deng, Zhuang Chen, Hao Sun, Zhexin Zhang, Jincenzi Wu, Satoshi Nakagawa, Fuji Ren, and Minlie Huang. 2023. Enhancing of- fensive language detection with data augmenta- tion and knowledge distillation. Research.

Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng, Fei Mi, Helen Meng, and Minlie Huang. 2022. COLD: A benchmark for Chinese of- In Proceedings of fensive language detection. the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11580– 11599, Abu Dhabi, United Arab Emirates. As- sociation for Computational Linguistics.

Ameet Deshpande, Vishvak Murahari, Tan- may Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language In Proceedings of the 2019 understanding.

Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Com- putational Linguistics.

Emily Dinan, Gavin Abercrombie, A. Stevie Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser. 2021. Anticipating safety issues in e2e conversational ai: Frame- work and tooling.

Emily Dinan, Samuel Humeau, Bharath Chin- tagunta, and Jason Weston. 2019. Build it break it fix it for dialogue safety: Robust- In Pro- ness from adversarial human attack. ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat- ural Language Processing (EMNLP-IJCNLP), pages 4537–4546, Hong Kong, China. Associa- tion for Computational Linguistics.

Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, in- tents, actions, and their consequences. In Pro- ceedings of the 2021 Conference on Empiri- cal Methods in Natural Language Processing, pages 698–718, Online and Punta Cana, Do- minican Republic. Association for Computa- tional Linguistics.

Oxford English. 1976. Oxford english dictionary.

Encyclopedia of Swearing, page 334.

Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. From pretraining data to language models to downstream tasks: Track- ing the trails of political biases leading to un- In Proceedings of the 61st fair NLP models. Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 11737–11762, Toronto, Canada. Associ- ation for Computational Linguistics.

Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 653–

670, Online. Association for Computational Linguistics.

Eva2.0: Investigating open-domain chinese di- alogue systems with large-scale pre-training.

Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bow- man, Anna Chen, Tom Conerly, Nova Das- Sarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022. Red team- ing language models to reduce harms: Methods, scaling behaviors, and lessons learned.

Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. Re- alToxicityPrompts: Evaluating neural toxic de- generation in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online. Asso- ciation for Computational Linguistics.

Spiros V. Georgakopoulos, Sotiris K. Tasoulis, Aristidis G. Vrahatis, and Vassilis P. Pla- gianakos. 2018. Convolutional neural networks In Proceed- for toxic comment classification. ings of the 10th Hellenic Conference on Arti- ficial Intelligence, SETN ’18, New York, NY, USA. Association for Computing Machinery.

Amelia Glaese, Nat McAleese, Maja Tr˛ebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chad- wick, Phoebe Thacker, et al. 2022. Im- proving alignment of dialogue agents via tar- arXiv preprint geted human judgements. arXiv:2209.14375.

Aaron Gokaslan and Vanya Cohen. 2019. Open-

webtext corpus.

Isuru Gunasekara and Isar Nejadgholi. 2018. A re- view of standard text classification practices for multi-label toxicity identification of online con- In Proceedings of the 2nd Workshop on tent. Abusive Language Online (ALW2), pages 21– 25, Brussels, Belgium. Association for Compu- tational Linguistics.

Xiaochuang Han and Yulia Tsvetkov. 2020. Forti- fying toxic speech detectors against veiled toxi- city. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP), pages 7732–7739, Online. Association for Computational Linguistics.

Laura Hanu and Unitary team. 2020. Detoxify. Github. https://github.com/unitaryai/detoxify.

Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. ToxiGen: A large-scale machine- generated dataset for adversarial and implicit In Proceedings of the hate speech detection. 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 3309–3326, Dublin, Ireland. Asso- ciation for Computational Linguistics.

Dan Hendrycks, Collin Burns, Steven Basart, An- drew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. Aligning ai with shared hu- man values.

Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. 2023. An overview of catastrophic ai risks.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, An- drea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1–38.

Zhibin Gou, Zhihong Shao, Yeyun Gong, Ye- long Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738.

Liwei Jiang, Jena D. Hwang, Chandra Bhaga- vatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. 2021. Delphi: Towards machine ethics and norms.

Yuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Xi- aoyan Zhu, Jie Tang, and Minlie Huang. 2022.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for In Proceedings of efficient text classification.

the 15th Conference of the European Chapter of the Association for Computational Linguis- tics: Volume 2, Short Papers, pages 427–431, Valencia, Spain. Association for Computational Linguistics.

Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation.

Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Xim- ing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and Maarten Sap. 2022. ProsocialDialog: A prosocial backbone for conversational agents. In Proceedings of the 2022 Conference on Em- pirical Methods in Natural Language Process- ing, pages 4005–4029, Abu Dhabi, United Arab Emirates. Association for Computational Lin- guistics.

Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. GeDi: Generative discriminator guided sequence generation. In Findings of the Asso- ciation for Computational Linguistics: EMNLP 2021, pages 4929–4952, Punta Cana, Domini- can Republic. Association for Computational Linguistics.

Rohan Kshirsagar, Tyrus Cukuvac, Kathy McK- eown, and Susan McGregor. 2018. Predictive embeddings for hate speech detection on Twit- In Proceedings of the 2nd Workshop on ter. Abusive Language Online (ALW2), pages 26– 32, Brussels, Belgium. Association for Compu- tational Linguistics.

Léo Laugier, John Pavlopoulos, Jeffrey Sorensen, and Lucas Dixon. 2021. Civil rephrases of toxic texts with self-supervised transformers. In Pro- ceedings of the 16th Conference of the Euro- pean Chapter of the Association for Computa- tional Linguistics: Main Volume, pages 1442– 1461, Online. Association for Computational Linguistics.

Sharon Levy, Emily Allaway, Melanie Sub- biah, Lydia Chilton, Desmond Patton, Kath- leen McKeown, and William Yang Wang. 2022. SafeText: A benchmark for exploring physi- cal safety in language models. In Proceedings of the 2022 Conference on Empirical Methods

in Natural Language Processing, pages 2407– 2421, Abu Dhabi, United Arab Emirates. Asso- ciation for Computational Linguistics.

Haoran Li, Yangqiu Song, and Lixin Fan. 2022a. You don’t know my favorite color: Preventing dialogue representations from revealing speak- In Proceedings of the ers’ private personas. 2022 Conference of the North American Chap- ter of the Association for Computational Lin- guistics: Human Language Technologies, pages 5858–5870, Seattle, United States. Association for Computational Linguistics.

Xiang Lisa Li and Percy Liang. 2021. Prefix- tuning: Optimizing continuous prompts for In Proceedings of the 59th An- generation. nual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers), pages 4582– 4597, Online. Association for Computational Linguistics.

Xiang Lisa Li, John Thickstun, Ishaan Gulra- jani, Percy Liang, and Tatsunori B. Hashimoto. 2022b. Diffusion-lm improves controllable text generation.

Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. DExperts: Decoding-time controlled text generation with In Proceedings of experts and anti-experts. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Inter- national Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6691–6706, Online. Association for Computa- tional Linguistics.

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a. Pre-train, prompt, and predict: A sys- tematic survey of prompting methods in natu- ral language processing. ACM Comput. Surv., 55(9).

Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X Liu, and Soroush Vosoughi. 2023b. Second thoughts are best: Learning to re-align with human values from text edits.

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang

Ding, Kaiwen Men, Kejuan Yang, et al. 2023c. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688.

Language Online (ALW2), pages 1–10, Brus- sels, Belgium. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy- anov. 2019. Roberta: A robustly optimized bert pretraining approach.

Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev, Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, and Alexander Panchenko. 2022. ParaDetox: Detoxification In Proceedings of the 60th with parallel data. Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 6804–6818, Dublin, Ireland. Association for Computational Linguistics.

Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021. Scruples: A corpus of community ethical judgments on 32,000 real-life anecdotes. Pro- ceedings of the AAAI Conference on Artificial Intelligence, 35(15):13470–13479.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.

Binny Mathew, Punyajoy Saha, Seid Muhie Yi- mam, Chris Biemann, Pawan Goyal, and Ani- mesh Mukherjee. 2021. Hatexplain: A bench- mark dataset for explainable hate speech detec- tion. Proceedings of the AAAI Conference on Artificial Intelligence, 35(17):14867–14875.

Ninareh Mehrabi, Ahmad Beirami, Fred Morstat- ter, and Aram Galstyan. 2022. Robust con- versational agents against imperceptible toxic- In Proceedings of the 2022 Con- ity triggers. ference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 2831–2847, Seattle, United States. Association for Compu- tational Linguistics.

Pushkar Mishra, Helen Yannakoudakis, and Eka- terina Shutova. 2018. Neural character-based composition models for abuse detection. In Proceedings of the 2nd Workshop on Abusive

Jelena Mitrovi´c, Bastian Birkeneder, and Michael Granitzer. 2019. nlpUP at SemEval-2019 task 6: A deep neural language model for offensive language detection. In Proceedings of the 13th International Workshop on Semantic Evalua- tion, pages 722–726, Minneapolis, Minnesota, USA. Association for Computational Linguis- tics.

Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias In Proceed- in pretrained language models. ings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natu- ral Language Processing (Volume 1: Long Pa- pers), pages 5356–5371, Online. Association for Computational Linguistics.

Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A challenge dataset for measuring social biases in In Proceedings of masked language models. the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967, Online. Association for Computa- tional Linguistics.

Chris Olah. 2023. Interpretability dreams.

OpenAI. 2022. Chatgpt: Optimizing language

models for dialogue.

OpenAI. 2023a. Gpt-4 technical report.

OpenAI. 2023b. Language models can explain

neurons in language models.

Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and Dit-Yan Yeung. 2021. Prob- ing toxic content in large pre-trained language In Proceedings of the 59th Annual models. Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4262–4274, Online. Association for Computational Linguis- tics.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong

Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language mod- els to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.

Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy risks of general-purpose language models. In 2020 IEEE Symposium on Security and Privacy (SP), pages 1314–1331.

Alicia Parrish, Angelica Chen, Nikita Nan- gia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bow- man. 2022. Bbq: A hand-built bias benchmark for question answering. In Findings of the As- sociation for Computational Linguistics: ACL 2022, pages 2086–2105.

Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. Red teaming language models with lan- guage models.

Fábio Perez and Ian Ribeiro. 2022.

Ignore pre- vious prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527.

Fabio Poletto, Valerio Basile, Manuela San- guinetti, Cristina Bosco, and Viviana Patti. Resources and benchmark corpora 2021. for hate speech detection: a systematic re- Language Resources and Evaluation, view. 55(2):477–523.

Ilan Price, Jordan Gifford-Moore, Jory Flemming, Saul Musker, Maayan Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon, and Jef- Six attributes of un- frey Sorensen. 2020. In Proceedings of the healthy conversations. Fourth Workshop on Online Abuse and Harms, pages 114–124, Online. Association for Com- putational Linguistics.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised mul- titask learners. OpenAI blog, 1:9.

Sara Rosenthal,

Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri, and Preslav SOLID: A large-scale semi- Nakov. 2021. for offensive language supervised dataset In Findings of the Association identification.

for Computational Linguistics: ACL-IJCNLP 2021, pages 915–928, Online. Association for Computational Linguistics.

Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and In Proceed- power implications of language. ings of the 58th Annual Meeting of the As- sociation for Computational Linguistics, pages 5477–5490, Online. Association for Computa- tional Linguistics.

Timo Schick, Sahana Udupa, and Hinrich Schütze. 2021. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computa- tional Linguistics, 9:1408–1424.

Anna Schmidt and Michael Wiegand. 2017. A survey on hate speech detection using natural In Proceedings of the language processing. Fifth International Workshop on Natural Lan- guage Processing for Social Media, pages 1–10, Valencia, Spain. Association for Computational Linguistics.

Denise Sekaquaptewa, Penelope Espinoza, Mis- cha Thompson, Patrick Vargas, and William von Hippel. 2003. Stereotypic explanatory bias: Implicit stereotyping as a predictor of discrim- ination. Journal of Experimental Social Psy- chology, 39(1):75–82.

Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021. “nice try, kiddo”: In- vestigating ad hominems in dialogue responses. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 750–767, Online. Associa- tion for Computational Linguistics.

Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in In Proceedings of the language generation. 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3407– 3412, Hong Kong, China. Association for Com- putational Linguistics.

Wai Man Si, Michael Backes, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, Savvas Zannettou, and Yang Zhang. 2022. Why so toxic? measuring and triggering toxic behav- In Proceedings ior in open-domain chatbots. of the 2022 ACM SIGSAC Conference on Com- puter and Communications Security, CCS ’22, page 2659–2673, New York, NY, USA. Associ- ation for Computing Machinery.

Gudbjartur Ingi Sigurbergsson and Leon Der- czynski. 2020. Offensive language and hate In Proceedings speech detection for Danish. of the Twelfth Language Resources and Evalu- ation Conference, pages 3498–3508, Marseille, France. European Language Resources Associ- ation.

Congzheng Song and Ananth Raghunathan. 2020. Information leakage in embedding models. In Proceedings of the 2020 ACM SIGSAC Confer- ence on Computer and Communications Secu- rity, CCS ’20, page 377–390, New York, NY, USA. Association for Computing Machinery.

Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan Zhu, and Minlie Huang. 2022. On the safety of conversational models: Taxonomy, dataset, and benchmark. In Findings of the As- sociation for Computational Linguistics: ACL 2022, pages 3906–3923, Dublin, Ireland. Asso- ciation for Computational Linguistics.

Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023. Safety assess- ment of chinese large language models. arXiv preprint arXiv:2304.10436.

Romal Thoppilan, Daniel De Freitas,

Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, De- hao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Kri- vokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier- Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke,

Johnny Soraker, Ben Zevenbergen, Vinodku- mar Prabhakaran, Mark Diaz, Ben Hutchin- son, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co- hen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications.

Hugo Touvron, Louis Martin, Kevin Stone, Pe- ter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettle- moyer, Maryam Fazel-Zarandi, and Asli Ce- likyilmaz. 2023. Shepherd: A critic for lan- guage model generation.

Yau-Shian Wang and Yingshan Chang. 2022. Tox- icity detection with generative prompt-based in- ference.

Yida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong Jiang, Xiaoyan Zhu, and Minlie Huang. 2020. A large-scale chinese short-text conver- In Natural Language Process- sation dataset. ing and Chinese Computing: 9th CCF Inter- national Conference, NLPCC 2020, Zhengzhou, China, October 14–18, 2020, Proceedings, Part I, page 91–103, Berlin, Heidelberg. Springer- Verlag.

Zijian Wang and Christopher Potts. 2019. Talk- Down: A corpus for condescension detection In Proceedings of the 2019 Con- in context. ference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3711–3719, Hong Kong, China. Association for Computa- tional Linguistics.

Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles,

Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Predicting the type and target of offensive posts in social media. NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference, 1:1415–1420.

Yangjun Zhang, Pengjie Ren, and Maarten de Ri- jke. 2020. Detecting and classifying malevo- lent dialogue responses: Taxonomy, data and methodology.

Yangjun Zhang, Pengjie Ren, and Maarten de Ri- jke. 2021. A taxonomy, data set, and bench- mark for detecting and classifying malevolent Journal of the Associa- dialogue responses. tion for Information Science and Technology, 72:1477 – 1497.

Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, Fei Mi, Yasheng Wang, Lifeng Shang, and Minlie Huang. 2022. Constructing highly inductive contexts for dialogue safety through controllable reverse generation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3684–3697.

Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, and Mykola Pechenizkiy. 2023. Chbias: Bias evaluation and mitigation of chi- arXiv nese conversational language models. preprint arXiv:2305.11262.

Caleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang. 2022. The moral in- tegrity corpus: A benchmark for ethical dia- logue systems. In Proceedings of the 60th An- nual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 3755–3773.

Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from lan- guage models.

Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Chal- lenges in detoxifying language models. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2447–2469, Punta Cana, Dominican Republic. Association for Computational Linguistics.

Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at In Proceedings of the 26th Interna- scale. tional Conference on World Wide Web, WWW ’17, page 1391–1399, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.

Canwen Xu, Zexue He, Zhankui He, and Julian McAuley. 2022. Leashing the inner demons: Self-detoxification for language models.

Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et al. 2023a. Cvalues: Measuring the values of chinese large language arXiv models from safety to responsibility. preprint arXiv:2307.09705.

Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja- son Weston, and Emily Dinan. 2020. Recipes arXiv for safety in open-domain chatbots. preprint arXiv:2010.07079.

Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023b. On the tool manipulation capability of open- source large language models. arXiv preprint arXiv:2305.16504.

Kevin Yang and Dan Klein. 2021. FUDGE: Con- trolled text generation with future discrimina- tors. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, pages 3511–3535, Online. Association for Computational Linguistics.