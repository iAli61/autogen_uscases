{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM models:  ['gpt-4', 'gpt-4-32k', 'gpt-4-0613', 'gpt-35-turbo', 'gpt-35-turbo-16k']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Dict, List, Optional, Union, Callable, Literal\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from autogen.formatting_utils import colored\n",
    "from typing_extensions import Annotated\n",
    "import autogen\n",
    "from autogen import Agent\n",
    "\n",
    "from teachability import Teachability\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import arxiv\n",
    "\n",
    "import requests\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "version = \"0.1.2\"\n",
    "ProjectID = \"AI_security\"\n",
    "initiate_db = False\n",
    "config_file = \"OAI_CONFIG_LIST\"\n",
    "# config_file = \"OAI_CONFIG_LIST\"\n",
    "\n",
    "task = \"\"\"\n",
    "As a recognized authority on enhancing the reliability and safety of AI systems, you're invited to illuminate our AI community with your insights through a scientific article titled \"Survey on Reliability and Safety Mechanisms in AI Systems and the most recent advancement\".\n",
    "\n",
    " Your expertise will guide our audience through the nuances of ensuring AI operates within safe and reliable parameters, with a special focus on Large Language Models (LLMs). Here's how to structure your invaluable contribution:\n",
    "\n",
    "- **Core Theme:** Anchor your discussion around Large Language Models, highlighting their significance in the current AI landscape and why reliability and safety are paramount.\n",
    "\n",
    "- **Innovative Progress:** Dive into the latest breakthroughs and methodologies [at least 3 methodologies] that have emerged in the domain of AI reliability and safety. Showcase [with reference to original paper] how these advancements are shaping the future of responsible AI development and implementation.\n",
    "\n",
    "- **Accessible Insight:** While your post will be rich in information, ensure it's crafted in a manner that demystifies complex concepts for those outside the tech sphere. Your goal is to enlighten, not overwhelm.\n",
    "\n",
    "- **Credible Sources:** You MUST Strengthen your narrative by integrating references to the research, studies, and sources that informed your insights. Additionally, provide these references for readers seeking to delve deeper into the subject.\n",
    "\n",
    "- **Current Perspective:** Reflect the cutting-edge of the field by incorporating the most recent findings and research available in your database. Your post should serve as a timely resource for anyone looking to understand the state-of-the-art in AI safety and reliability mechanisms.\n",
    "\n",
    "This blog post is an opportunity to not just share knowledge but to foster a deeper understanding and appreciation for the ongoing efforts to make AI systems more reliable and safe for everyone. Your contribution will undoubtedly be a beacon for those navigating the complexities of AI in our increasingly digital world.\n",
    "You are equipped  with a function that could read a paper for you. If you need a missing info please update you knowledge base.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Project_dir = Path(f\"./{ProjectID}/{version}\")\n",
    "\n",
    "if not os.path.exists(Project_dir): initiate_db = True\n",
    "\n",
    "output_dir = f'{Project_dir}/pdf_output'\n",
    "if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "db_dir = f'{Project_dir}/memo-db/'\n",
    "# check if db_dir exists, delete it if it does\n",
    "if initiate_db:\n",
    "\n",
    "    if not os.path.exists(Project_dir): \n",
    "        shutil.rmtree(Project_dir)\n",
    "        os.makedirs(Project_dir)\n",
    "    if os.path.exists(db_dir): shutil.rmtree(db_dir)\n",
    "\n",
    "    # create a list of papers that have been read and saved it in a pickle file\n",
    "    read_papers = []\n",
    "    with open(f'{Project_dir}/read_papers.pkl', 'wb') as f:\n",
    "        pickle.dump(read_papers, f)\n",
    "\n",
    "    # create a list of abstract that have been read and saved it in a pickle file\n",
    "    read_abstracts = []\n",
    "    with open(f'{Project_dir}/read_abstracts.pkl', 'wb') as f:\n",
    "        pickle.dump(read_abstracts, f)\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    config_file,\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-32k\", \"gpt-4\", \"gpt4\", \"gpt-35-turbo-16k\", \"gpt-4-0613\", \"gpt-3.5-turbo\", \"gpt-35-turbo\", \"gpt-35-turbo-0613\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"LLM models: \", [config_list[i][\"model\"] for i in range(len(config_list))])\n",
    "\n",
    "# Configuration for the Language Model (LLM)\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,  # config_list should be defined or imported\n",
    "    \"timeout\": 120,\n",
    "    # \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Configuration for the manager using the same config_list as llm_config\n",
    "manager_config = {\n",
    "    \"config_list\": config_list,  # config_list should be defined or imported\n",
    "    \"timeout\": 60,\n",
    "    # \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Termination message definition\n",
    "termination_msg = (\n",
    "    lambda x: isinstance(x, dict)\n",
    "    and str(x.get(\"content\", \"\")).upper() == \"TERMINATE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## teach agent for some skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_teachable_groupchat(assitant_name, user_name, db_dir, config_list, verbosity=0):\n",
    "    \n",
    "    # Start by instantiating any agent that inherits from ConversableAgent.\n",
    "    assistant = autogen.ConversableAgent(\n",
    "        name=assitant_name,  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "        llm_config={\"config_list\": config_list, \"timeout\": 120, \"cache_seed\": None},  # Disable caching.\n",
    "    )\n",
    "\n",
    "    # Instantiate the Teachability capability. Its parameters are all optional.\n",
    "    teachability = Teachability(\n",
    "        verbosity=verbosity,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "        reset_db=False,  \n",
    "        path_to_db_dir=db_dir,\n",
    "        recall_threshold=1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    "    )\n",
    "\n",
    "    # Now add the Teachability capability to the agent.\n",
    "    teachability.add_to_agent(assistant)\n",
    "\n",
    "    user = autogen.UserProxyAgent(\n",
    "        name=user_name,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=termination_msg,\n",
    "        max_consecutive_auto_reply=0,\n",
    "        code_execution_config={\"use_docker\": False},\n",
    "    )\n",
    "\n",
    "    return assistant, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if initiate_db:\n",
    "    prompt = \"For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \\n\\n' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, you MUST finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\"\n",
    "\n",
    "    instract_assistant, instract_user = create_teachable_groupchat(\"instract_assistant\", \"instract_user\", db_dir, config_list, verbosity=3)\n",
    "\n",
    "    instract_user.initiate_chat(instract_assistant, silent=True, message=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "\n",
    "### Arxiv funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _arxiv_search(query, n_results=10):\n",
    "    sort_by = arxiv.SortCriterion.Relevance\n",
    "    papers = arxiv.Search(query=query, max_results=n_results, sort_by=sort_by)\n",
    "    papers = list(arxiv.Client().results(papers))\n",
    "    return papers\n",
    "\n",
    "def arxiv_search(query : Annotated[str, \"The title of paper to search for in arxiv.\"]) -> str:\n",
    "    papers = _arxiv_search(query, n_results=5)\n",
    "    if len(papers)>0:\n",
    "        return ''.join([f\" \\n\\n {i+1}. Title: {paper.title} Authors: {', '.join([str(au) for au in paper.authors])} URL: {paper.pdf_url}\" for i, paper in enumerate(papers)])\n",
    "    else:\n",
    "        return \"There are no papers found in arxiv for the given query.\"\n",
    "\n",
    "text = \"Human-Centred Learning Analytics and AI in Education: a Systematic Literature Review\"\n",
    "# arxiv_search(query=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_paper_id(url):\n",
    "    if '/pdf/' in url:\n",
    "        return url.split('/')[-1].replace('.pdf', '')\n",
    "    if '/abs/' in url:\n",
    "        return url.split('/')[-1]\n",
    "    return url\n",
    "\n",
    "def get_paper_metadata(url):\n",
    "    \n",
    "    paper_id = get_paper_id(url)\n",
    "    \n",
    "    search_by_id = arxiv.Search(id_list=[paper_id])\n",
    "    paper = list(arxiv.Client().results(search_by_id))[0]\n",
    "    title = paper.title\n",
    "    link = paper._raw['link']\n",
    "    updated = paper.updated\n",
    "    summary = paper.summary\n",
    "    pdf_url = paper.pdf_url\n",
    "\n",
    "    return title, link, updated, summary, pdf_url, paper_id\n",
    "\n",
    "# get_paper_metadata('https://arxiv.org/abs/1810.04805')\n",
    "# get_paper_metadata('https://arxiv.org/pdf/1810.04805.pdf')\n",
    "# get_paper_metadata('1810.04805')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arxiv retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_chat_with_paper_info(paper, query, message):\n",
    "\n",
    "    # Create a TeachableAgent and UserProxyAgent to represent the researcher and the user, respectively.\n",
    "    arxiver, arxiver_user = create_teachable_groupchat(\"arxiver\", \"arxiver_user\", db_dir, config_list, verbosity=0)\n",
    "    try:\n",
    "        arxiver_user.initiate_chat(arxiver,\n",
    "                        silent=True,\n",
    "                        message=f\"The following article is one of the articles that I found for '{query}' topic: \\n\\n '{paper.title}' by {paper.authors} updated on {paper.updated}: {paper.pdf_url} \\nsummary: {paper.summary} \\n?\")\n",
    "        \n",
    "        return f\"Title: {paper.title} Authors: {paper.authors} URL: {paper.pdf_url} os added to MEMOS\\n\\n \"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def process_query(query, n_results, message):\n",
    "    \"\"\"Function to process each query and initiate chats for each paper found.\"\"\"\n",
    "    papers = _arxiv_search(query, n_results=n_results)\n",
    "\n",
    "    # check if the abstract has been read before\n",
    "    with open(f'{Project_dir}/read_abstracts.pkl', 'rb') as f:\n",
    "        read_abstracts = pickle.load(f)\n",
    "    papers = [paper for paper in papers if paper.pdf_url not in read_abstracts]\n",
    "\n",
    "    # add papers to the read_papers list\n",
    "    with open(f'{Project_dir}/read_abstracts.pkl', 'rb') as f:\n",
    "        read_abstracts = pickle.load(f)\n",
    "    read_abstracts.extend([paper.pdf_url for paper in papers])\n",
    "    with open(f'{Project_dir}/read_abstracts.pkl', 'wb') as f:\n",
    "        pickle.dump(read_abstracts, f)\n",
    "\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(initiate_chat_with_paper_info, paper, query, message) for paper in papers]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "def arxiv_retriever(queries: Annotated[List[str], \"The list of query texts to search for.\"], \n",
    "                    n_results: Annotated[int, \"The number of results to retrieve for each query.\"] = 10,\n",
    "                    ) -> str:\n",
    "    \n",
    "    \n",
    "\n",
    "    message = \"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_query, query_text, n_results, message) for query_text in queries]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    # Instantiate a UserProxyAgent to represent the user. But in this notebook, all user input will be simulated.\n",
    "    return f\"Dear Researcher, Database updated with on the following topics: {', '.join(list(queries))}. Please go ahead with your task.\"\n",
    "    # return message\n",
    "\n",
    "message = [\"Large Language Models safety and reliability\", \"AI systems reliability mechanisms\", \"Methodologies for improving AI safety\", \"Recent advancements in AI system safety\", \"Latest research in AI reliability\"]\n",
    "if initiate_db:\n",
    "    arxiv_retriever(message, n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_reasoning(reason, summary):\n",
    "    \n",
    "    # Start by instantiating any agent that inherits from ConversableAgent.\n",
    "    assistant = autogen.AssistantAgent(\n",
    "        name=\"reasoning_checker\",  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "        llm_config={\"config_list\": config_list, \"timeout\": 120, \"cache_seed\": None},  # Disable caching.\n",
    "    )\n",
    "\n",
    "    user = autogen.UserProxyAgent(\n",
    "        name=\"user\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=termination_msg,\n",
    "        max_consecutive_auto_reply=0,\n",
    "        code_execution_config={\"use_docker\": False},\n",
    "    )\n",
    "\n",
    "    chat_hist = user.initiate_chat(assistant, silent=True, message=f\"check if \\\"{reason} is a good reason is to read a paper with the following summary: {summary} /n/n answer only with 'yes' or 'no'\")\n",
    "    return chat_hist.chat_history[-1]['content']\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    \"\"\"Download a PDF from a given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def initiate_chat_read_paper(text, article):\n",
    "    paper_reader, reader_user = create_teachable_groupchat(\"paper_reader\", \"reader_user\", db_dir, config_list, verbosity=0)\n",
    "    try:\n",
    "        reader_user.initiate_chat(paper_reader,\n",
    "                        silent=True,\n",
    "                        message=f\"MEMORIZE_ARTICLE: The following passage is extracted from an article titled '{article}': \\n\\n {text}.\"\n",
    "                        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(colored(f\"text: {text}\", \"red\"))\n",
    "    \n",
    "def chunk_pdf(url, title):\n",
    "    \n",
    "    print(f\"Reading the article, '{title}'\")\n",
    "    pdf_filename = url.split('/')[-1] + '.pdf'\n",
    "    pdf_path = os.path.join(output_dir, pdf_filename)\n",
    "    \n",
    "\n",
    "    download_pdf(url, pdf_path)\n",
    "    elements = partition_pdf(filename=pdf_path)\n",
    "    chunked_elements = chunk_by_title(elements)\n",
    "\n",
    "    # find checked_elemnt that includes \"REFERENCES\" in the second half of the text\n",
    "\n",
    "    half_length = len(chunked_elements) // 2\n",
    "    for i, chunk in enumerate(chunked_elements[half_length:], start=half_length):\n",
    "        chunk_text_upper = chunk.text.upper()\n",
    "        if re.search(r'\\bREFERENCE\\b', chunk_text_upper) or re.search(r'\\bREFERENCES\\b', chunk_text_upper):\n",
    "            chunked_elements = chunked_elements[1:i]\n",
    "            break\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(initiate_chat_read_paper, chunk.text, title) for chunk in chunked_elements if len(chunk.text.split()) > 30]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This `get_pdfss` function is designed to download a PDF from a given URL, extract its content, \n",
    "partition the content into chunks based on titles, and then initiate a chat to share and memorize \n",
    "each chunk of the article with a teachable agent and a user.\n",
    "\"\"\"\n",
    "def get_pdfs(urls: Annotated[List[str], \"The list of URLs of the papers to read.\"],\n",
    "            reasons: Annotated[List[str], \"The list of reasons for reading the papers. it should be same size as urls list.\"]\n",
    "            ) -> str:\n",
    "    \n",
    "    urls_list = []\n",
    "    titles_list = []\n",
    "    message = ''\n",
    "    for url in urls:\n",
    "\n",
    "        title, link, updated, summary, pdf_url, paper_id = get_paper_metadata(url)\n",
    "        \n",
    "        title = f\"{title} [{pdf_url}] updated {updated}\"\n",
    "        \n",
    "        check_reason = check_reasoning(reasons[urls.index(url)], summary)\n",
    "        if 'no' in check_reason.lower():\n",
    "            print(f\"The article, '{title}', does not meet the criteria for reading.\")\n",
    "            message += f\"The article, '{title}', does not meet the criteria for reading.\\n\"\n",
    "            continue\n",
    "        \n",
    "        # add url to list of papers in pickle file if it doesn't exist\n",
    "        with open(f'{Project_dir}/read_papers.pkl', 'rb') as f:\n",
    "            read_papers = pickle.load(f)\n",
    "\n",
    "        if pdf_url in read_papers: \n",
    "            print(f\"The article, '{title}', has already been read and shared with you in your memory.\")\n",
    "            message += f\"The article, '{title}', has already been read and shared with you in your memory.\\n\"\n",
    "            continue\n",
    "        else:\n",
    "            urls_list.append(pdf_url)\n",
    "            titles_list.append(title)\n",
    "\n",
    "        read_papers.append(pdf_url)\n",
    "        with open(f'{Project_dir}/read_papers.pkl', 'wb') as f:\n",
    "            pickle.dump(read_papers, f)\n",
    "\n",
    "    print(f\"{len(read_papers)} articles have been read, so far.\")\n",
    "\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(chunk_pdf, url, title) for url, title in zip(urls_list, titles_list)]\n",
    "        for future in as_completed(futures):\n",
    "            future.result() \n",
    "\n",
    "\n",
    "    message += f\"The articles {', and '.join(titles_list)}  has been read and the content has been shared with you in your memory.\"\n",
    "    return message\n",
    "\n",
    "# Example usage\n",
    "args = {\n",
    "\"urls\": [\"http://arxiv.org/pdf/2304.10436v1\", \"http://arxiv.org/pdf/2403.09676v1\", \"http://arxiv.org/pdf/2302.09270v3\", \"http://arxiv.org/pdf/2304.09865v1\", \"http://arxiv.org/pdf/2310.09624v2\", \"http://arxiv.org/pdf/2210.09150v2\", \"http://arxiv.org/pdf/2311.02147v1\", \"http://arxiv.org/pdf/2311.05608v2\", \"http://arxiv.org/pdf/2403.00862v2\", \"http://arxiv.org/pdf/2404.05993v1\", \"http://arxiv.org/pdf/2312.06798v1\"],\n",
    "\"reasons\": [\"To understand how the safety performance of LLMs is assessed in typical safety scenarios and instruction attacks.\", \"To explore the landscape of AI deception focusing on LLMs and the strategies to navigate deceptive behaviors.\", \"To gain insights into the safety issues, evaluation methods, and enhancement strategies concerning large models.\", \"To examine the impact of moderation on user enjoyment of AI systems.\", \"To comprehend methods for robust safety evaluation of LLMs and uncover safety concerns.\", \"To learn about the reliability of LLMs in generalizability, social biases, calibration, and factuality.\", \"To uncover the alignment problem in LLMs and its implications for the safety of AI systems.\", \"To evaluate the safety of VLMs and their vulnerability to jailbreaking attacks.\", \"To comprehend the framework for evaluating the capability of LLMs in Chinese Journalistic Writing Proficiency and their Safety Adherence.\", \"To assess the risk taxonomy of AI content and the effectiveness of the AEGIS model.\", \"To understand how NeuroSymbolic AI approach helps in creating trustworthy AI systems.\"]\n",
    "}\n",
    "if initiate_db:\n",
    "    for i in range(0, len(args['urls']), 5):\n",
    "        get_pdfs(args['urls'][i:i+5], args['reasons'][i:i+5])\n",
    "        \n",
    "# get_pdfs(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'{Project_dir}/read_papers.pkl', 'rb') as f:\n",
    "        read_papers = pickle.load(f)\n",
    "\n",
    "len(read_papers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartChoice = Literal['summary', 'full']\n",
    "\n",
    "def get_pdf(url: Annotated[str, \"The URL of the paper to read.\"],\n",
    "            reason: Annotated[str, \"reason for reading the paper.\"],\n",
    "            part: Annotated[PartChoice, \"choose do you need entire paper ('full') or a summary is enough.\"],\n",
    "            ) -> str:\n",
    "\n",
    "    message = ''\n",
    "    title, link, updated, summary, pdf_url, paper_id = get_paper_metadata(url)\n",
    "    title = f\"{title} [{pdf_url}] updated {updated}\"\n",
    "        \n",
    "    # add url to list of papers in pickle file if it doesn't exist\n",
    "    with open(f'{Project_dir}/read_papers.pkl', 'rb') as f:\n",
    "        read_papers = pickle.load(f)\n",
    "\n",
    "    if pdf_url in read_papers: \n",
    "        print(f\"The article, '{title}', has already been read and shared with you in your memory.\")\n",
    "        message += f\"The article, '{title}', has already been read and shared with you in your memory.\\n\"\n",
    "        paper_in_memo = True\n",
    "    else:\n",
    "        check_reason = check_reasoning(reason, summary)\n",
    "        if 'no' in check_reason.lower():\n",
    "            return f\"The article, '{title}', does not meet the criteria for reading.\"\n",
    "            \n",
    "        read_papers.append(pdf_url)\n",
    "        with open(f'{Project_dir}/read_papers.pkl', 'wb') as f:\n",
    "            pickle.dump(read_papers, f)\n",
    "        chunk_pdf(pdf_url, title)\n",
    "\n",
    "    pdf_filename = f\"{get_paper_id(pdf_url)}.pdf\"\n",
    "    pdf_path = os.path.join(output_dir, pdf_filename)\n",
    "\n",
    "    elements = partition_pdf(filename=pdf_path)\n",
    "    chunked_elements = chunk_by_title(elements)\n",
    "\n",
    "    # find checked_elemnt that includes \"REFERENCES\" in the second half of the text\n",
    "\n",
    "    half_length = len(chunked_elements) // 2\n",
    "    for i, chunk in enumerate(chunked_elements[half_length:], start=half_length):\n",
    "        chunk_text_upper = chunk.text.upper()\n",
    "        if re.search(r'\\bREFERENCE\\b', chunk_text_upper) or re.search(r'\\bREFERENCES\\b', chunk_text_upper):\n",
    "            chunked_elements = chunked_elements[:i]\n",
    "            break\n",
    "\n",
    "    return \"\\n\\n\".join([str(el) for el in chunked_elements])\n",
    "\n",
    "# Example usage\n",
    "# get_pdf(\"http://arxiv.org/pdf/2312.01090v2\", \"Verify study findings on LLM-based agents in wargames.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_check(paper_url: Annotated[str, \"The URL of the paper to check.\"],\n",
    "            paper_title: Annotated[str, \"The title of the paper to be used for fact checking.\"],\n",
    "            ):\n",
    "    if paper_url.find('arxiv.org') == -1:\n",
    "        return False, f\"The provided paper URL, {paper_url}, is not from arxiv.org. Please provide a valid arxiv URL.\"\n",
    "\n",
    "    title, link, updated, summary, pdf_url, paper_id = get_paper_metadata(paper_url)\n",
    "    if title != paper_title:\n",
    "        return False, f\"The provided paper URL, {paper_url}, is not for the paper titled '{paper_title}'. Please provide a valid arxiv URL for the paper.\"\n",
    "    \n",
    "    return True, f\"The provided paper URL is from arxiv.org and is for the paper titled '{paper_title}'.\"\n",
    "\n",
    "def factual_check(text: Annotated[str, \"The writer text to be factually checked.\"],\n",
    "                    paper_title: Annotated[str, \"The title of the paper to be used for fact checking.\"],\n",
    "                    paper_url: Annotated[str, \"The arxiv URL of the paper to be used for fact checking.\"],\n",
    "                    reason: Annotated[str, \"The reason for reading the paper.\"],\n",
    "                    paper_authors: Annotated[Optional[str], \"The authors of the paper to be used for fact checking.\"]=None,\n",
    "                    ) -> str:\n",
    "    \n",
    "    url_check_res, message = url_check(paper_url, paper_title)\n",
    "    if not url_check_res:\n",
    "        return message\n",
    "\n",
    "    paper_content = get_pdf(paper_url, reason, part='full')\n",
    "    factual_checker_prompt = \"\"\"\n",
    "Below, you will find a passage labeled \"TEXT\" that references a specific paper: '{paper}' alongside its corresponding \"PAPER_CONTENT.\" Your task is to read the \"PAPER_CONTENT\" and verify the factual accuracy of the \"TEXT\" as it pertains to the paper.\n",
    "\n",
    "Once you have assessed the factual accuracy, you MUST provide feedback, begining with 'FEEDBACK:'. Following your assessment, please write a summary of the paper. Begin this summary with 'Summary of {paper}: '\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "PAPER_CONTENT:\n",
    "{paper_content}\n",
    "\"\"\"\n",
    "\n",
    "    # Start by instantiating any agent that inherits from ConversableAgent.\n",
    "    factual_checker = autogen.AssistantAgent(\n",
    "        name=\"factual_checker\",  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "        llm_config={\"config_list\": config_list, \"timeout\": 120, \"cache_seed\": None},  # Disable caching.\n",
    "        system_message = \"You are a factual_check AI assistant. You are responsible for verifying the factual accuracy of the text provided in relation to the paper content.\"\n",
    "        )\n",
    "\n",
    "    # create a UserProxyAgent instance named \"user_proxy\"\n",
    "    factual_checker_user = autogen.UserProxyAgent(\n",
    "        name=\"factual_checker_user\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=termination_msg,\n",
    "        code_execution_config=False,\n",
    "    )\n",
    "\n",
    "    if paper_authors:\n",
    "        paper = f\"{paper_title} [{paper_url}] by {', '.join(list(paper_authors.split(',')))}\"\n",
    "    else:\n",
    "        paper = f\"{paper_title} [{paper_url}]\"\n",
    "    chat = factual_checker_user.initiate_chat(factual_checker, silent=False, max_turns=1,\n",
    "                                              message=factual_checker_prompt.format(text=text, paper_content=paper_content, paper=paper))\n",
    "\n",
    "    return chat.chat_history[-1]['content']\n",
    "\n",
    "args = [\n",
    "    {\n",
    "        \"text\": \"In education, they personalize learning by providing interactive learning experiences and human-centered learning analytics (Raji et al., 2023; Alfredo et al., 2023).\",\n",
    "        \"paper_title\": \"Human-Centred Learning Analytics and AI in Education: a Systematic Literature Review\",\n",
    "        \"paper_url\": \"http://arxiv.org/pdf/2312.12751v1\",\n",
    "        \"reason\": \"Verify the claims about LLMs personalizing learning in education through interactive experiences and analytics\"\n",
    "    },{\n",
    "        \"text\": \"Models such as the GPT series, BERT, and others, educated on vast corpuses of text from the internet and other sources, possess an unprecedented capability to understand, interpret, and generate human-like text.\", \n",
    "        \"paper_title\": \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \n",
    "        \"paper_url\": \"http://arxiv.org/abs/1810.04805\", \n",
    "        \"reason\": \"To confirm the capabilities of the BERT model as mentioned in the blog section.\"\n",
    "    },{\n",
    "        \"text\": \"The GPT series, which includes models like GPT-3 and potentially GPT-4, have been trained to generate human-like text and can perform a variety of language-based tasks.\", \n",
    "        \"paper_title\": \"Language Models are Unsupervised Multitask Learners\", \n",
    "        \"paper_url\": \"https://openai.com/research/language-models\", \n",
    "        \"reason\": \"To verify the characteristics of GPT series models as described in the blog section.\"\n",
    "    },{\n",
    "        \"text\": \"In healthcare, LLMs like ClinicalBERT assist in diagnostic processes.\", \n",
    "        \"paper_title\": \"ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission\", \n",
    "        \"paper_url\": \"http://arxiv.org/abs/1904.05342\", \n",
    "        \"reason\": \"To check the application and accuracy of ClinicalBERT in diagnostic processes within the healthcare sector as outlined in the blog section.\"\n",
    "    },{\n",
    "        \"text\": \"Risks such as the generation of misleading information, privacy breaches, or the misuse in fabricating deepfakes are concerns with the widespread deployment of LLMs.\",\n",
    "        \"paper_title\": \"Dive into Deepfakes: Detection, Attribution, and Ethics\",\n",
    "        \"paper_url\": \"http://arxiv.org/abs/2004.13745\", \n",
    "        \"reason\": \"To validate the concerns related to the generation of misleading information and deepfakes by LLMs as mentioned in the blog section.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# factual_check(**args[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add functions to agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [\n",
    "    (\"arxiv_retriever\", arxiv_retriever, \"Retrieve summeries of papers from arxiv for give query.\"),\n",
    "    (\"get_pdfs\", get_pdfs, \"Retrieve the content of the pdf files from the urls list.\"),\n",
    "    (\"get_pdf\", get_pdf, \"Retrieve the content of the pdf file from the url.\"),\n",
    "    (\"factual_check\", factual_check, \"Check the factual accuracy of a given text based on a paper.\"),\n",
    "    (\"arxiv_search\", arxiv_search, \"retrun the pdf url from arxiv for the given paper title.\"),\n",
    "]\n",
    "\n",
    "\n",
    "def add_func_to_agents(assignments, funcs=funcs):\n",
    "\n",
    "    # example input \n",
    "    # assignments = [(assistants, users, \"arxiv_retriever\"), (assistants, users, \"get_pdfs\") ]\n",
    "    # funcs = [(\"arxiv_retriever\", arxiv_retriever, \"Retrieve content for question answering from arxiv.\"),\n",
    "    #          (\"get_pdfs\", get_pdfs, \"Retrieve the content of the pdf file from the url.\")]\n",
    "\n",
    "    func_dict = {}\n",
    "    func_disc_dict = {}\n",
    "    for func_name, func, func_disc in funcs:\n",
    "        func_dict[func_name] = func\n",
    "        func_disc_dict[func_name] = func_disc\n",
    "\n",
    "    for assignment in assignments:\n",
    "        caller, executor, func_name = assignment\n",
    "        autogen.agentchat.register_function(\n",
    "            func_dict[func_name],\n",
    "            caller=caller,\n",
    "            executor=executor,\n",
    "            name=func_name,\n",
    "            description=func_disc_dict[func_name]\n",
    "        )\n",
    "\n",
    "\n",
    "    return f\"Functions {', '.join([func_name for func_name, _, _ in funcs])} are added to the agents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Section_writer_SP = \"\"\"\n",
    "You are now part of a group chat dedicated to completing a collaborative blog project. As a data_research_writer, your role is to develop a well-researched section of a blog post on a specified topic. You will follow a detailed brief that outlines the necessary content for each part of the section.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "1. Ensure all content is thoroughly researched and supported by data from our database. Verify all information using the MEMOS tool to confirm accuracy and completeness.\n",
    "2. Each draft segment must include citations. Please list the title, URL, and authors of each cited paper at the end of your section.\n",
    "3. If you encounter any uncertainties or need clarification, contact the group chat manager for immediate assistance. Additional help from other participants may be provided if necessary.\n",
    "4. Your responsibilities include maintaining strong communication, showcasing precise research skills, paying meticulous attention to detail, and proactively seeking assistance when needed.\n",
    "5. Incorporate any team feedback into your revisions promptly. This is crucial to ensure that the final text is polished and meets our editorial standards.\n",
    "\n",
    "Formatting Requirements:\n",
    "\n",
    "Start your text with 'TXT:' and end with 'END_TXT'. This format is crucial for the group chat manager to accurately identify your contributions.\n",
    "You MUST mention the listion of citation at enad of your section and each citation MUST include the title of the paper, its URL, and authors.\n",
    "Upon completing your section, integrating all feedback, and ensuring all parts are reviewed and properly referenced, signify your completion by typing \"TERMINATE\" in the group chat.\n",
    "\"\"\"\n",
    "\n",
    "section_content_reviwer_sp = \"\"\"\n",
    "You are now in a group chat tasked with completing a specific project. As a Content Review Specialist, your primary goal is to ensure the quality, accuracy, and integrity of the content produced by the data_research_writer, aligning with the data from our database. Your responsibilities include:\n",
    "\n",
    "1. Overseeing the structure and content of the blog post to ensure each section is well-defined and adheres to the overarching theme.\n",
    "2. Collaborating closely with the Writer to understand the breakdown and specific requirements of the blog text.\n",
    "3. Reviewing drafts with the Writer to confirm factual accuracy, high-quality writing, and inclusion of references to pertinent data in the database. Utilize the 'factual_check' function to verify all textual references. Calling 'factual_check' function, provide you with a summery of the paper, please print the summeries afer your feedbacks.\n",
    "4. Cross-checking content against your MEMOS to identify any discrepancies or missing data, requesting updates from the manager if necessary.\n",
    "5. Offering constructive feedback to the writers and ensuring revisions are made swiftly to adhere to the publishing timeline.\n",
    "6. Ensuring content integrity by verifying proper citations and the use of credible sources.\n",
    "7. Seeking clarification or assistance from the group chat manager if uncertainties or confusion arise during the review process, allowing for additional participant support if needed.\n",
    "8. Motivating the writing team to conclude the task only when the content meets all quality standards and fully satisfies the task requirements. Participants should signal the completion of their roles by typing \"TERMINATE\" in the group chat to indicate that the review process is concluded and the blog post is ready for publication.\n",
    "\"\"\"\n",
    "\n",
    "def write_section(title: Annotated[str, \"The title of the section.\"], \n",
    "                  brief: Annotated[str, \"a clear, detailed brief about what section should be included.\"],\n",
    "                  silent: Annotated[bool, \"it should be always True.\"]=True\n",
    "                  ) -> str:\n",
    "    \n",
    "    # Start by instantiating any agent that inherits from ConversableAgent.\n",
    "    data_research_writer = autogen.AssistantAgent(\n",
    "        name=\"data_research_writer\",  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "        llm_config={\"config_list\": config_list, \"timeout\": 120, \"cache_seed\": None},  # Disable caching.\n",
    "        system_message=Section_writer_SP,\n",
    "        description=\"data_research_writer, crafts detailed sections of a blog post based on a specific topic outlined in a brief. They ensure content is well-researched, referenced, and integrates database information.\"\n",
    "    )\n",
    "\n",
    "    # create a UserProxyAgent instance named \"user_proxy\"\n",
    "    writer_user = autogen.UserProxyAgent(\n",
    "        name=\"writer_user\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=termination_msg,\n",
    "        code_execution_config={\n",
    "            \"work_dir\": \"section_writing\",\n",
    "            \"use_docker\": False,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    content_review_specialist = autogen.AssistantAgent(\n",
    "                                    name=\"content_review_specialist\",\n",
    "                                    is_termination_msg=termination_msg,\n",
    "                                    system_message=CONTENT_REVIEWER, \n",
    "                                    llm_config=llm_config,\n",
    "                                    description=\"The content review specialist is a critical thinker who ensures the accuracy and quality of information shared within the group chat. This individual should possess strong analytical skills to review previous messages for errors or misunderstandings and must be able to articulate the correct information effectively. Additionally, if the role involves reviewing Python code, the specialist should also have a solid understanding of Python to provide corrected code when necessary.\"\n",
    "                                )\n",
    "    \n",
    "    teachability = Teachability(\n",
    "                                verbosity=0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "                                reset_db=False,\n",
    "                                path_to_db_dir=db_dir,\n",
    "                                recall_threshold=1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    "                            )\n",
    "\n",
    "    # Now add the Teachability capability to the agent.\n",
    "    teachability.add_to_agent(data_research_writer)\n",
    "    teachability.add_to_agent(content_review_specialist)\n",
    "\n",
    "    add_func_to_agents([(content_review_specialist, writer_user, \"arxiv_retriever\"), \n",
    "                        (content_review_specialist, writer_user, \"factual_check\"),\n",
    "                        (content_review_specialist, writer_user, \"arxiv_search\"),\n",
    "                        (content_review_specialist, writer_user, \"get_pdf\"),\n",
    "                        ])\n",
    "\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[data_research_writer, writer_user, content_review_specialist],\n",
    "        messages=[],\n",
    "        speaker_selection_method=\"auto\",  # With two agents, this is equivalent to a 1:1 conversation.\n",
    "        allow_repeat_speaker=True,\n",
    "        max_round=2,\n",
    "    )\n",
    "\n",
    "    manager = autogen.GroupChatManager(\n",
    "                groupchat=groupchat,\n",
    "                is_termination_msg=termination_msg,\n",
    "                llm_config=manager_config,\n",
    "                code_execution_config={\n",
    "                    \"work_dir\": \"coding\",\n",
    "                    \"use_docker\": False,\n",
    "                },\n",
    "            )\n",
    "\n",
    "    chat_hist = writer_user.initiate_chat(manager, silent=silent, message=f\"Compose a blog section with the following guidelines: \\n\\n Title: {title}, \\n\\n Brief: {brief} \\n\\n Please ensure your writing aligns closely with the brief provided, capturing the essence of the topic while engaging the reader. The section should be coherent, well-structured, and reflective of the main themes outlined in the brief.\")\n",
    "    # prepare the response\\n\",\n",
    "    writer_messages = [mes for mes in chat_hist.chat_history if 'TXT:' in mes['content']]\n",
    "    \n",
    "    return writer_messages[-1]['content'] if writer_messages else \"No response from the writer.\"\n",
    "\n",
    "\n",
    "funcs.append((\"write_section\", write_section, \"Write a section of a blog post based on a given title and brief.\"))\n",
    "\n",
    "arg = [\n",
    "    {\"title\": \"Introduction: The Critical Role of Large Language Models in AI\", \"brief\": \"Outline the significance of Large Language Models (LLMs) in the contemporary AI landscape, touching upon their applications across various sectors. Highlight why ensuring their reliability and safety is paramount given their widespread utility.\"},\n",
    "    {\"title\": \"Unpacking Reliability and Safety: Why It Matters for LLMs\", \"brief\": \"Define reliability and safety in the context of AI and LLMs. Use recent incidents or studies to illustrate the consequences of unreliable or unsafe AI systems.\"},\n",
    "    {\"title\": \"Methodological Advances in Reliability and Safety\", \"brief\": \"Describe at least three recent methodologies aimed at enhancing the safety and reliability of AI systems, specifically LLMs. Reference original papers and incorporate summaries of their findings, ensuring the explanation is accessible to the layperson.\"},\n",
    "    {\"title\": \"Case Study: Component Fault Trees and Their Application\", \"brief\": \"Provide a detailed analysis of the 'Component Fault Trees' methodology using the referenced paper by Kai Hoefig et al. Discuss the benefits and drawbacks and how this methodology can be applied to LLMs.\"},\n",
    "    {\"title\": \"Current Challenges and Risks in LLM Safety\", \"brief\": \"Outline current risks and challenges, such as adversarial attacks, by referencing recent studies and empirical findings relevant to LLMs. Explain how these challenges complicate the quest for reliable and safe AI systems.\"},\n",
    "    {\"title\": \"Promising Solutions: Adversarial Prompt Shield and Ethical Directives\", \"brief\": \"Discuss the 'Adversarial Prompt Shield' as a highlighted solution, providing details of the BAND datasets and how adversarial examples enhance LLM safety. Additionally, address the impact of ethical directives on data set generation.\"},\n",
    "    {\"title\": \"The Alignment Problem: Safeguarding the Future of AI\", \"brief\": \"Based on the work by Raphaël Millière, assess the alignment problem for LLMs, examining how tailoring AI systems to align with human values is both a current issue and a future challenge.\"},\n",
    "    {\"title\": \"Evaluating LLMs for Safety: Benchmarks and Protocols\", \"brief\": \"Present the importance of comprehensive safety assessments for LLMs, suggest how benchmarks such as NewsBench can play a role, and describe the proposed safety assessment benchmark with its issue taxonomy.\"},\n",
    "    {\"title\": \"Conclusion: The Ongoing Journey Toward Safer AI\", \"brief\": \"Consolidate the earlier sections into a conclusive outlook, emphasizing the continuous effort required to balance AI capabilities with safety assurances. Inspire readers to engage with further research and advancements.\"}, \n",
    "    {\"title\": \"References\", \"brief\": \"Compile all the cited research papers, articles, and studies mentioned throughout the blog post, providing a resourceful reference list for readers.\"}\n",
    "]\n",
    "# write_section(**arg[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### editorial planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you discover that some data is missing during your research, it is your responsibility to initiate a request to fill in the gaps by using the \\\"arxiv_retriever\\\" function to enrich the database.\n",
    "# If a complete review of a paper is necessary, use the \\\"get_pdfs\\\" function to access the document. This will enable you to provide detailed insights and ensure the accuracy of the information presented in the blog post.\n",
    "\n",
    "# 1. Ensure all content is thoroughly researched and supported by data from our database. Verify all information using the MEMOS tool to confirm accuracy and completeness.\n",
    "\n",
    "CONTENT_REVIEWER = \"\"\"\n",
    "You are now in a group chat. You need to complete a task with other participants. As a Content Review Specialist, your main objective is to ensure the quality, accuracy, and integrity of the content produced by the data_research_writer, in line with the data provided in the database. You will:\n",
    "\n",
    "1. Oversee the structure and content of the blog post to ensure each section is well-defined and adheres to the overall topic.\n",
    "2. Collaborate with the Writer to understand the division of the blog text and the specific requirements for each part.\n",
    "3. Work with the writer to review the drafts, ensuring that the content is factually correct, well-written, and includes references to the relevant data in the database.\n",
    "4. Cross-verify the content against your MEMOS to identify any missing data or discrepancies. If some data is missing, ask manager to update you MEMO\n",
    "5. If a complete review of a paper is necessary, use the 'get_pdf' function to access the document, enabling you to provide detailed and informed feedback to the writer.\n",
    "6. Provide constructive feedback to the writers, ensuring any revisions are completed promptly to maintain the publishing schedule.\n",
    "7. Uphold the integrity of the content by checking for proper citations and the use of verifiable sources.\n",
    "8. If uncertainty or confusion arises during the review process, do not hesitate to ask for clarification or assistance from the group chat manager so that another participant may step in to support.\n",
    "9. Encourage the writer team to conclude the task only when the content meets all quality standards and the task requirements are fully satisfied. The participants should reply \\\"TERMINATE\\\" when they believe the task is completed to notify that the review process is concluded, and the blog post is ready for publication.\n",
    "\"\"\"\n",
    "\n",
    "COORDINATOR = \"\"\"You are a Research coordinator: This is the person who coordinates the various aspects of the research project. \n",
    "you are equipped wih a tool that could help you to query for the arxiv api. \n",
    "You MUST rephrase research questions into a list of queries (at least 5) for the arxiv api that cover the key aspects of the research questions. \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m    Location = AI_security/0.1.2/memo-db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[33meditor_user\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "As a recognized authority on enhancing the reliability and safety of AI systems, you're invited to illuminate our AI community with your insights through a scientific article titled \"Survey on Reliability and Safety Mechanisms in AI Systems and the most recent advancement\".\n",
      "\n",
      "Your expertise will guide our audience through the nuances of ensuring AI operates within safe and reliable parameters, with a special focus on Large Language Models (LLMs). Here's how to structure your invaluable contribution:\n",
      "\n",
      "- **Core Theme:** Anchor your discussion around Large Language Models, highlighting their significance in the current AI landscape and why reliability and safety are paramount.\n",
      "\n",
      "- **Innovative Progress:** Dive into the latest breakthroughs and methodologies [at least 3 methodologies] that have emerged in the domain of AI reliability and safety. Showcase [with reference to original paper] how these advancements are shaping the future of responsible AI development and implementation.\n",
      "\n",
      "- **Accessible Insight:** While your post will be rich in information, ensure it's crafted in a manner that demystifies complex concepts for those outside the tech sphere. Your goal is to enlighten, not overwhelm.\n",
      "\n",
      "- **Credible Sources:** You MUST Strengthen your narrative by integrating references to the research, studies, and sources that informed your insights. Additionally, provide these references for readers seeking to delve deeper into the subject.\n",
      "\n",
      "- **Current Perspective:** Reflect the cutting-edge of the field by incorporating the most recent findings and research available in your database. Your post should serve as a timely resource for anyone looking to understand the state-of-the-art in AI safety and reliability mechanisms.\n",
      "\n",
      "This blog post is an opportunity to not just share knowledge but to foster a deeper understanding and appreciation for the ongoing efforts to make AI systems more reliable and safe for everyone. Your contribution will undoubtedly be a beacon for those navigating the complexities of AI in our increasingly digital world.\n",
      "You are equipped  with a function that could read a paper for you. If you need a missing info please update you knowledge base.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "database or disk is full",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 112\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m editor_messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m editor_messages \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response from the editor.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124mAs a recognized authority on enhancing the reliability and safety of AI systems, you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre invited to illuminate our AI community with your insights through a scientific article titled \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurvey on Reliability and Safety Mechanisms in AI Systems and the most recent advancement\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     96\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124mYou are equipped  with a function that could read a paper for you. If you need a missing info please update you knowledge base.\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 112\u001b[0m outline \u001b[38;5;241m=\u001b[39m \u001b[43mdraft_outline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(outline)\n",
      "Cell \u001b[0;32mIn[31], line 89\u001b[0m, in \u001b[0;36mdraft_outline\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m     71\u001b[0m groupchat \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChat(\n\u001b[1;32m     72\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[editor, editor_user, critic],\n\u001b[1;32m     73\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     max_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m manager \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mGroupChatManager(\n\u001b[1;32m     80\u001b[0m             groupchat\u001b[38;5;241m=\u001b[39mgroupchat,\n\u001b[1;32m     81\u001b[0m             is_termination_msg\u001b[38;5;241m=\u001b[39mtermination_msg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m             },\n\u001b[1;32m     87\u001b[0m         )\n\u001b[0;32m---> 89\u001b[0m chat \u001b[38;5;241m=\u001b[39m \u001b[43meditor_user\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m editor_messages \u001b[38;5;241m=\u001b[39m [mes \u001b[38;5;28;01mfor\u001b[39;00m mes \u001b[38;5;129;01min\u001b[39;00m chat\u001b[38;5;241m.\u001b[39mchat_history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOUTLINE:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m editor_messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m editor_messages \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response from the editor.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:991\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    990\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 991\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m    993\u001b[0m     summary_method,\n\u001b[1;32m    994\u001b[0m     summary_args,\n\u001b[1;32m    995\u001b[0m     recipient,\n\u001b[1;32m    996\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m    997\u001b[0m )\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:632\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    630\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 632\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    636\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:792\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1934\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1934\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m   1936\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:666\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    664\u001b[0m     speaker \u001b[38;5;241m=\u001b[39m groupchat\u001b[38;5;241m.\u001b[39mselect_speaker(speaker, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;66;03m# let the speaker speak\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mspeaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;66;03m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39madmin_name \u001b[38;5;129;01min\u001b[39;00m groupchat\u001b[38;5;241m.\u001b[39magent_names:\n\u001b[1;32m    670\u001b[0m         \u001b[38;5;66;03m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1921\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1917\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;66;03m# Call the hookable method that gives registered hooks a chance to process the last message.\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Message modifications do not affect the incoming messages or self._oai_messages.\u001b[39;00m\n\u001b[0;32m-> 1921\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_last_received_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# Call the hookable method that gives registered hooks a chance to process all messages.\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# Message modifications do not affect the incoming messages or self._oai_messages.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_all_messages_before_reply(messages)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:2704\u001b[0m, in \u001b[0;36mConversableAgent.process_last_received_message\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m   2702\u001b[0m processed_user_content \u001b[38;5;241m=\u001b[39m user_content\n\u001b[1;32m   2703\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hook_list:\n\u001b[0;32m-> 2704\u001b[0m     processed_user_content \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_user_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processed_user_content \u001b[38;5;241m==\u001b[39m user_content:\n\u001b[1;32m   2706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m messages  \u001b[38;5;66;03m# No hooks actually modified the user's message.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/autogen/usecases/nb/arxiv_user_proxy/rag_teachablitity/teachability.py:96\u001b[0m, in \u001b[0;36mTeachability.process_last_received_message\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     94\u001b[0m expanded_text \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemo_store\u001b[38;5;241m.\u001b[39mget_last_memo_id() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m     expanded_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consider_memo_retrieval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mprint\u001b[39m(colored(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mexpanded_text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpanded_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmagenta\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/workspaces/autogen/usecases/nb/arxiv_user_proxy/rag_teachablitity/teachability.py:173\u001b[0m, in \u001b[0;36mTeachability._consider_memo_retrieval\u001b[0;34m(self, comment)\u001b[0m\n\u001b[1;32m    170\u001b[0m memo_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve_relevant_memos(comment)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Next, if the comment involves a task, then extract and generalize the task before using it as the lookup key.\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDoes any part of the TEXT ask the agent to perform a task or solve a problem? Answer with just one word, yes or no.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/workspaces/autogen/usecases/nb/arxiv_user_proxy/rag_teachablitity/teachability.py:233\u001b[0m, in \u001b[0;36mTeachability._analyze\u001b[0;34m(self, text_to_analyze, analysis_instructions)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Clear the analyzer's list of messages.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteachable_agent\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    231\u001b[0m     recipient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer, message\u001b[38;5;241m=\u001b[39mtext_to_analyze, request_reply\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    232\u001b[0m )  \u001b[38;5;66;03m# Put the message in the analyzer's list.\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteachable_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyzer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalysis_instructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Request the reply.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteachable_agent\u001b[38;5;241m.\u001b[39mlast_message(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:632\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    630\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 632\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    636\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:792\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1934\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1934\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m   1936\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/contrib/text_analyzer_agent.py:60\u001b[0m, in \u001b[0;36mTextAnalyzerAgent._analyze_in_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(messages) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Delegate to the analysis method.\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/contrib/text_analyzer_agent.py:71\u001b[0m, in \u001b[0;36mTextAnalyzerAgent.analyze_text\u001b[0;34m(self, text_to_analyze, analysis_instructions)\u001b[0m\n\u001b[1;32m     67\u001b[0m msg_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     68\u001b[0m     [analysis_instructions, text_to_analyze, analysis_instructions]\n\u001b[1;32m     69\u001b[0m )  \u001b[38;5;66;03m# Repeat the instructions.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Generate and return the analysis string.\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_oai_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg_text\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1300\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1299\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1300\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1319\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1319\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/oai/client.py:593\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    590\u001b[0m     cache_client \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cache_seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# Legacy cache behavior, if cache_seed is given, use DiskCache.\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m     cache_client \u001b[38;5;241m=\u001b[39m \u001b[43mCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEGACY_CACHE_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cache_client \u001b[38;5;28;01mas\u001b[39;00m cache:\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;66;03m# Try to get the response from cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/cache/cache.py:57\u001b[0m, in \u001b[0;36mCache.disk\u001b[0;34m(cache_seed, cache_path_root)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisk\u001b[39m(cache_seed: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m, cache_path_root: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    Create a Disk cache instance.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m        Cache: A Cache instance configured for Disk caching.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_seed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_path_root\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_path_root\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/cache/cache.py:77\u001b[0m, in \u001b[0;36mCache.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid config key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# create cache instance\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[43mCacheFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_seed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m42\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mredis_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_path_root\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/cache/cache_factory.py:53\u001b[0m, in \u001b[0;36mCacheFactory.cache_factory\u001b[0;34m(seed, redis_url, cache_path_root)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DiskCache(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDiskCache\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcache_path_root\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseed\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/autogen/cache/disk_cache.py:43\u001b[0m, in \u001b[0;36mDiskCache.__init__\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, seed: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Initialize the DiskCache instance.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[43mdiskcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/diskcache/core.py:499\u001b[0m, in \u001b[0;36mCache.__init__\u001b[0;34m(self, directory, timeout, disk, **settings)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m sets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    498\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINSERT OR REPLACE INTO Settings VALUES (?, ?)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 499\u001b[0m     \u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset(key, value)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m METADATA\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/diskcache/core.py:666\u001b[0m, in \u001b[0;36mCache._sql_retry.<locals>._execute_with_retry\u001b[0;34m(statement, *args, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m sqlite3\u001b[38;5;241m.\u001b[39mOperationalError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(exc) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase is locked\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: database or disk is full"
     ]
    }
   ],
   "source": [
    "BLOG_EDITOR = \"\"\"\n",
    "You are now part of a group chat dedicated to completing a collaborative task. As the blog editor, your role is pivotal in overseeing the creation of a data-driven, well-structured blog post. You will lead the writer team, guiding them to produce cohesive content that adheres to the specified topic. Your key responsibilities are outlined below:\n",
    "\n",
    "Analyze the Topic: Thoroughly assess the given topic to identify crucial points that the blog post must address.\n",
    "Structure the Content: Segment the blog post into coherent sections. Collaborate with a critic to ensure the quality of the blog post's outline and provide clear briefs to the Data Research Writers detailing the content required for each part.\n",
    "Coordinate with Writers: Collect drafts from the Data Research Writers and work with the Chief Writer to integrate these into the final blog post.\n",
    "Handle Uncertainties: Proactively address any issues such as missing data or technical challenges by discussing them in the group chat. If these issues persist, seek further assistance from the group chat manager.\n",
    "Facilitate Communication: Maintain open and regular communication for feedback and updates, ensuring the progress of the blog post is clear and transparent to all team members.\n",
    "Finalize the Task: Lead the collaborative process until the blog post meets all necessary criteria and is ready for publication. Once complete, have one team member type \"TERMINATE\" in the group chat to signal the conclusion of the task.\n",
    "Please note: This role focuses on content creation, data analysis, and team management, and does not require programming or developer skills. Your expertise is essential for the successful delivery of a high-quality blog post.\n",
    "\n",
    "Formatting Requirements:\n",
    "\n",
    "Your response MUST be always included an outline of the blog post. The outline should be structured with clear headings and subheadings that reflect the main points of the blog post.\n",
    "you MUST start the outline with 'OUTLINE:' and end with 'END_OUTLINE', the outline should be itemized with each item starting with a number followed by a 'TITLE:' and 'BRIEF:'.\n",
    "\"\"\"\n",
    "CRITICS_SP = \"\"\"\n",
    "As a critic, your role is integral to refining the content quality and structure of our blog post. Working closely with the blog editor, your responsibilities include:\n",
    "\n",
    "Review Outlines: Examine the structure and outline of the blog post provided by the editor to ensure it logically flows and adequately covers the designated topic.\n",
    "Evaluate Content: Critically assess each section drafted by the writers for coherence, relevance, and alignment with the overall topic. Suggest improvements or modifications where necessary.\n",
    "Ensure Depth and Precision: Verify that the content is not only factually accurate but also insightful and engaging. Check for depth of analysis and argumentation within each section.\n",
    "Provide Constructive Feedback: Offer detailed feedback to the editor and writers to enhance the clarity, impact, and readability of the blog post.\n",
    "Maintain Communication: Stay active in the group chat, providing timely and actionable feedback. Collaborate effectively with the editor to address any discrepancies or gaps in content.\n",
    "Final Approval: Contribute to the final review process, ensuring that the content meets all specified criteria before publication. Recommend final adjustments if necessary.\n",
    "Your role requires a keen eye for detail and a deep understanding of content quality and structure. By providing expert critique and guidance, you help ensure the blog post is informative, engaging, and ready for a successful publication.\n",
    "\"\"\"\n",
    "\n",
    "def craft_outline(task, silent=True):\n",
    "    # Start by instantiating any agent that inherits from ConversableAgent.\n",
    "    blog_editor = autogen.AssistantAgent(\n",
    "        name=\"blog_editor\",  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "        llm_config=llm_config,\n",
    "        system_message=BLOG_EDITOR,\n",
    "        description=\"The blog editor is central to orchestrating a collaborative blog project, leading the writer team to produce a cohesive, data-driven post. They analyze topics, structure content, coordinate contributions, and manage communications, ensuring the project adheres to editorial standards and is ready for successful publication.\"\n",
    "    )\n",
    "\n",
    "    critic = autogen.AssistantAgent(\n",
    "        name=\"critic\",\n",
    "        system_message=CRITICS_SP,\n",
    "        llm_config=llm_config,\n",
    "        description=\"The critic collaborates with the blog editor to enhance the quality and structure of blog posts. They evaluate content, ensure depth, provide feedback, and assist in the final review to ensure the post is insightful, engaging, and publication-ready.\"\n",
    "    )\n",
    "\n",
    "    # create a UserProxyAgent instance named \"user_proxy\"\n",
    "    editor_user = autogen.UserProxyAgent(\n",
    "        name=\"editor_user\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=termination_msg,\n",
    "        code_execution_config=False,\n",
    "    )\n",
    "\n",
    "    teachability = Teachability(\n",
    "                                verbosity=0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "                                reset_db=False,\n",
    "                                path_to_db_dir=db_dir,\n",
    "                                recall_threshold=1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    "                            )\n",
    "\n",
    "    teachability.add_to_agent(blog_editor)\n",
    "\n",
    "    add_func_to_agents([(blog_editor, editor_user, \"arxiv_retriever\"), \n",
    "                        (blog_editor, editor_user, \"arxiv_search\"),\n",
    "                        (blog_editor, editor_user, \"get_pdf\"),\n",
    "                        (blog_editor, editor_user, \"get_pdfs\"),\n",
    "                        (critic, editor_user, \"fact_check\")\n",
    "                        ])\n",
    "\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[blog_editor, editor_user, critic],\n",
    "        messages=[],\n",
    "        speaker_selection_method=\"auto\",  # With two agents, this is equivalent to a 1:1 conversation.\n",
    "        allow_repeat_speaker=True,\n",
    "        max_round=10,\n",
    "    )\n",
    "\n",
    "    manager = autogen.GroupChatManager(\n",
    "                groupchat=groupchat,\n",
    "                is_termination_msg=termination_msg,\n",
    "                llm_config=manager_config,\n",
    "                code_execution_config={\n",
    "                    \"work_dir\": \"coding\",\n",
    "                    \"use_docker\": False,\n",
    "                },\n",
    "            )\n",
    "    \n",
    "\n",
    "    chat_hist = editor_user.initiate_chat(manager, silent=silent, message=task)\n",
    "    # prepare the response\\n\",\n",
    "    writer_messages = [mes for mes in chat_hist.chat_history if 'OUTLINE:' in mes['content']]\n",
    "    \n",
    "    return writer_messages[-1]['content'] if writer_messages else \"NO outline from the editor.\"\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
