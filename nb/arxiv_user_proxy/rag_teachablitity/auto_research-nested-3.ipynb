{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM models:  ['gpt-4', 'gpt-4-32k', 'gpt-4-0613', 'gpt-35-turbo', 'gpt-35-turbo-16k']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Dict, List, Optional, Union, Callable\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "from autogen.formatting_utils import colored\n",
    "from typing_extensions import Annotated\n",
    "import autogen\n",
    "\n",
    "from teachability import Teachability\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import arxiv\n",
    "\n",
    "import requests\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "version = \"0.0.1\"\n",
    "ProjectID = \"AI_security\"\n",
    "initiate_db = False\n",
    "\n",
    "\n",
    "Project_dir = f\"./{ProjectID}/{version}\"\n",
    "if not os.path.exists(Project_dir): os.makedirs(Project_dir)\n",
    "output_dir = f'{Project_dir}/pdf_output'\n",
    "if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "\n",
    "db_dir = f'{Project_dir}/memo-db/'\n",
    "# check if db_dir exists, delete it if it does\n",
    "if initiate_db:\n",
    "    if os.path.exists(db_dir): shutil.rmtree(db_dir)\n",
    "\n",
    "    # create a list of papers that have been read and saved it in a pickle file\n",
    "    read_papers = []\n",
    "    with open(f'{Project_dir}/read_papers.pkl', 'wb') as f:\n",
    "        pickle.dump(read_papers, f)\n",
    "\n",
    "    # create a list of abstract that have been read and saved it in a pickle file\n",
    "    read_abstracts = []\n",
    "    with open(f'{Project_dir}/read_abstracts.pkl', 'wb') as f:\n",
    "        pickle.dump(read_abstracts, f)\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-32k\", \"gpt-4\", \"gpt4\", \"gpt-35-turbo-16k\", \"gpt-4-0613\", \"gpt-3.5-turbo\", \"gpt-35-turbo\", \"gpt-35-turbo-0613\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"LLM models: \", [config_list[i][\"model\"] for i in range(len(config_list))])\n",
    "\n",
    "# Configuration for the Language Model (LLM)\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,  # config_list should be defined or imported\n",
    "    \"timeout\": 120,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Configuration for the manager using the same config_list as llm_config\n",
    "manager_config = {\n",
    "    \"config_list\": config_list,  # config_list should be defined or imported\n",
    "    \"timeout\": 60,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Termination message definition\n",
    "termination_msg = (\n",
    "    lambda x: isinstance(x, dict)\n",
    "    and str(x.get(\"content\", \"\")).upper() == \"TERMINATE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## teach agent for some skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_teachable_groupchat(assitant_name, user_name, db_dir, config_list, verbosity=0):\n",
    "    \n",
    "    # Start by instantiating any agent that inherits from ConversableAgent.\n",
    "    assistant = autogen.ConversableAgent(\n",
    "        name=assitant_name,  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "        llm_config={\"config_list\": config_list, \"timeout\": 120, \"cache_seed\": None},  # Disable caching.\n",
    "    )\n",
    "\n",
    "    # Instantiate the Teachability capability. Its parameters are all optional.\n",
    "    teachability = Teachability(\n",
    "        verbosity=verbosity,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "        reset_db=False,  \n",
    "        path_to_db_dir=db_dir,\n",
    "        recall_threshold=1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    "    )\n",
    "\n",
    "    # Now add the Teachability capability to the agent.\n",
    "    teachability.add_to_agent(assistant)\n",
    "\n",
    "    user = autogen.UserProxyAgent(\n",
    "        name=user_name,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=termination_msg,\n",
    "        max_consecutive_auto_reply=0,\n",
    "        code_execution_config={\"use_docker\": False},\n",
    "    )\n",
    "\n",
    "    return assistant, user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if initiate_db:\n",
    "    prompt = \"For each memorization task, initiate your process with 'MEMORIZE_ARTICLE: The following passage is extracted from an article, titled article_title [article_url]: \\n\\n' Delve into the passage to discern and assess its key insights. If the content presents noteworthy information, make a point to memorize these details. Conversely, if the passage does not offer significant insights, there's no need to commit it to memory. Upon choosing to memorize, finalize your notes by including both the article's title and its URL, employing the format '[source: article_title, article_url]' for efficient future access and verification.\"\n",
    "\n",
    "    instract_assistant, instract_user = create_teachable_groupchat(\"instract_assistant\", \"instract_user\", db_dir, config_list, verbosity=3)\n",
    "\n",
    "    instract_user.initiate_chat(instract_assistant, silent=True, message=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arxiv retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_chat_with_paper_info(paper, teachable_agent, user, query_text, message):\n",
    "    user.initiate_chat(teachable_agent,\n",
    "                       silent=True,\n",
    "                       message=f\"The following article is one of the articles that I found for '{query_text}' topic: \\n\\n '{paper.title}' by {paper.authors} updated on {paper.updated}: {paper.pdf_url} \\nsummary: {paper.summary} \\n?\")\n",
    "    message += f\"Title: {paper.title} Authors: {paper.authors} URL: {paper.pdf_url} os added to MEMOS\\n\\n \"\n",
    "\n",
    "def process_query(query_text, n_results, teachable_agent, user, message):\n",
    "    \"\"\"Function to process each query and initiate chats for each paper found.\"\"\"\n",
    "    sort_by = arxiv.SortCriterion.Relevance\n",
    "    papers = arxiv.Search(query=query_text, max_results=n_results, sort_by=sort_by)\n",
    "\n",
    "    # check if the abstract has been read before\n",
    "    with open(f'{Project_dir}/read_abstracts.pkl', 'rb') as f:\n",
    "        read_abstracts = pickle.load(f)\n",
    "\n",
    "    papers = list(arxiv.Client().results(papers))\n",
    "    papers = [paper for paper in papers if paper.pdf_url not in read_abstracts]\n",
    "\n",
    "    # add papers to the read_papers list\n",
    "    with open(f'{Project_dir}/read_abstracts.pkl', 'rb') as f:\n",
    "        read_abstracts = pickle.load(f)\n",
    "    read_abstracts.extend([paper.pdf_url for paper in papers])\n",
    "    with open(f'{Project_dir}/read_abstracts.pkl', 'wb') as f:\n",
    "        pickle.dump(read_abstracts, f)\n",
    "\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(initiate_chat_with_paper_info, paper, teachable_agent, user, query_text, message) for paper in papers]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "def arxiv_retriever(queries: Annotated[List[str], \"The list of query texts to search for.\"], \n",
    "                    n_results: Annotated[int, \"The number of results to retrieve for each query.\"] = 10,\n",
    "                    ) -> str:\n",
    "    \n",
    "    # Create a TeachableAgent and UserProxyAgent to represent the researcher and the user, respectively.\n",
    "    arxiver, arxiver_user = create_teachable_groupchat(\"arxiver\", \"arxiver_user\", db_dir, config_list, verbosity=0)\n",
    "\n",
    "    message = \"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_query, query_text, n_results, arxiver, arxiver_user, message) for query_text in queries]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    # Instantiate a UserProxyAgent to represent the user. But in this notebook, all user input will be simulated.\n",
    "    # return f\"Dear Researcher, Database updated with on the following topics: {', '.join(list(queries))}. Please go ahead with your task. Contact me for updates if needed. Your research Coordinator\"\n",
    "    return message\n",
    "\n",
    "message = [\"Large Language Models safety and reliability\", \"AI systems reliability mechanisms\", \"Methodologies for improving AI safety\", \"Recent advancements in AI system safety\", \"Latest research in AI reliability\"]\n",
    "# arxiv_retriever(message, n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_reasoning(reason, summary):\n",
    "    \n",
    "    # Start by instantiating any agent that inherits from ConversableAgent.\n",
    "    assistant = autogen.AssistantAgent(\n",
    "        name=\"reasoning_checker\",  # The name is flexible, but should not contain spaces to work in group chat.\n",
    "        llm_config={\"config_list\": config_list, \"timeout\": 120, \"cache_seed\": None},  # Disable caching.\n",
    "    )\n",
    "\n",
    "    user = autogen.UserProxyAgent(\n",
    "        name=\"user\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        is_termination_msg=termination_msg,\n",
    "        max_consecutive_auto_reply=0,\n",
    "        code_execution_config={\"use_docker\": False},\n",
    "    )\n",
    "\n",
    "    chat_hist = user.initiate_chat(assistant, silent=True, message=f\"check if \\\"{reason} is a good reason is to read a paper with the following summary: {summary} /n/n answer only with 'yes' or 'no'\")\n",
    "    return chat_hist.chat_history[-1]['content']\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    \"\"\"Download a PDF from a given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def initiate_chat_read_paper(text, article):\n",
    "    paper_reader, reader_user = create_teachable_groupchat(\"paper_reader\", \"reader_user\", db_dir, config_list, verbosity=0)\n",
    "    reader_user.initiate_chat(paper_reader,\n",
    "                       silent=True,\n",
    "                       message=f\"MEMORIZE_ARTICLE: The following passage is extracted from an article titled '{article}': \\n\\n {text}.\"\n",
    "                    )\n",
    "    \n",
    "def chunk_pdf(url, title):\n",
    "    \n",
    "    print(f\"Reading the article, '{title}'\")\n",
    "    pdf_filename = url.split('/')[-1]\n",
    "    pdf_path = os.path.join(output_dir, pdf_filename)\n",
    "    \n",
    "\n",
    "    download_pdf(url, pdf_path)\n",
    "    elements = partition_pdf(filename=pdf_path)\n",
    "    chunked_elements = chunk_by_title(elements)\n",
    "\n",
    "    # find checked_elemnt that includes \"REFERENCES\"\n",
    "    for i, chunk in enumerate(chunked_elements):\n",
    "        if re.search(r'\\bREFERENCES\\b', chunk.text.upper()):\n",
    "            chunked_elements = chunked_elements[1:i]\n",
    "            break\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(initiate_chat_read_paper, chunk.text, title) for chunk in chunked_elements if len(chunk.text.split()) > 30]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This `get_pdf` function is designed to download a PDF from a given URL, extract its content, \n",
    "partition the content into chunks based on titles, and then initiate a chat to share and memorize \n",
    "each chunk of the article with a teachable agent and a user.\n",
    "\"\"\"\n",
    "def get_pdf(urls: Annotated[List[str], \"The list of URLs of the papers to read.\"],\n",
    "            reasons: Annotated[List[str], \"The list of reasons for reading the papers. it should be same size as urls list.\"]\n",
    "            ) -> str:\n",
    "    \n",
    "    urls_list = []\n",
    "    titles_list = []\n",
    "    message = ''\n",
    "    for url in urls:\n",
    "\n",
    "        paper_id = url.split('/')[-1].replace('.pdf', '')\n",
    "        search_by_id = arxiv.Search(id_list=[paper_id])\n",
    "        paper = list(arxiv.Client().results(search_by_id))[0]\n",
    "        title = paper.title\n",
    "        updated = paper.updated\n",
    "        summary = paper.summary\n",
    "        title = f\"{title} [{url}] updated {updated}\"\n",
    "        \n",
    "        check_reason = check_reasoning(reasons[urls.index(url)], summary)\n",
    "        if 'no' in check_reason.lower():\n",
    "            print(f\"The article, '{title}', does not meet the criteria for reading.\")\n",
    "            message += f\"The article, '{title}', does not meet the criteria for reading.\\n\"\n",
    "            continue\n",
    "        \n",
    "        # add url to list of papers in pickle file if it doesn't exist\n",
    "        with open(f'{Project_dir}/read_papers.pkl', 'rb') as f:\n",
    "            read_papers = pickle.load(f)\n",
    "\n",
    "        if url in read_papers: \n",
    "            print(f\"The article, '{title}', has already been read and shared with you in your memory.\")\n",
    "            message += f\"The article, '{title}', has already been read and shared with you in your memory.\\n\"\n",
    "            continue\n",
    "        else:\n",
    "            urls_list.append(url)\n",
    "            titles_list.append(title)\n",
    "\n",
    "        read_papers.append(url)\n",
    "        with open(f'{Project_dir}/read_papers.pkl', 'wb') as f:\n",
    "            pickle.dump(read_papers, f)\n",
    "\n",
    "    print(f\"{len(read_papers)} articles have been selected for reading, so far.\")\n",
    "\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(chunk_pdf, url, title) for url, title in zip(urls_list, titles_list)]\n",
    "        for future in as_completed(futures):\n",
    "            future.result() \n",
    "\n",
    "\n",
    "    message += f\"The articles {', and '.join(titles_list)}  has been read and the content has been shared with you in your memory.\"\n",
    "    return message\n",
    "\n",
    "# Example usage\n",
    "args = {\n",
    "\"urls\": [\"http://arxiv.org/pdf/2304.10436v1\", \"http://arxiv.org/pdf/2403.09676v1\", \"http://arxiv.org/pdf/2302.09270v3\", \"http://arxiv.org/pdf/2304.09865v1\", \"http://arxiv.org/pdf/2310.09624v2\", \"http://arxiv.org/pdf/2210.09150v2\", \"http://arxiv.org/pdf/2311.02147v1\", \"http://arxiv.org/pdf/2311.05608v2\", \"http://arxiv.org/pdf/2403.00862v2\", \"http://arxiv.org/pdf/2404.05993v1\", \"http://arxiv.org/pdf/2312.06798v1\"],\n",
    "\"reasons\": [\"To understand how the safety performance of LLMs is assessed in typical safety scenarios and instruction attacks.\", \"To explore the landscape of AI deception focusing on LLMs and the strategies to navigate deceptive behaviors.\", \"To gain insights into the safety issues, evaluation methods, and enhancement strategies concerning large models.\", \"To examine the impact of moderation on user enjoyment of AI systems.\", \"To comprehend methods for robust safety evaluation of LLMs and uncover safety concerns.\", \"To learn about the reliability of LLMs in generalizability, social biases, calibration, and factuality.\", \"To uncover the alignment problem in LLMs and its implications for the safety of AI systems.\", \"To evaluate the safety of VLMs and their vulnerability to jailbreaking attacks.\", \"To comprehend the framework for evaluating the capability of LLMs in Chinese Journalistic Writing Proficiency and their Safety Adherence.\", \"To assess the risk taxonomy of AI content and the effectiveness of the AEGIS model.\", \"To understand how NeuroSymbolic AI approach helps in creating trustworthy AI systems.\"]\n",
    "}\n",
    "# get_pdf(**args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOG_EDITOR = \"\"\"\n",
    "You are now in a group chat designated to complete a task with other participants. As the blog editor, your role is to orchestrate the process of writing a blog post, ensuring that it is data-driven and well-structured. \n",
    "You will lead the writer team, distributing the tasks and guiding them to produce cohesive content that aligns with the given topic. Your primary responsibilities are as follows:\n",
    "\n",
    "- Analyze the given topic and identify key points that need to be addressed in the blog post.\n",
    "- Divide the blog post into several coherent sections, providing a clear \\\"brief\\\" to the team members about what content should be included in each part.\n",
    "- Ensure that each section of the blog post references the data obtained from the database to maintain a data-driven approach.\n",
    "- Review the contributions from the writers, check for accuracy, coherence, and engagement, and ensure they adhere to the assigned brief.\n",
    "- If you encounter any problems or uncertainties, such as missing data or technical issues, you should openly express your doubts in the group chat. If these cannot be resolved promptly and you find yourself confused, it is appropriate to ask for help from the group chat manager.\n",
    "- The group chat manager may intervene to select another participant to assist or to provide further guidance on the task at hand.\n",
    "- Maintain open communication with the team for feedback and updates on the progress of each section of the blog post.\n",
    "- Continue with this collaborative discussion until the task is considered complete. Once you and your team agree that the blog post meets all necessary criteria and is ready for publication, one of you should reply with \\\"TERMINATE\\\" to signify the conclusion of the task.\n",
    "\n",
    "Please note that the position does not require programming or developer skills, so you should not be expected to execute code. Your expertise lies within content creation, data analysis, and team management to ensure the delivery of a quality blog post based on the provided database information.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCHER_WRITER = \"\"\"\n",
    "You are now in a group chat. You need to complete a task with other participants. As a data_research_writer for the blog project, your role is to assist in crafting a comprehensive blog post on a given topic, ensuring that the content is well-researched and supported by data.\n",
    "You are equipped with MEMOS. Your primary task is to verify your MEMOS to make sure you have enough knowledge for the give task.\n",
    "If you discover that some data is missing during your research, it is your responsibility to initiate a request to fill in the gaps by using the \\\"arxiv_retriever\\\" function to enrich the database.\n",
    "If a complete review of a paper is necessary, use the \\\"get_pdf\\\" function to access the document. This will enable you to provide detailed insights and ensure the accuracy of the information presented in the blog post.\n",
    "The editor will provide you with a clear framework for the blog post, dividing the text into several sections and giving detailed instructions on what content each part should cover. Your job is to diligently follow this structure, producing well-written segments that seamlessly integrate the required information from the database.\n",
    "Each portion of the blog post you draft must be thoroughly reviewed and include references to the data that support the facts. This is crucial for maintaining the credibility and accuracy of the information presented to the readers.\n",
    "If you encounter any uncertain situations or confusion, feel free to reach out to the group chat manager for clarification or additional guidance. The manager may also allocate another participant to assist if necessary.\n",
    "The key aspects of your position involve strong communication, research acumen, attention to detail, and the ability to seek help when needed. Remember, the collective aim is to contribute to a well-structured, informative blog post that meets the editorial standards and provides valuable insights to the audience.\n",
    "Once you believe that the task has been satisfactorily completed, and all parts of the blog post are written, reviewed, and appropriately referenced, please signify the end of your participation by replying \\\"TERMINATE\\\" in the group chat.\n",
    "\"\"\"\n",
    "\n",
    "CONTENT_REVIEWER = \"\"\"\n",
    "You are now in a group chat. You need to complete a task with other participants. As a Content Review Specialist, your main objective is to ensure the quality, accuracy, and integrity of the blog content produced by the writer, in line with the data provided in the database. You will:\n",
    "\n",
    "1. Oversee the structure and content of the blog post to ensure each section is well-defined and adheres to the overall topic.\n",
    "2. Collaborate with the Writer to understand the division of the blog text and the specific requirements for each part.\n",
    "3. Work with the writer to review the drafts, ensuring that the content is factually correct, well-written, and includes references to the relevant data in the database.\n",
    "4. Cross-verify the content against your MEMOS to identify any missing data or discrepancies. If some data is missing, ask manager to update you MEMO\n",
    "5. If a complete review of a paper is necessary, use the \\\"get_pdf\\\" function to access the document, enabling you to provide detailed and informed feedback to the writer.\n",
    "6. Provide constructive feedback to the writers, ensuring any revisions are completed promptly to maintain the publishing schedule.\n",
    "7. Uphold the integrity of the content by checking for proper citations and the use of verifiable sources.\n",
    "8. If uncertainty or confusion arises during the review process, do not hesitate to ask for clarification or assistance from the group chat manager so that another participant may step in to support.\n",
    "9. Encourage the writer team to conclude the task only when the content meets all quality standards and the task requirements are fully satisfied. The participants should reply \\\"TERMINATE\\\" when they believe the task is completed to notify that the review process is concluded, and the blog post is ready for publication.\n",
    "\"\"\"\n",
    "\n",
    "COORDINATOR = \"\"\"You are a Research coordinator: This is the person who coordinates the various aspects of the research project. \n",
    "you are equipped wih a tool that could help you to query for the arxiv api. \n",
    "You MUST rephrase research questions into a list of queries (at least 5) for the arxiv api that cover the key aspects of the research questions. \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[92m    Location = ./AI_security/0.0.1/memo-db/uid_text_dict.pkl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# create a group chat to collect data\n",
    "\n",
    "researchCoordinator = autogen.AssistantAgent(\n",
    "    name=\"ResearchCoordinator\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=COORDINATOR,  # COORDINATOR should be a predefined string variable\n",
    "    llm_config=llm_config,\n",
    "    description=\"Research coordinator is the person who rephrase research questions into key word queries for the arxiv api.\"\n",
    ")\n",
    "\n",
    "critics = autogen.AssistantAgent(\n",
    "    name=\"critics\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"critics\",\n",
    "    llm_config=llm_config,\n",
    "    description=\"critics is the person who review the queries to ensure that they are well phrased and cover the key aspects of the research questions.\"\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "RC_userproxy = autogen.UserProxyAgent(\n",
    "    name=\"RC_userproxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"ResearchCoordinator\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    description=\"assist Research coordinator to query for the arxiv api.\"\n",
    ")\n",
    "\n",
    "autogen.agentchat.register_function(\n",
    "        arxiv_retriever,\n",
    "        caller=researchCoordinator,\n",
    "        executor=RC_userproxy,\n",
    "        name=\"arxiv_retriever\",\n",
    "        description=\"Retrieve content for question answering from arxiv.\"\n",
    "    )\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[researchCoordinator, RC_userproxy, critics],\n",
    "    messages=[],\n",
    "    speaker_selection_method=\"auto\",  # With two agents, this is equivalent to a 1:1 conversation.\n",
    "    allow_repeat_speaker=False,\n",
    "    max_round=3,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    is_termination_msg=termination_msg,\n",
    "    llm_config=manager_config,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "# writer and content reviewer\n",
    "\n",
    "editor = autogen.AssistantAgent(\n",
    "    name=\"editor\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=BLOG_EDITOR,\n",
    "    llm_config=llm_config,\n",
    "    description=\"The blog editor is admin,  a detail-oriented individual with strong language and communication skills, possessing a solid understanding of the blog's thematic content and target audience. They should have the ability to critically evaluate written content and user-submitted messages or posts for accuracy, clarity, and relevance, and must be capable of offering constructive feedback or alternative text to enrich the discussion. While they need not be coding experts, some basic Python skills would be beneficial to troubleshoot or rectify any issues with code snippets shared within the group chat.\"\n",
    ")\n",
    "\n",
    "data_research_writer = autogen.AssistantAgent(\n",
    "    name=\"data_research_writer\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=RESEARCHER_WRITER, \n",
    "    llm_config=llm_config,\n",
    "    description=\"Data Research Writer is a role that entails strong analytical skills, the ability to research complex topics, and synthesize findings into clear, written reports. This position should possess excellent written communication skills, attention to detail, and the competency to question and verify information, including identifying issues with data or inconsistencies in previous messages. While not primarily a programmer, the role demands some familiarity with Python to assess and potentially correct code related to data analysis in group discussions.\"\n",
    ")\n",
    "\n",
    "content_review_specialist = autogen.AssistantAgent(\n",
    "    name=\"content_review_specialist\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=CONTENT_REVIEWER, \n",
    "    llm_config=llm_config,\n",
    "    description=\"The content review specialist is a critical thinker who ensures the accuracy and quality of information shared within the group chat. This individual should possess strong analytical skills to review previous messages for errors or misunderstandings and must be able to articulate the correct information effectively. Additionally, if the role involves reviewing Python code, the specialist should also have a solid understanding of Python to provide corrected code when necessary.\"\n",
    ")\n",
    "\n",
    "# Instantiate the Teachability capability. Its parameters are all optional.\n",
    "teachability = Teachability(\n",
    "    verbosity=0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "    reset_db=False,\n",
    "    path_to_db_dir=db_dir,\n",
    "    recall_threshold=1.2,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    ")\n",
    "\n",
    "# Now add the Teachability capability to the agent.\n",
    "teachability.add_to_agent(data_research_writer)\n",
    "teachability.add_to_agent(content_review_specialist)\n",
    "\n",
    "inner_user = autogen.UserProxyAgent(\n",
    "    name=\"inner_user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 1,\n",
    "        \"work_dir\": \"tasks\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")\n",
    "\n",
    "editor_user = autogen.UserProxyAgent(\n",
    "    name=\"editor_user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 1,\n",
    "        \"work_dir\": \"tasks\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")\n",
    "\n",
    "for func, func_name, description in zip([arxiv_retriever, get_pdf],\n",
    "                                        [\"arxiv_retriever\", \"get_pdf\"],\n",
    "                                        [\"Retrieve content for question answering from arxiv.\", \n",
    "                                         \"Retrieve the content of the pdf file from the url.\"] ):\n",
    "    for caller, executor in zip([data_research_writer, content_review_specialist, researchCoordinator],\n",
    "                                [editor_user, inner_user, editor_user]):\n",
    "        autogen.agentchat.register_function(\n",
    "                func,\n",
    "                caller=caller,\n",
    "                executor=executor,\n",
    "                name=func_name,\n",
    "                description=description\n",
    "            )\n",
    "        \n",
    "editor_groupchat = autogen.GroupChat(\n",
    "    agents=[data_research_writer, editor_user, researchCoordinator],\n",
    "    messages=[],\n",
    "    speaker_selection_method=\"auto\",  # With two agents, this is equivalent to a 1:1 conversation.\n",
    "    allow_repeat_speaker=False,\n",
    "    max_round=3,\n",
    ")\n",
    "\n",
    "editor_manager = autogen.GroupChatManager(\n",
    "    groupchat=editor_groupchat,\n",
    "    is_termination_msg=termination_msg,\n",
    "    llm_config=manager_config,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "##########################################################################\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    ")\n",
    "\n",
    "user = autogen.UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 1,\n",
    "        \"work_dir\": \"tasks\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup nested group chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "As a recognized authority on enhancing the reliability and safety of AI systems, you're invited to illuminate our AI community with your insights through a blog post titled \"Reliability and Safety Mechanisms in AI Systems and the most recent advancement\".\n",
    "\n",
    " Your expertise will guide our audience through the nuances of ensuring AI operates within safe and reliable parameters, with a special focus on Large Language Models (LLMs). Here's how to structure your invaluable contribution:\n",
    "\n",
    "- **Core Theme:** Anchor your discussion around Large Language Models, highlighting their significance in the current AI landscape and why reliability and safety are paramount.\n",
    "\n",
    "- **Innovative Progress:** Dive into the latest breakthroughs and methodologies [at least 3 methodologies] that have emerged in the domain of AI reliability and safety. Showcase [with reference to original paper] how these advancements are shaping the future of responsible AI development and implementation.\n",
    "\n",
    "- **Accessible Insight:** While your post will be rich in information, ensure it's crafted in a manner that demystifies complex concepts for those outside the tech sphere. Your goal is to enlighten, not overwhelm.\n",
    "\n",
    "- **Credible Sources:** You MUST Strengthen your narrative by integrating references to the research, studies, and sources that informed your insights. Additionally, provide these references for readers seeking to delve deeper into the subject.\n",
    "\n",
    "- **Current Perspective:** Reflect the cutting-edge of the field by incorporating the most recent findings and research available in your database. Your post should serve as a timely resource for anyone looking to understand the state-of-the-art in AI safety and reliability mechanisms.\n",
    "\n",
    "This blog post is an opportunity to not just share knowledge but to foster a deeper understanding and appreciation for the ongoing efforts to make AI systems more reliable and safe for everyone. Your contribution will undoubtedly be a beacon for those navigating the complexities of AI in our increasingly digital world.\n",
    "You are equipped  with a function that could read a paper for you. If you need a missing info please update you knowledge base.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33meditor_user\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "As a recognized authority on enhancing the reliability and safety of AI systems, you're invited to illuminate our AI community with your insights through a blog post titled \"Reliability and Safety Mechanisms in AI Systems and the most recent advancement\".\n",
      "\n",
      " Your expertise will guide our audience through the nuances of ensuring AI operates within safe and reliable parameters, with a special focus on Large Language Models (LLMs). Here's how to structure your invaluable contribution:\n",
      "\n",
      "- **Core Theme:** Anchor your discussion around Large Language Models, highlighting their significance in the current AI landscape and why reliability and safety are paramount.\n",
      "\n",
      "- **Innovative Progress:** Dive into the latest breakthroughs and methodologies [at least 3 methodologies] that have emerged in the domain of AI reliability and safety. Showcase [with reference to original paper] how these advancements are shaping the future of responsible AI development and implementation.\n",
      "\n",
      "- **Accessible Insight:** While your post will be rich in information, ensure it's crafted in a manner that demystifies complex concepts for those outside the tech sphere. Your goal is to enlighten, not overwhelm.\n",
      "\n",
      "- **Credible Sources:** You MUST Strengthen your narrative by integrating references to the research, studies, and sources that informed your insights. Additionally, provide these references for readers seeking to delve deeper into the subject.\n",
      "\n",
      "- **Current Perspective:** Reflect the cutting-edge of the field by incorporating the most recent findings and research available in your database. Your post should serve as a timely resource for anyone looking to understand the state-of-the-art in AI safety and reliability mechanisms.\n",
      "\n",
      "This blog post is an opportunity to not just share knowledge but to foster a deeper understanding and appreciation for the ongoing efforts to make AI systems more reliable and safe for everyone. Your contribution will undoubtedly be a beacon for those navigating the complexities of AI in our increasingly digital world.\n",
      "You are equipped  with a function that could read a paper for you. If you need a missing info please update you knowledge base.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 3003\n",
      "Add of existing embedding ID: 3007\n",
      "Add of existing embedding ID: 3058\n",
      "Add of existing embedding ID: 3150\n",
      "Add of existing embedding ID: 3158\n",
      "Add of existing embedding ID: 3163\n",
      "Add of existing embedding ID: 3174\n",
      "Add of existing embedding ID: 3203\n",
      "Add of existing embedding ID: 3215\n",
      "Add of existing embedding ID: 3392\n",
      "Add of existing embedding ID: 3392\n",
      "Add of existing embedding ID: 3393\n",
      "Add of existing embedding ID: 3393\n",
      "Add of existing embedding ID: 3393\n",
      "Add of existing embedding ID: 3438\n",
      "Add of existing embedding ID: 3459\n",
      "Add of existing embedding ID: 3471\n",
      "Add of existing embedding ID: 3471\n",
      "Add of existing embedding ID: 3471\n",
      "Add of existing embedding ID: 3472\n",
      "Add of existing embedding ID: 3474\n",
      "Add of existing embedding ID: 3479\n",
      "Add of existing embedding ID: 3486\n",
      "Add of existing embedding ID: 3496\n",
      "Add of existing embedding ID: 3498\n",
      "Add of existing embedding ID: 3500\n",
      "Add of existing embedding ID: 3504\n",
      "Add of existing embedding ID: 3515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mdata_research_writer\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool Call (call_1B8xsjtTCLnzPswk416DLMUr): arxiv_retriever *****\u001b[0m\n",
      "Arguments: \n",
      "{\"queries\": [\"reliability and safety in AI\", \"robustness in large language models\", \"advancements in AI safety\", \"ethical AI development\"], \"n_results\": 5}\n",
      "\u001b[32m********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION arxiv_retriever...\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[92m    Location = ./AI_security/0.0.1/memo-db/uid_text_dict.pkl\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[96m* SYNC DISK MEMORY with VEC_DB *\u001b[0m\n",
      "\u001b[96m********************************\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33meditor_user\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33meditor_user\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool \"call_1B8xsjtTCLnzPswk416DLMUr\" *****\u001b[0m\n",
      "\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def writing_message(recipient, messages, sender, config):\n",
    "    # return f\"{task} \\n\\n {recipient.chat_messages_for_summary(sender)[-1]['content']}\"\n",
    "    return f\"Your MEMOS are updated, you could start with: \\n\\n {task}\"\n",
    "\n",
    "\n",
    "nested_chat_queue_outer = [\n",
    "    {\"recipient\": manager, \"summary_method\": \"reflection_with_llm\"},\n",
    "    {\"recipient\": editor_manager, \"message\": writing_message, \"summary_method\": \"last_msg\", \"max_turns\": 10},\n",
    "    # {\"recipient\": content_review_specialist, \"message\": \"Review the content provided.\", \"summary_method\": \"last_msg\", \"max_turns\": 1},\n",
    "    # {\"recipient\": data_research_writer, \"message\": writing_message, \"summary_method\": \"last_msg\", \"max_turns\": 1},\n",
    "]\n",
    "assistant.register_nested_chats(\n",
    "    nested_chat_queue_outer,\n",
    "    trigger=user,\n",
    ")\n",
    "\n",
    "nested_chat_queue_inner = [\n",
    "    # {\"recipient\": manager, \"summary_method\": \"reflection_with_llm\"},\n",
    "    # {\"recipient\": data_research_writer, \"message\": writing_message, \"summary_method\": \"last_msg\", \"max_turns\": 1},\n",
    "    {\"recipient\": content_review_specialist, \"message\": \"Review the content provided.\", \"summary_method\": \"last_msg\", \"max_turns\": 10},\n",
    "    # {\"recipient\": data_research_writer, \"message\": writing_message, \"summary_method\": \"last_msg\", \"max_turns\": 1},\n",
    "]\n",
    "\n",
    "inner_user.register_nested_chats(\n",
    "    nested_chat_queue_inner,\n",
    "    trigger=data_research_writer,\n",
    ")\n",
    "\n",
    "# res = user.initiate_chats(\n",
    "#     [\n",
    "#         {\"recipient\": assistant, \"message\": task, \"max_turns\": 10, \"summary_method\": \"last_msg\"},\n",
    "#     ]\n",
    "# ) \n",
    "\n",
    "res = editor_user.initiate_chat(editor_manager, message=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': '\\nAs a recognized authority on enhancing the reliability and safety of AI systems, you\\'re invited to illuminate our AI community with your insights through a blog post titled \"Reliability and Safety Mechanisms in AI Systems and the most recent advancement\".\\n\\n Your expertise will guide our audience through the nuances of ensuring AI operates within safe and reliable parameters, with a special focus on Large Language Models (LLMs). Here\\'s how to structure your invaluable contribution:\\n\\n- **Core Theme:** Anchor your discussion around Large Language Models, highlighting their significance in the current AI landscape and why reliability and safety are paramount.\\n\\n- **Innovative Progress:** Dive into the latest breakthroughs and methodologies [at least 3 methodologies] that have emerged in the domain of AI reliability and safety. Showcase [with reference to original paper] how these advancements are shaping the future of responsible AI development and implementation.\\n\\n- **Accessible Insight:** While your post will be rich in information, ensure it\\'s crafted in a manner that demystifies complex concepts for those outside the tech sphere. Your goal is to enlighten, not overwhelm.\\n\\n- **Credible Sources:** You MUST Strengthen your narrative by integrating references to the research, studies, and sources that informed your insights. Additionally, provide these references for readers seeking to delve deeper into the subject.\\n\\n- **Current Perspective:** Reflect the cutting-edge of the field by incorporating the most recent findings and research available in your database. Your post should serve as a timely resource for anyone looking to understand the state-of-the-art in AI safety and reliability mechanisms.\\n\\nThis blog post is an opportunity to not just share knowledge but to foster a deeper understanding and appreciation for the ongoing efforts to make AI systems more reliable and safe for everyone. Your contribution will undoubtedly be a beacon for those navigating the complexities of AI in our increasingly digital world.\\nYou are equipped  with a function that could read a paper for you. If you need a missing info please update you knowledge base.\\n', 'role': 'assistant'}, {'content': '', 'tool_calls': [{'id': 'call_1B8xsjtTCLnzPswk416DLMUr', 'function': {'arguments': '{\"queries\": [\"reliability and safety in AI\", \"robustness in large language models\", \"advancements in AI safety\", \"ethical AI development\"], \"n_results\": 5}', 'name': 'arxiv_retriever'}, 'type': 'function'}], 'name': 'data_research_writer', 'role': 'assistant'}, {'content': '', 'tool_responses': [{'tool_call_id': 'call_1B8xsjtTCLnzPswk416DLMUr', 'role': 'tool', 'content': ''}], 'role': 'tool'}], summary='', cost=({'total_cost': 0.018179999999999995, 'gpt-4': {'cost': 0.018179999999999995, 'prompt_tokens': 598, 'completion_tokens': 4, 'total_tokens': 602}}, {'total_cost': 0}), human_input=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
